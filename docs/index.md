# CS231n Convolutional Neural Networks for Visual Recognition

[Course Website](http://cs231n.stanford.edu/)

These notes accompany the Stanford CS class [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/). For questions/concerns/bug reports, please submit a pull request directly to our [git repo](https://github.com/cs231n/cs231n.github.io).

## Spring 2023 Assignments

[Assignment #1: Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network](https://cs231n.github.io/assignments2023/assignment1/)

[Assignment #2: Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization](https://cs231n.github.io/assignments2023/assignment2/)

(To be released) Assignment #3: Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning

## Module 0: Preparation

[Software Setup](https://cs231n.github.io/setup-instructions/)

[Python / Numpy Tutorial (with Jupyter and Colab)](https://cs231n.github.io/python-numpy-tutorial/)

### Module 1: Neural Networks

[Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits](https://cs231n.github.io/classification/)

L1/L2 distances, hyperparameter search, cross-validation

[Linear classification: Support Vector Machine, Softmax](https://cs231n.github.io/linear-classify/)

parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo

[Optimization: Stochastic Gradient Descent](https://cs231n.github.io/optimization-1/)

optimization landscapes, local search, learning rate, analytic/numerical gradient

[Backpropagation, Intuitions](https://cs231n.github.io/optimization-2/)

chain rule interpretation, real-valued circuits, patterns in gradient flow

[Neural Networks Part 1: Setting up the Architecture](https://cs231n.github.io/neural-networks-1/)

model of a biological neuron, activation functions, neural net architecture, representational power

[Neural Networks Part 2: Setting up the Data and the Loss](https://cs231n.github.io/neural-networks-2/)

preprocessing, weight initialization, batch normalization, regularization (L2/dropout), loss functions

[Neural Networks Part 3: Learning and Evaluation](https://cs231n.github.io/neural-networks-3/)

gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles

[Putting it together: Minimal Neural Network Case Study](https://cs231n.github.io/neural-networks-case-study/)

minimal 2D toy data example

Module 2: Convolutional Neural Networks

[Convolutional Neural Networks: Architectures, Convolution / Pooling Layers](https://cs231n.github.io/convolutional-networks/)

layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational considerations

[Understanding and Visualizing Convolutional Neural Networks](https://cs231n.github.io/understanding-cnn/)

tSNE embeddings, deconvnets, data gradients, fooling ConvNets, human comparisons

[Transfer Learning and Fine-tuning Convolutional Neural Networks](https://cs231n.github.io/transfer-learning/)

Student-Contributed Posts

[Taking a Course Project to Publication](https://cs231n.github.io/choose-project/)[Recurrent Neural Networks](https://cs231n.github.io/rnn/)

- [ cs231n](https://github.com/cs231n)
- [ cs231n](https://twitter.com/cs231n)
