
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="CS231n: Deep Learning for Computer Vision">
      
      
      
        <link rel="canonical" href="https://eanyang7.github.io/cs231n/generative-models/">
      
      
      
      
      <link rel="icon" href="../logo.jpg">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.3.1">
    
    
      
        <title>Generative Modeling - CS231n</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-5F8XNH7BCX"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-5F8XNH7BCX",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-5F8XNH7BCX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generative-modeling" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="CS231n" class="md-header__button md-logo" aria-label="CS231n" data-md-component="logo">
      
  <img src="../logo.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CS231n
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generative Modeling
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换为暗黑模式"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="切换为暗黑模式" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="切换为浅色模式"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="切换为浅色模式" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="/cs231n/" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="https://cs231n.github.io/" hreflang="en" class="md-select__link">
              English(源网站)
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EanYang7/cs231n" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EanYang7/cs231n
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href=".." class="md-tabs__link">
          
  
  首页

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../courses/preparation/setup/" class="md-tabs__link">
          
  
  课程

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../assignments/2023/assignment1/" class="md-tabs__link">
          
  
  任务

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="CS231n" class="md-nav__button md-logo" aria-label="CS231n" data-md-component="logo">
      
  <img src="../logo.jpg" alt="logo">

    </a>
    CS231n
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EanYang7/cs231n" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EanYang7/cs231n
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    首页
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            首页
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CS231n 卷积神经网络视觉识别
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    课程
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            课程
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    准备工作
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            准备工作
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/preparation/setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    软件设置
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/preparation/python-numpy-tutorial/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python/Numpy教程
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            神经网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    图像分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/linear-classify/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    线性分类
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/optimization-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    优化：随机梯度下降
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/optimization-2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    反向传播
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2_5" >
        
          <label class="md-nav__link" for="__nav_2_2_5" id="__nav_2_2_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    神经网络
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_2_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2_5">
            <span class="md-nav__icon md-icon"></span>
            神经网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/neural-networks-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    创建结构
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/neural-networks-2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    创建数据和损失
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/neural-networks-3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    学习和评估
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/neural_network/neural-networks-case-study/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    最小神经网络案例研究
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
    
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    卷积神经网络
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            卷积神经网络
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/CNN/convolutional-networks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    结构、卷积/池化层
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/CNN/understanding-cnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    理解与可视化
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../courses/CNN/transfer-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    迁移学习与微调
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    任务
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            任务
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../assignments/2023/assignment1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    任务1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../assignments/2023/assignment2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    任务2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    <li class="md-nav__item">
      <a href="../assignments/2023/assignment3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    任务2
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/EanYang7/cs231n/edit/dev/docs/generative-models.md" title="编辑此页" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/EanYang7/cs231n/raw/dev/docs/generative-models.md" title="查看本页的源代码" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0 8a5 5 0 0 1-5-5 5 5 0 0 1 5-5 5 5 0 0 1 5 5 5 5 0 0 1-5 5m0-12.5C7 4.5 2.73 7.61 1 12c1.73 4.39 6 7.5 11 7.5s9.27-3.11 11-7.5c-1.73-4.39-6-7.5-11-7.5Z"/></svg>
    </a>
  


<h1 id="generative-modeling">Generative Modeling<a class="headerlink" href="#generative-modeling" title="Permanent link">⚓︎</a></h1>
<p>With generative modeling, we aim to learn how to generate new samples from the same distribution of the given training data. Specifically, there are two major objectives: </p>
<ul>
<li>Learn <span class="arithmatex">\(p_{\text{model}}(x)\)</span> that approximates true data distribution <span class="arithmatex">\(p_{\text{data}}(x)\)</span></li>
<li>Sampling new <span class="arithmatex">\(x\)</span> from <span class="arithmatex">\(p_{\text{model}}(x)\)</span></li>
</ul>
<p>The former can be structured as learning how likely a given sample is drawn from a true data distribution; the latter means the model should be able to produce new samples that are similar but not exactly the same as the training samples. One way to judge if the model has learned the correct underlying representation of the training data distribution is the quality of the new samples produced by the trained model. </p>
<p>These objectives can be formulated as density estimation problems. There are two different approaches: </p>
<ul>
<li><strong>Explicit density estimation</strong>: explicitly define and solve for <span class="arithmatex">\(p_{\text{model}}(x)\)</span></li>
<li><strong>Implicit density estimation</strong>: learn model that can sample from <span class="arithmatex">\(p_{\text{model}}(x)\)</span> without explicitly define it </li>
</ul>
<p>The explicit approach can be challenging because it is generally difficult to find an expression for image likelihood function from a high dimensional space. The implicit approach may be preferable in situations that the only interest is to generate new samples. In this case, instead of finding the specific expression of the density function, we can simply training the model to directly sample from the data distribution without going through the process of explicit modeling. </p>
<p><img src="assets/generative-models/taxonomy.svg" width="946"></img></p>
<p>Generative models are widely used in various computer vision tasks. For instance, they are used in super-resolution applications in which the model fills in the details of the low resolution inputs and generates higher resolution images. They are also used for colorization in which greyscale images get converted to color images. </p>
<h1 id="pixelrnn-and-pixelcnn">PixelRNN and PixelCNN<a class="headerlink" href="#pixelrnn-and-pixelcnn" title="Permanent link">⚓︎</a></h1>
<p>PixelRNN and PixelCNN <a href="https://arxiv.org/abs/1601.06759">[van den Oord et al., 2016]</a> are examples of a <strong>fully visible belief network (FVBN)</strong> in which data likelihood function <span class="arithmatex">\(p(x)\)</span> is explicitly modeled given image input <span class="arithmatex">\(x\)</span>: </p>
<p><span class="arithmatex">\( p(x) = p(x_1, x_2, \cdots, x_n) \)</span></p>
<p>where <span class="arithmatex">\(x_1, x_2, \cdots\)</span> are each pixel in the image. In other words, the likelihood of an image (LHS) is the joint likelihood of each pixel in the image (RHS). We then use chain rule to decompose the joint likelihood into product of 1-d distributions: </p>
<p><span class="arithmatex">\( \displaystyle p(x) = \prod_{i=1}^{n} p(x_i \mid x_1, \cdots, x_{i-1}) \)</span></p>
<p>where each distribution in the product gives the probability of <span class="arithmatex">\(i\)</span><sup>th</sup> pixel value given all previous pixels. Choice of pixel ordering (hence "previous") might have implications on computational efficiencies in training and inference. Following is an example of ordering in which each pixel <span class="arithmatex">\(x_i\)</span> (in red) is conditioned on all the previously generated pixels left and above of <span class="arithmatex">\(x_i\)</span> (in blue): </p>
<p><img src="assets/generative-models/ordering.svg" width="200"></img></p>
<p>To train the model, we could try to maximize the defined likelihood of the training data. </p>
<h2 id="pixelrnn">PixelRNN<a class="headerlink" href="#pixelrnn" title="Permanent link">⚓︎</a></h2>
<p>The problem with above naive approach is that the conditional distributions can be extremely complex. To mitigate the difficulty, the PixelRNN instead expresses conditional distribution of each pixel as sequence modeling problem. It uses RNNs (more specifically LSTMs) to model the joint likelihood function <span class="arithmatex">\(p(x_i \mid x_1, \cdots, x_{i-1})\)</span>. The main idea is that we model the dependencies of one pixel on all previous pixels by the hidden state of an RNN. Then the process of feeding previous predicted pixel back to the network to get the next pixel allows us to sequentially generate all pixels of an image. </p>
<p><img src="assets/generative-models/pixelrnn.svg" width="622.02"></img></p>
<p>Another thing that the PixelRNN model does slightly differently is that it defines pixel order diagonally. This allows some level of parallelization, which makes training and generation a bit faster. With this ordering, generation process starts from the top-left corner, and then makes its way down and right until the entire image is produced. </p>
<p><img src="assets/generative-models/diagonal.gif" width="200"></img></p>
<h2 id="pixelcnn">PixelCNN<a class="headerlink" href="#pixelcnn" title="Permanent link">⚓︎</a></h2>
<p>Because the generation process even with this diagonal ordering is still largely sequential, it is expensive to train such model. To achieve further parallelization, instead of taking all previous pixels into consideration, we could instead only model dependencies on pixels in a context region. This gives rise to the PixelCNN model in which a context region is defined by a masked convolution. The receptive field of a masked convolution is an incomplete square around a central pixel (darker blue squares). This ensures that each pixel only depends on the already generated pixels in the region. The paper shows that with enough masked convolutional layers, the effective receptive field is the same as the pixel generation process that directly models dependencies on all previous pixels (all blue squares), like the PixelRNN model. </p>
<p><img src="assets/generative-models/pixelcnn.svg" width="508"></img></p>
<p>Because context region values are known from training images, PixelCNN is faster in training thanks to convolution parallelizations. However, generation is still slow as the process is inherently sequential. For instance, for a <span class="arithmatex">\(32 \times 32\)</span> image, the model needs to perform forward pass <span class="arithmatex">\(1024\)</span> times to generate a single image. </p>
<p><img src="assets/generative-models/pixelcnn_samples.png" width="700"></img></p>
<p>From the generation samples on <a href="https://www.cs.toronto.edu/~kriz/cifar.html"><code>CIFAR-10</code></a> (left) and <a href="https://www.image-net.org/"><code>ImageNet</code></a> (right), we see these models are able to capture the distribution of training data to some extent, yet the generated samples do not look like natural images. Later models like flow based deep generative models are able to strike a better balance between training and generation efficiencies, and generate better quality images. </p>
<p>In summary, PixelRNN and PixelCNN models explicitly compute likelihood, and thus are relatively easy to optimize. The major drawback of these models is the sequential generation process which is time consuming. There have been follow-up efforts on improving PixelCNN performance, ranging from architecture changes to training tricks. </p>
<h1 id="variational-autoencoder">Variational Autoencoder<a class="headerlink" href="#variational-autoencoder" title="Permanent link">⚓︎</a></h1>
<p>We introduce a new latent variable <span class="arithmatex">\(z\)</span> that allows us to decompose the data likelihood as the marginal distribution of the conditional data likelihood with respect to this latent variable <span class="arithmatex">\(z\)</span>: </p>
<p><span class="arithmatex">\( \displaystyle p_{\theta}(x) = \int p_{\theta}(z) \cdot p_{\theta}(x \mid z) ~dz \)</span></p>
<p>In other words, all pixels of an image are independent with each other given latent variable <span class="arithmatex">\(z\)</span>. This makes simultaneous generation of all pixels possible. However, we cannot directly optimize this likelihood expression. Instead, we optimize a lower bound of this expression to approximate the optimization we'd like to perform. </p>
<h2 id="autoencoder">Autoencoder<a class="headerlink" href="#autoencoder" title="Permanent link">⚓︎</a></h2>
<p>On a high-level, the goal of an autoencoder is to learn a lower-dimensional feature representation from un-labeled training data. The "encoder" component of an autoencoder aims at compressing input data into a lower-dimensional feature vector <span class="arithmatex">\(z\)</span>. Then the "decoder" component decodes this feature vector and converts it back to the data in the original dimensional space. </p>
<p><img src="assets/generative-models/autoencoder.svg" width="302"></img></p>
<p>The idea of the dimensionality reduction step is that we want every dimension of the feature vector <span class="arithmatex">\(z\)</span> captures meaningful factors of variation in data. We feed the feature vector into the decoder network and have it learn how to reconstruct the original input data with some pre-defined pixel-wise reconstruction loss (L2 is one of the most common choices). By training an autoencoder model, we hope feature vector <span class="arithmatex">\(z\)</span> eventually encodes the most essential information about possible variables of the data. </p>
<p>Now the autoencoder gives a way to effectively represent the underlying structure of the data distribution, which is one of the objectives of generative modeling. However, since do not know the entire latent space that <span class="arithmatex">\(z\)</span> is in (not every latent feature in the latent space can be decoded into a meaningful image), we are unable to arbitrarily generate new images from an autoencoder. </p>
<h2 id="variational-autoencoder_1">Variational Autoencoder<a class="headerlink" href="#variational-autoencoder_1" title="Permanent link">⚓︎</a></h2>
<p>To be able to sample from the latent space, we take a probabilistic approach to autoencoder models. Assume training data <span class="arithmatex">\(\big\{x^{(i)}\big\}_{i=1}^{N}\)</span> is generated from the distribution of unobserved latent representation <span class="arithmatex">\(z\)</span>. So <span class="arithmatex">\(x\)</span> follows the conditional distribution given <span class="arithmatex">\(z\)</span>; that is, <span class="arithmatex">\(p_{\theta^{\ast}}(x \mid z^{(i)})\)</span>. And <span class="arithmatex">\(z^{(i)}\)</span> follows the prior distribution <span class="arithmatex">\(p_{\theta^{\ast}}(z)\)</span>. In other words, we assume each image <span class="arithmatex">\(x\)</span> is generated by first sampling a new <span class="arithmatex">\(z\)</span> that has a slight different factors of variation and then sampling the image conditionally on that chosen variable <span class="arithmatex">\(z\)</span>. </p>
<p><img src="assets/generative-models/vae_assumptions.svg" width="536"></img></p>
<p>With variational autoencoder <a href="https://arxiv.org/abs/1312.6114">[Kingma and Welling, 2014]</a>, we would like to estimate true parameters <span class="arithmatex">\(\theta^{\ast}\)</span> of both the prior and conditional distributions of the training data. We choose prior <span class="arithmatex">\(p_{\theta}(z)\)</span> to be a simple distribution (e.g. a diagonal/isotropic Gaussian distribution), and use a neural network, denoted as <strong>decoder</strong> network, to decode a latent sample from prior <span class="arithmatex">\(pp_{\theta}(z)\)</span> to a conditional distribution of the image <span class="arithmatex">\(p_{\theta}(x \mid z)\)</span>. We have the data likelihood: </p>
<p><span class="arithmatex">\(\displaystyle p_{\theta}(x) = \int p_{\theta}(z) \cdot p_{\theta}(x \mid z) ~dz \)</span></p>
<p>We note that to train the model, we need to compute the integral which involves computing <span class="arithmatex">\(p(x \mid z)\)</span> for every possible <span class="arithmatex">\(z\)</span>. Hence it is intractably to directly optimize the likelihood expression. We could use Monte Carlo estimation technique but there will incur high variance because of the high dimensionality nature of the density function. If we look at the posterior distribution using the Baye's rule: </p>
<p><span class="arithmatex">\( p_{\theta}(z \mid x) = \dfrac{p_{\theta}(x \mid z) \cdot p_{\theta}(z)}{p_{\theta}(x)} \)</span></p>
<p>we see it is still intractable to compute because <span class="arithmatex">\(p_{\theta}(x)\)</span> shows up in the denominator. </p>
<p>To make it tractable, we instead learn another distribution <span class="arithmatex">\(q_{\phi}(z \mid x)\)</span> that approximates the true posterior distribution <span class="arithmatex">\(p_{\theta}(z \mid x)\)</span>. We denote this approximate distribution as probabilistic <strong>encoder</strong> because given an input image, it produces a distribution over the possible values of latent feature vector <span class="arithmatex">\(z\)</span> from which the image could have been sampled from. This approximate posterior distribution <span class="arithmatex">\(q_{\phi}(z \mid x)\)</span> allows us to derive a lower bound on the data likelihood. We then can optimize the tractable lower bound instead. The goal of the variational inference is to approximate the unknown posterior distribution <span class="arithmatex">\(p_{\theta}(z \mid x)\)</span> from only the observed data. </p>
<h3 id="tractable-lower-bound">Tractable Lower Bound<a class="headerlink" href="#tractable-lower-bound" title="Permanent link">⚓︎</a></h3>
<p>To derive the tractable lower bound, we start from the log likelihood of an observed example: </p>
<p><span class="arithmatex">\( \begin{aligned}
  \log p_{\theta}(x^{(i)}) &amp;= \mathbb{E}_{z \sim q_{\phi}(z \mid x^{(i)})} \Big[\log p_{\theta}(x^{(i)})\Big] \quad \cdots \small\mathsf{(1)} \\
  &amp;= \mathbb{E}_{z} \bigg[\log \frac{p_{\theta}(x^{(i)} \mid z) \cdot p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})}\bigg] \quad \cdots \small\mathsf{(2)} \\
  &amp;= \mathbb{E}_{z} \bigg[\log \bigg(\frac{p_{\theta}(x^{(i)} \mid z) \cdot p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \cdot \frac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})}\bigg)\bigg] \quad \cdots \small\mathsf{(3)} \\
  &amp;= \mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big] - \mathbb{E}_{z} \bigg[\log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)}\bigg] + \mathbb{E}_{z} \bigg[\log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})}\bigg] \quad \cdots \small\mathsf{(4)} \\
  &amp;= \mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big] - D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z)\Big) + D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z \mid x^{(i)})\Big) \quad \cdots \small\mathsf{(5)}
\end{aligned} \)</span></p>
<ul>
<li>
<p>Step <span class="arithmatex">\(\mathrm{(1)}\)</span>: the true data distribution is independent of the estimated posterior <span class="arithmatex">\(q_{\phi}(z \mid x^{(i)})\)</span>; moreover, since <span class="arithmatex">\(q_{\phi}(z \mid x^{(i)})\)</span> is represented by a neural network, we are able to sample from distribution <span class="arithmatex">\(q_{\phi}\)</span>. </p>
</li>
<li>
<p>Step <span class="arithmatex">\(\mathrm{(2)}\)</span>: by the Baye's rule: </p>
</li>
</ul>
<p><span class="arithmatex">\( \begin{aligned}
  &amp; p_{\theta}(z \mid x) = \dfrac{p_{\theta}(x \mid z) \cdot p_{\theta}(z)}{p_{\theta}(x)} \\
  \Longrightarrow \quad &amp; p_{\theta}(x) = \dfrac{p_{\theta}(x \mid z) \cdot p_{\theta}(z)}{p_{\theta}(z \mid x)}
\end{aligned} \)</span></p>
<ul>
<li>
<p>Step <span class="arithmatex">\(\mathrm{(3)}\)</span>: multiplying the expression by <span class="arithmatex">\(1 = \dfrac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})}\)</span></p>
</li>
<li>
<p>Step <span class="arithmatex">\(\mathrm{(4)}\)</span>: by logarithm properties as well as linearity of expectation: </p>
</li>
</ul>
<p><span class="arithmatex">\( \begin{aligned}
  &amp;~ \mathbb{E}_{z} \bigg[\log \bigg(\frac{p_{\theta}(x^{(i)} \mid z) \cdot p_{\theta}(z)}{p_{\theta}(z \mid x^{(i)})} \cdot \frac{q_{\phi}(z \mid x^{(i)})}{q_{\phi}(z \mid x^{(i)})}\bigg)\bigg] \\
  =&amp;~ \mathbb{E}_{z} \bigg[\log \bigg(p_{\theta}(x^{(i)} \mid z) \cdot \frac{p_{\theta}(z)}{q_{\phi}(z \mid x^{(i)})} \cdot \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})}\bigg)\bigg] \\
  =&amp;~ \mathbb{E}_{z} \bigg[\log p_{\theta}(x^{(i)} \mid z) + \log \frac{p_{\theta}(z)}{q_{\phi}(z \mid x^{(i)})} + \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})}\bigg] \\
  =&amp;~ \mathbb{E}_{z} \bigg[\log p_{\theta}(x^{(i)} \mid z) - \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)} + \log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})}\bigg] \\
  =&amp;~ \mathbb{E}_{z} \bigg[\log p_{\theta}(x^{(i)} \mid z)\bigg] - \mathbb{E}_{z}\bigg[\log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z)}\bigg] + \mathbb{E}_{z}\bigg[\log \frac{q_{\phi}(z \mid x^{(i)})}{p_{\theta}(z \mid x^{(i)})}\bigg] \\
\end{aligned} \)</span></p>
<ul>
<li>Step <span class="arithmatex">\(\mathrm{(5)}\)</span>: by definition of the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence</a>. The KL divergence gives a measure of the “distance” between two distributions. </li>
</ul>
<p>We see the first term <span class="arithmatex">\(\mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big]\)</span> involves <span class="arithmatex">\(p_{\theta}(x^{(i)} \mid z)\)</span> that is given by the decoder network. With some tricks, this term can be estimated through sampling. </p>
<p>The second term is the KL divergence between the approximate posterior and the prior (a Gaussian distribution). Assuming the approximate posterior posterior takes on a Gaussian form with diagonal covariance matrix, the KL divergence then has an analytical closed form solution. </p>
<p>The third term is the KL divergence between between the approximate posterior and the true posterior. Even though we it is intractable to computer, by non-negativity of KL divergence, we know this term is non-negative. </p>
<p>Therefore we obtain the tractable lower bound of log likelihood of the data: </p>
<p><span class="arithmatex">\( \log p_{\theta}(x^{(i)}) = \underbrace{\mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big] - D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z)\Big)}_{\mathcal{L}(x^{(i)}; \theta, \phi)} + \underbrace{D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z \mid x^{(i)})\Big)}_{\geqslant 0} \)</span></p>
<p>We note that <span class="arithmatex">\(\mathcal{L}(x^{(i)}; \theta, \phi)\)</span>, as known as the <strong>evidence lower bound (ELBO)</strong>, is differentiable, hence we can apply gradient descent methods to optimize the lower bound. </p>
<p>The lower bound can also be interpreted as encoder component <span class="arithmatex">\(D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z)\Big)\)</span> that seeks to approximate posterior distribution close to prior; and decoder component <span class="arithmatex">\(\mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big]\)</span> that concerns with reconstructing the original input data. </p>
<h3 id="training">Training<a class="headerlink" href="#training" title="Permanent link">⚓︎</a></h3>
<p><img src="assets/generative-models/vae_training.svg" width="702"></img></p>
<p>For a given input, we first use the encoder network to to generate the mean <span class="arithmatex">\(\mu_{z \mid x}\)</span> and variance <span class="arithmatex">\(\Sigma_{z \mid x}\)</span> of the approximate posterior Gaussian distribution. Notice that here <span class="arithmatex">\(\Sigma_{z \mid x}\)</span> is represented by a vector instead of a matrix because the approximate posterior is assumed to have a diagonal covariance matrix. Then we can compute the gradient of the KL divergence term <span class="arithmatex">\(D_{\mathrm{KL}} \Big(q_{\phi}(z \mid x^{(i)}) \parallel p_{\theta}(z)\Big)\)</span> as it has an analytical solution. </p>
<p>Next we compute the gradient of the expectation term <span class="arithmatex">\(\mathbb{E}_{z} \Big[\log p_{\theta}(x^{(i)} \mid z) \Big]\)</span>. Since <span class="arithmatex">\(p_{\theta}(x^{(i)} \mid z)\)</span> is represented by the decoder network, we need to sample <span class="arithmatex">\(z\)</span> from the approximate posterior <span class="arithmatex">\(\mathcal{N}(\mu_{z \mid x}, \Sigma_{z \mid x})\)</span>. However, since <span class="arithmatex">\(z\)</span> is not part of the computation graph, we won't be able to find the gradient of the expression that entails the sampling process. To solve this problem, we perform <strong>reparameterization</strong>. Specifically, we take advantage of the Gaussian distribution assumption, and sample <span class="arithmatex">\(\varepsilon \sim \mathcal{N}(0, I)\)</span>. Then we represent <span class="arithmatex">\(z\)</span> as <span class="arithmatex">\(z = \mu_{z \mid x} + \varepsilon \Sigma_{z \mid x}\)</span>. This way <span class="arithmatex">\(z\)</span> has the same Gaussian distribution as before; moreover, since <span class="arithmatex">\(\varepsilon\)</span> is seen as an input to the computation graph, and both <span class="arithmatex">\(\mu_{z \mid x}\)</span> and <span class="arithmatex">\(\Sigma_{z \mid x}\)</span> are part of the computation graph, the sampling process now becomes differentiable. </p>
<p>Lastly, we use the decoder network to produce the pixel-wise conditional distribution <span class="arithmatex">\(p_{\theta}(x \mid z)\)</span>. We are now able to perform the maximum likelihood of the original input. In practice, L2 distance between the predicted image and the actual input image is commonly used. </p>
<p>For every minibatch of input data, we compute the forward pass and then perform the back-propagation. </p>
<h3 id="inference">Inference<a class="headerlink" href="#inference" title="Permanent link">⚓︎</a></h3>
<p><img src="assets/generative-models/vae_inference.svg" width="647"></img></p>
<p>We take a sample <span class="arithmatex">\(z\)</span> from the prior distribution <span class="arithmatex">\(p_{\theta}(z)\)</span> (e.g. a Gaussian distribution), then feed the sample into the trained decoder network to obtain the conditional distributions <span class="arithmatex">\(p_{\theta}(x \mid z)\)</span>. Lastly we sample a new image from the conditional distribution. </p>
<h3 id="generated-samples">Generated Samples<a class="headerlink" href="#generated-samples" title="Permanent link">⚓︎</a></h3>
<p>Since we assumed diagonal prior for <span class="arithmatex">\(z\)</span>, components of latent variable are independent of each other. This means different dimensions of <span class="arithmatex">\(z\)</span> encode interpretable factors of variation. </p>
<p><img src="assets/generative-models/vae_interpretations_mnist.png" height="320"></img></p>
<p>After training the model using a <span class="arithmatex">\(2\)</span>-dimensional latent variable <span class="arithmatex">\(z\)</span> on <a href="https://yann.lecun.com/exdb/mnist/"><code>MNIST</code></a>, we discover that varying the samples of <span class="arithmatex">\(z\)</span> would induce interpretable variations in the image space. For instance, one possible interpretation would be that <span class="arithmatex">\(z_1\)</span> morphs digit <code>6</code> to <code>9</code> through <code>7</code>, and <span class="arithmatex">\(z_2\)</span> is related to the orientation of digits. </p>
<p><img src="assets/generative-models/vae_interpretations_face.png" height="320"></img></p>
<p>Similarly, we also find that dimensions of latent variable <span class="arithmatex">\(z\)</span> can be interpretable after training the model on head pose dataset. For instance, it appears <span class="arithmatex">\(z_1\)</span> encodes degree of smile and <span class="arithmatex">\(z_2\)</span> encodes head pose orientation. </p>
<p><img src="assets/generative-models/vae_samples.png" width="750"></img></p>
<p>From above generation samples on <a href="https://www.cs.toronto.edu/~kriz/cifar.html"><code>CIFAR-10</code></a> (left) and labeled face images (right), we see newly generated images are similar to the original ones. However, these generated images are still blurry and generating high quality images is an active area for research. </p>
<h1 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)<a class="headerlink" href="#generative-adversarial-networks-gans" title="Permanent link">⚓︎</a></h1>
<p>We would like to train a model to directly generate high quality samples without modeling any explicit density function <span class="arithmatex">\(p(x)\)</span>. With GAN <a href="https://arxiv.org/abs/1406.2661">[Goodfellow et al., 2014]</a>, our goal is to train a <strong>generator network</strong> to learn transformation from a simple distribution (e.g. random noise) that we can easily sample from to the high-dimensional training distribution followed by the data. The challenge is that because we do not model any data distribution, we don't have the mapping between random sample <span class="arithmatex">\(z\)</span> to a training image <span class="arithmatex">\(x\)</span>. This means we cannot directly train the model with supervised reconstruction loss. </p>
<p>To overcome this challenge, we recognize the general objective that all the images generated from the latent space of <span class="arithmatex">\(z\)</span> should exhibit "realness". In other words, all the generated images should look like they belong to the original training data. To formulate this general objective into a learning objective, we introduce another <strong>discriminator network</strong> that learns to identify whether an image is from the training data distribution or not. Specifically, the discriminator network performs a two-class classification task in which an input image feeds into the network and a label indicating if the input image is from the training data distribution or is produced by the generated network. </p>
<p>We then can use the output from the discriminator network to compute gradient and perform back-propagation to the generator network to gradually improve the image generation process. Overtime, learning signal from the discriminator will inform the generator on how to produce more "realistic" samples. Similarly, as generated images from the generator become more and more close to the real training data, the discriminator adapt its decision boundary to fit the training data distribution better. The discriminator effectively learns to model the data distribution without explicitly defining it. </p>
<p><img src="assets/generative-models/gan.svg" width="553"></img></p>
<p>In summary: </p>
<ul>
<li><strong>discriminator network</strong>: try to distinguish between real and fake images</li>
<li><strong>generator network</strong>: try to fool the discriminator by generating real-looking images</li>
</ul>
<h2 id="training-gans">Training GANs<a class="headerlink" href="#training-gans" title="Permanent link">⚓︎</a></h2>
<p>Training GAN can be formulated as the minimax optimization of a two-player adversarial game. Assume that the discriminator outputs likelihood in <span class="arithmatex">\((0,1)\)</span> of real image, the objective function is the following: </p>
<p><span class="arithmatex">\( \displaystyle \min_{\theta_g} \max_{\theta_d} \Big\{\mathbb{E}_{x \sim p_{\mathrm{data}}}\big[\log \underbrace{D_{\theta_d}(x)}_{\mathsf{(1)}}\big] + \mathbb{E}_{z \sim p(z)}\big[\log\big(1 - \underbrace{D_{\theta_d}(G_{\theta_g}(z))\big)}_{\mathsf{(2)}}\big]\Big\} \)</span></p>
<ul>
<li><span class="arithmatex">\(\mathsf{(1)}\)</span>: <span class="arithmatex">\(D_{\theta_d}(x)\)</span> is the discriminator output (score) for real data <span class="arithmatex">\(x\)</span></li>
<li><span class="arithmatex">\(\mathsf{(2)}\)</span>: <span class="arithmatex">\(D_{\theta_d}(G_{\theta_g}(z))\big)\)</span> is the discriminator output (score) for generated fake data <span class="arithmatex">\(G(z)\)</span></li>
</ul>
<p>The inner maximization is the discriminator objective. The discriminator aims to find maximizer <span class="arithmatex">\(\theta_g\)</span> such that real data <span class="arithmatex">\(D(x)\)</span> is close to <span class="arithmatex">\(1\)</span> (real) while generated fake data <span class="arithmatex">\(D(G(z))\)</span> is close to <span class="arithmatex">\(0\)</span> (fake). </p>
<p>The outer minimization is the generator objective. The generator aims to find minimizer <span class="arithmatex">\(\theta_g\)</span> such that generated fake data <span class="arithmatex">\(D(G(z))\)</span> is close to <span class="arithmatex">\(1\)</span> (real). This means the generator seeks to fool discriminator into thinking that generated fake data <span class="arithmatex">\(D(G(z))\)</span> is real. </p>
<p>Naively, we could alternate between maximization and minimization by performing <strong>gradient ascent on discriminator</strong>: </p>
<p><span class="arithmatex">\( \displaystyle \max_{\theta_d} \Big\{\mathbb{E}_{x \sim p_{\mathrm{data}}}\big[\log D_{\theta_d}(x)\big] + \mathbb{E}_{z \sim p(z)}\big[\log \big(1 - D_{\theta_d}(G_{\theta_g}(z))\big)\big]\Big\} \)</span></p>
<p>and <strong>gradient descent on generator</strong>: </p>
<p><span class="arithmatex">\( \displaystyle \min_{\theta_g} \Big\{\mathbb{E}_{z \sim p(z)}\big[\log \big(1 - D_{\theta_d}(G_{\theta_g}(z))\big)\big]\Big\} \)</span></p>
<p>However, we note that when a sample is likely fake—hence <span class="arithmatex">\(D(G(z))\)</span> is small, expression <span class="arithmatex">\(\log \big(1 - D_{\theta_d}(G_{\theta_g}(z))\big)\)</span> in the generator objective function has small derivative with respect to <span class="arithmatex">\(D(G(z))\)</span>. This means in the beginning of the training, the gradient of the generator objective function is small; that is updates to parameters <span class="arithmatex">\(\theta_g\)</span> are small. Conversely, the updates are large (strong gradient signal) when samples are already realistic (<span class="arithmatex">\(D(G(z))\)</span> is large). This creates an unfavorable situation as ideally we would hope the generator is able to learn fast when the discriminator outsmarts the generator. </p>
<p><img src="assets/generative-models/gan_training.svg" width="498"></img></p>
<p>To remedy this problem, we now maximize likelihood of the discriminator being wrong, as opposed to minimizing the likelihood of it being correct: </p>
<p><span class="arithmatex">\( \displaystyle \max{\theta_g} \Big\{\mathbb{E}_{z \sim p(z)}\big[\log D_{\theta_d}\big(G_{\theta_g}(z)\big)\big]\Big\} \)</span></p>
<p>The objective remains unchanged, yet there will be higher gradient signal to the generator for unrealistic samples (in the eyes of the discriminator), which improves training performance. </p>
<blockquote>
<p><strong>for</strong> number of training iterations <strong>do</strong>:<br />
&nbsp;&nbsp; <strong>for</strong> <span class="arithmatex">\(k\)</span> steps <strong>do</strong>:<br />
&nbsp;&nbsp;&nbsp;&nbsp; - Sample minibatch of <span class="arithmatex">\(m\)</span> noise samples <span class="arithmatex">\(\{z^{(1)}, \cdots, z^{(m)}\}\)</span> from noise prior <span class="arithmatex">\(p(z)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp; - Sample minibatch of <span class="arithmatex">\(m\)</span> samples <span class="arithmatex">\(\{x^{(1)}, \cdots, x^{(m)}\}\)</span> from data generating distribution <span class="arithmatex">\(p_{\mathrm{data}}(x)\)</span><br />
&nbsp;&nbsp;&nbsp;&nbsp; - Update the discriminator by ascending its stochastic gradient: </p>
<p><span class="arithmatex">\( \displaystyle \nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^{m} \Big[\log D_{\theta_d}(x^{(i)}) + \log \big(1 - D_{\theta_d}(G_{\theta_g}(z^{(i)}))\big)\Big] \)</span><br />
&nbsp;&nbsp; <strong>end for</strong><br />
&nbsp;&nbsp; - Sample minibatch of <span class="arithmatex">\(m\)</span> samples <span class="arithmatex">\(\{x^{(1)}, \cdots, x^{(m)}\}\)</span> from data generating distribution <span class="arithmatex">\(p_{\mathrm{data}}(x)\)</span><br />
&nbsp;&nbsp; - Update the generator by ascending its stochastic gradient of the improved objective: </p>
<p><span class="arithmatex">\( \displaystyle \nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^{m} \log D_{\theta_d}(G_{\theta_g}(z^{(i)})) \)</span> <br />
<strong>end for</strong>  </p>
</blockquote>
<p>Here <span class="arithmatex">\(k \geqslant 1\)</span> is the hyper-parameter and there is not best rule as to the value of <span class="arithmatex">\(k\)</span>. In general, GANs are difficult to train, and followup work like Wasserstein GAN <a href="https://arxiv.org/abs/1701.07875">[Arjovsky et al., 2017]</a> and BEGAN <a href="https://arxiv.org/abs/1703.10717">[Berthelot et al., 2017]</a> sets out to achieve better training stability. </p>
<h2 id="inference_1">Inference<a class="headerlink" href="#inference_1" title="Permanent link">⚓︎</a></h2>
<p>After training, we use the generator network to generate images. Specifically, we first draw a sample <span class="arithmatex">\(z\)</span> from noise prior <span class="arithmatex">\(p(z)\)</span>; then we feed the sampled <span class="arithmatex">\(z\)</span> into the generator network. The output from the network gives us an image that is similar to the training images. </p>
<h2 id="generated-samples_1">Generated Samples<a class="headerlink" href="#generated-samples_1" title="Permanent link">⚓︎</a></h2>
<p><img src="assets/generative-models/gan_samples.png" width="900"></img></p>
<p>From the generated samples, we see GAN can generate high quality samples, indicating the model does not simply memorize exact images from the training data. Training sets from left to right: <a href="https://yann.lecun.com/exdb/mnist/"><code>MNIST</code></a>, <code>Toronto Face Dataset (TFD)</code>, <a href="https://www.cs.toronto.edu/~kriz/cifar.html"><code>CIFAR-10</code></a>. The highlighted columns show the nearest training example of the neighboring generated sample. </p>
<p>There have been numerous followup studies on improving sample quality, training stability, and other aspects of GANs. The ICLR 2016 paper <a href="https://arxiv.org/abs/1511.06434">[Radford et al., 2015]</a> proposed deep convolutional networks and other architecture features (deep convolutional generative adversarial networks, or DCGANs) to achieve better image quality and training stability: </p>
<ul>
<li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).</li>
<li>Use batchnorm in both the generator and the discriminator.</li>
<li>Remove fully connected hidden layers for deeper architectures.</li>
<li>Use ReLU activation in generator for all layers except for the output, which uses Tanh.</li>
<li>Use LeakyReLU activation in the discriminator for all layers.</li>
</ul>
<p><img src="assets/generative-models/dcgan_examples.png" width="621"></img></p>
<p>Generated samples from DCGANs trained on <a href="https://www.yf.io/p/lsun"><code>LSUN</code></a> bedrooms dataset show promising improvements as the model can produce high resolution and high quality images without memorizing (overfitting) training examples. </p>
<p>Similar to VAE, we are also able to find structures in the latent space and meaningfully interpolate random points in the latent space. This means we observe smooth semantic changes to the image generations along any direction of the manifold, which suggests that model has learned relevant  representations (as opposed to memorization). </p>
<p><img src="assets/generative-models/dcgan_transitions.png" width="621"></img></p>
<p>The above figure shows smooth transitions between a series of <span class="arithmatex">\(9\)</span> random points in the latent space. Every image in the interpolation reasonably looks like a bedroom. For instance, generated samples in the <span class="arithmatex">\(6\)</span><sup>th</sup> row exhibit transition from a room without a window to a room with a large window; in the <span class="arithmatex">\(10\)</span><sup>th</sup>, an TV-alike object morphs into a window. </p>
<p>Additionally, we can also perform arithmetic on <span class="arithmatex">\(z\)</span> vectors in the latent space. By averaging the <span class="arithmatex">\(z\)</span> vector for three exemplary generated samples of different visual concepts, we see consistent and stable generations that semantically obeyed the arithmetic. </p>
<p><img src="assets/generative-models/dcgan_interpretations_latent_1.png" width="649"></img></p>
<p><img src="assets/generative-models/dcgan_interpretations_latent_2.png" width="649"></img></p>
<p>Arithmetic is performed on the mean vectors and the resulting vector feeds into the generator to produce the center sample on the right hand side. The remaining samples around the center are produced by adding uniform noise in <span class="arithmatex">\([-0.25, 0.25]\)</span> to the vector. </p>
<p><img src="assets/generative-models/dcgan_interpretations_image_1.png" width="399"></img></p>
<p><img src="assets/generative-models/dcgan_interpretations_image_2.png" width="399"></img></p>
<p>We note that same arithmetic performed pixel-wise in the image space does not behave similarly, as it
only yields in noise overlap due to misalignment. Therefore latent representations learned by the model and associated vector arithmetic have the potential to compactly model conditional generative process of complex image distributions.</p>
<h2 id="other-variants">Other Variants<a class="headerlink" href="#other-variants" title="Permanent link">⚓︎</a></h2>
<ul>
<li>new loss function (LSGAN): <a href="https://arxiv.org/abs/1611.04076">Mao et al., Least Squares Generative Adversarial Networks, 2016</a></li>
<li>new training methods: <ul>
<li>Wasserstein GAN: <a href="https://arxiv.org/abs/1701.07875">Arjovsky et al., Wasserstein GAN, 2017</a></li>
<li>Improved Wasserstein GAN: <a href="https://arxiv.org/abs/1704.00028">Gulrajani et al., Improved Training of Wasserstein GANs, 2017</a></li>
<li>Progressive GAN: <a href="https://arxiv.org/abs/1710.10196">Karras et al., Progressive Growing of GANs for Improved Quality, Stability, and Variation, 2017</a></li>
</ul>
</li>
<li>source-to-target domain transfer (CycleGAN): <a href="https://arxiv.org/abs/1703.10593">Zhu et al., Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017</a></li>
<li>text-to-image synthesis: <a href="https://arxiv.org/abs/1605.05396">Reed et al., Generative Adversarial Text to Image Synthesis, 2016</a></li>
<li>image-to-image translation (Pix2pix): <a href="https://arxiv.org/abs/1611.07004">Isola et al., Image-to-Image Translation with Conditional Adversarial Networks, 2016</a></li>
<li>high-resolution and high-quality generations (BigGAN): <a href="https://arxiv.org/abs/1809.11096">Brock et al., Large Scale GAN Training for High Fidelity Natural Image Synthesis, 2018</a></li>
<li>scene graphs to GANs: <a href="https://arxiv.org/abs/1804.01622">Johnson et al., Image Generation from Scene Graphs, 2018</a></li>
<li>benchmark for generative models: <a href="https://arxiv.org/abs/1904.01121">Zhou, Gordon, Krishna et al., HYPE: Human eYe Perceptual Evaluations, 2019</a></li>
<li>many more: <a href="https://github.com/hindupuravinash/the-gan-zoo">"the GAN zoo"</a></li>
</ul>

  <hr>
<div class="md-source-file">
  <small>
    
      最后更新:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 16, 2023</span>
      
        <br>
        创建日期:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">September 16, 2023</span>
      
    
  </small>
</div>


  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        对当前页面有任何疑问吗？
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M5 9v12H1V9h4m4 12a2 2 0 0 1-2-2V9c0-.55.22-1.05.59-1.41L14.17 1l1.06 1.06c.27.27.44.64.44 1.05l-.03.32L14.69 8H21a2 2 0 0 1 2 2v2c0 .26-.05.5-.14.73l-3.02 7.05C19.54 20.5 18.83 21 18 21H9m0-2h9.03L21 12v-2h-8.79l1.13-5.32L9 9.03V19Z"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 15V3h4v12h-4M15 3a2 2 0 0 1 2 2v10c0 .55-.22 1.05-.59 1.41L9.83 23l-1.06-1.06c-.27-.27-.44-.64-.44-1.06l.03-.31.95-4.57H3a2 2 0 0 1-2-2v-2c0-.26.05-.5.14-.73l3.02-7.05C4.46 3.5 5.17 3 6 3h9m0 2H5.97L3 12v2h8.78l-1.13 5.32L15 14.97V5Z"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              感谢您的反馈！
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              感谢您的反馈！请点击这里<a href="https://github.com/EanYang7/cs231n/issues" target="_blank" rel="noopener">这里</a>提供问题反馈.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/EanYang7/cs231n" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.top", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.path", "toc.follow", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>