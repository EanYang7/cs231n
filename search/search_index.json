{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"CS231n \u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u89c6\u89c9\u200b\u8bc6\u522b","text":"<p>\u200b\u8bfe\u7a0b\u200b\u5730\u5740\u200b</p> <p>\u200b\u8fd9\u4e9b\u200b\u7b14\u8bb0\u200b\u662f\u200b\u4e0e\u200b\u65af\u5766\u798f\u5927\u5b66\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\u8bfe\u7a0b\u200bCS231n\uff1a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u89c6\u89c9\u200b\u8bc6\u522b\u200b\u76f8\u5173\u200b\u7684\u200b\u3002 \u200b\u5982\u200b\u6709\u200b\u95ee\u9898\u200b/\u200b\u5173\u6ce8\u200b/\u200b\u6f0f\u6d1e\u200b\u62a5\u544a\u200b\uff0c\u200b\u8bf7\u200b\u76f4\u63a5\u200b\u5411\u200bgit repo\u200b\u63d0\u4ea4\u200bpull request\u3002</p>"},{"location":"#2023","title":"2023\u200b\u5e74\u200b\u6625\u5b63\u200b\u4f5c\u4e1a\u200b\u4f5c\u4e1a\u200b#1: \u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff0ckNN\uff0cSVM\uff0cSoftmax\uff0c\u200b\u5168\u200b\u8fde\u63a5\u200b\u795e\u7ecf\u7f51\u7edc\u200bFully Connected Neural Network\u200b\u4f5c\u4e1a\u200b#2: \u200b\u5168\u200b\u8fde\u63a5\u200b\u548c\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\uff0c\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\uff0cdropout\uff0cPytorch\u200b\u548c\u200b\u7f51\u7edc\u200b\u53ef\u89c6\u5316\u200b\u4f5c\u4e1a\u200b#3\uff1a\u200b\u4f7f\u7528\u200bRNN\u200b\u548c\u200bTransformer\u200b\u8fdb\u884c\u200b\u56fe\u50cf\u200b\u5b57\u5e55\u200bImage Captioning\u200b\u751f\u6210\u200b\uff0c\u200b\u7f51\u7edc\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u751f\u6210\u200b\u5bf9\u6297\u200b\u7f51\u7edc\u200bGenerative Adversarial Networks\uff0c\u200b\u81ea\u200b\u76d1\u7763\u200b\u5bf9\u6bd4\u200b\u5b66\u4e60\u200b Self-Supervised Contrastive Learning","text":"<p> <p></p> <p> <p> <p></p>"},{"location":"#0","title":"\u6a21\u5757\u200b0\uff1a\u200b\u51c6\u5907\u200b\u8f6f\u4ef6\u200b\u8bbe\u7f6e\u200bPython / Numpy\u200b\u6559\u7a0b\u200b(\u200b\u4f7f\u7528\u200bJupyter\u200b\u548c\u200bColab)","text":"<p> <p></p> <p> <p></p>"},{"location":"#1","title":"\u6a21\u5757\u200b1\uff1a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff1a\u200b\u6570\u636e\u200b\u9a71\u52a8\u200b\u65b9\u6cd5\u200b\uff0ck-\u200b\u8fd1\u90bb\u200bk-Nearest Neighbor\uff0c\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u5206\u5272\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\uff1a\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200bSVM\uff0cSoftmax\u200b\u4f18\u5316\u200b\uff1a\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200bStochastic Gradient Descent\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200bBackpropagation\uff0c\u200b\u76f4\u89c9\u200bIntuitions\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7b2c\u200b1\u200b\u90e8\u5206\u200b\uff1a\u200b\u8bbe\u7f6e\u200b\u67b6\u6784\u200barchitecture\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7b2c\u200b2\u200b\u90e8\u5206\u200b\uff1a\u200b\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u548c\u200b\u635f\u5931\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7b2c\u200b3\u200b\u90e8\u5206\u200b\uff1a\u200b\u5b66\u4e60\u200b\u548c\u200b\u8bc4\u4f30\u200b\u5c06\u200b\u5176\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\uff1a\u200b\u6700\u5c0f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6848\u4f8b\u200b\u7814\u7a76","text":"<p> <p>    L1/L2\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u8d85\u200b\u53c2\u6570\u200b\u641c\u7d22\u200bhyperparameter search\uff0c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200bcross-validation   </p> <p></p> <p> <p> \u200b\u53c2\u6570\u200b\u5316\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u504f\u5dee\u200bbias\u200b\u6280\u5de7\u200b\uff0c\u200b\u94f0\u94fe\u200b\u635f\u5931\u200bhinge loss\uff0c\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200bcross-entropy loss\uff0cL2\u200b\u6b63\u5219\u200b\u5316\u200bregularization\uff0c\u200b\u7f51\u7edc\u200b\u6f14\u793a\u200b   </p> <p></p> <p></p> <p> <p> \u200b\u4f18\u5316\u200b\u7a7a\u95f4\u200boptimize landscapes\uff0c\u200b\u5c40\u90e8\u200b\u641c\u7d22\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u5206\u6790\u200b/\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b   </p> <p></p> <p> <p> \u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u89e3\u91ca\u200bchain rule interpretation\uff0c\u200b\u5b9e\u503c\u200b\u7535\u8def\u200breal-valued circuits\uff0c\u200b\u68af\u5ea6\u200b\u6d41\u200b\u6a21\u5f0f\u200bpatterns in gradient flow   </p> <p></p> <p> <p> \u200b\u751f\u7269\u200b\u795e\u7ecf\u5143\u200bbiological neuron\u200b\u6a21\u578b\u200b\uff0c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\uff0c\u200b\u8868\u793a\u200b\u80fd\u529b\u200brepresentational power   </p> <p></p> <p> <p> \u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200bbatch normalization\uff0c\u200b\u6b63\u5219\u200b\u5316\u200bregularization (L2/dropout)\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b   </p> <p></p> <p> <p> \u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\uff0c\u200b\u5408\u7406\u6027\u200bsanity\u200b\u68c0\u67e5\u200b\uff0c\u200b\u770b\u62a4\u200bbabysitting\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u52a8\u91cf\u200bmomentum\uff08+ nesterov\uff09\uff0c\u200b\u4e8c\u9636\u200bsecond-order\u200b\u65b9\u6cd5\u200b\uff0cAdagrad / RMSprop\uff0c\u200b\u8d85\u200b\u53c2\u6570\u200bhyperparameter\u200b\u4f18\u5316\u200b\uff0c\u200b\u6a21\u578b\u200b\u96c6\u6210\u200bensembles   </p> <p></p> <p> <p> \u200b\u6700\u5c0f\u200b2D\u200b\u73a9\u5177\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b   </p> <p></p>"},{"location":"#2","title":"\u6a21\u5757\u200b2\uff1a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff1a\u200b\u67b6\u6784\u200b\uff0c\u200b\u5377\u79ef\u200b/\u200b\u6c60\u5316\u200bPooling\u200b\u5c42\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7406\u89e3\u200b\u548c\u200b\u53ef\u89c6\u5316\u200b \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200bTransfer Learning\u200b\u548c\u200b\u5fae\u8c03\u200bFine-tuning\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc","text":"<p> <p> \u200b\u5c42\u200b\uff0c\u200b\u7a7a\u95f4\u200b\u6392\u5217\u200bspatial arrangement\uff0c\u200b\u5c42\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5c42\u200b\u5927\u5c0f\u200b\u6a21\u5f0f\u200blayer sizing patterns\uff0cAlexNet / ZFNet / VGGNet\u200b\u6848\u4f8b\u200b\u7814\u7a76\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u8003\u8651\u200b\u56e0\u7d20\u200bcomputational considerations   </p> <p></p> <p> <p> tSNE\u200b\u5d4c\u5165\u200b\uff0cdeconvnets\uff0c\u200b\u6570\u636e\u200b\u68af\u5ea6\u200b\uff0cfooling ConvNets\uff0c\u200b\u4eba\u7c7b\u200b\u6bd4\u8f83\u200bhuman comparisons   </p> <p></p> <p> <p></p>"},{"location":"#_1","title":"\u5b66\u751f\u200b\u6295\u7a3f\u200b\u5c06\u200b\u8bfe\u7a0b\u200b\u9879\u76ee\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u51fa\u7248\u7269\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200bRecurrent Neural Networks","text":"<p> <p></p> <p> <p></p>   \u200b               \u200b                cs231n "},{"location":"Readme/","title":"Readme","text":"<p>\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u90e8\u7f72\u200b\u5728\u200b\u672c\u5730\u200b\uff0c\u200b\u65b9\u4fbf\u200b\u968f\u65f6\u200b\u67e5\u770b\u200b\u548c\u200b\u505a\u200b\u7b14\u8bb0\u200b</p>"},{"location":"adversary-attacks/","title":"Adversary attacks","text":"<p>Table of contents:</p>"},{"location":"adversary-attacks/#adversarial-attacks","title":"Adversarial Attacks","text":"<ul> <li>Poisoning Attack vs. Evasion Attack</li> <li>White-box Attack vs. Black-box Attack</li> <li>Adversarial Goals</li> </ul>"},{"location":"adversary-attacks/#adversarial-examples","title":"Adversarial Examples","text":"<ul> <li>Fast Gradient Sign Method (FGSM)</li> <li>One-Pixel Attack</li> </ul>"},{"location":"adversary-attacks/#defense-strategies-against-adversarial-examples","title":"Defense Strategies against Adversarial Examples","text":"<ul> <li>Adversarial Training</li> <li>Defensive Distillation</li> <li>Denoising</li> </ul>"},{"location":"adversary-attacks/#adversarial-attacks_1","title":"Adversarial Attacks","text":"<p>In class, we have seen examples where image classification models can be fooled with adversarial examples. By adding small and often imperceptible perturbations to images, these adversarial attacks can deceive a deep learning model to label the modified image as a completely different class. In this note, we will give an overview of the various types of adversarial attacks on machine learning models, discuss several representative methods for generating adversarial examples, as well as some possible defense methods and mitigation strategies against such adversarial attacks.</p> <p>Adversary. In computer security, the term \u201cadversary\u201d refers to people or machines that attempt to penetrate or corrupt a computer network or system. In the context of machine learning and deep learning, adversaries can use a variety of attack methods to disrupt a machine learning model, and cause it to behave erratically (e.g. to misclassify a dog image as a cat image). In general, attacks can happen either during model training (known as a \u201cpoisoning\u201d attack) or after the model has finished training (an \u201cevasion\u201d attack).</p>"},{"location":"adversary-attacks/#poisoning-attack-vs-evasion-attack","title":"Poisoning Attack vs. Evasion Attack","text":"<p>Poisoning attack. A poisoning attack involves polluting a machine learning model's training data. Such attacks take place during the training time of the machine learning model, when an adversary presents data that is intentionally mislabeled to the model, therefore instilling misleading knowledge, and will eventually cause the model to make inaccurate predictions at test time. Poisoning attacks require that an adversary has access to the model\u2019s training data, and is able to inject misleading data into the training set. To make the attacks less noticeable, the adversary may decide to slowly introduce ill-intentioned samples over an extended period of time.</p> <p>An example of a poisoning attack took place in 2016, when Microsoft launched a Twitter chatbot, Tay. Tay was designed to mimic the language patterns of a 18- to 24- year-old in the U.S. for entertainment purposes, to engage people through \u201ccasual and playful conversation\u201d, and to learn from its conversations with human users on Twitter. However, soon after its launch, a vulnerability in Tay was exploited by some adversaries, who interacted with Tay using profane and offensive language. The attack caused the chatbot to learn and internalize inappropriate language. The more Tay engaged with adversarial users, the more offensive Tay\u2019s tweets became. As a result, Tay was quickly shut down by Microsoft, only 16 hours after its launch.</p> <p>Evasion attack. In evasion attacks, the adversary tries to evade or fool the system by adjusting malicious samples during the testing phase. Compared with poisoning attacks, evasion attacks are more common, and easier to conduct. One main reason is that with evasion attacks, adversaries don\u2019t necessarily need to access the training data, nor inject bad data into the training process of the model.</p>"},{"location":"adversary-attacks/#white-box-vs-black-box-attacks","title":"White-box vs. Black-box Attacks","text":"<p>The evasion attacks discussed above occur during the testing phase of the model. The effectiveness of such attacks depends on the amount of information available to the adversary about the model. Before we dive into the various methods for generating adversarial examples, let\u2019s first briefly discuss the differences between white-box attacks and black-box attacks, and understand the various adversarial goals.</p> <p>White-box attack. In a white-box attack, the adversary is assumed to have total knowledge about the model, such as the model architecture, number of layers, the weights of the final trained model, etc. The adversary also has knowledge on the model\u2019s training process, such as the optimization algorithm (e.g. Adam, RMSProp etc.) that is used, the data that the model is trained on, the distribution of the training data, and the model\u2019s performance on the training data. It can be very dangerous if the adversary is able to identify the feature space where the model has a high error rate, and use that information to construct adversarial examples and exploit the model. The more the adversary knows, the more severe the attacks can be, and so are the consequences.</p> <p>Black-box attack. On the contrary, in black-box attacks, the adversary assumes no knowledge about the model. Instead of constructing adversarial examples based on prior knowledge, the adversary exploits a model by providing a series of carefully crafted inputs and observing outputs. Through trial and error, the attacks may eventually be successful in misleading the model to make the wrong predictions.</p>"},{"location":"adversary-attacks/#adversarial-goals","title":"Adversarial Goals","text":"<p>The goals of the adversarial attacks can be broadly categorized as follows: - Confidence Reduction. The adversary aims to reduce the model\u2019s confidence in its predictions, which does not necessarily lead to the wrong class output. For example, due to the adversarial attack, a model which originally classifies an image of a cat with high probability ends up outputting a lower probability for the same image and class pair. - Untargeted Misclassification. The adversary tries to misguide the model to predict any of the incorrect classes. For example, when presented with an image of a cat, the model outputs any class that is non-cat (e.g. dog, airplane, computer, etc.). - Targeted Misclassification. The adversary tries to misguide the model to output a particular class other than the true class. For example, when presented with an image of a cat, the model is forced to classify it as a dog image, where the output class of dog is specified by the adversary.</p> <p>Generally speaking, targeted attacks are more sophisticated than untargeted attacks, which are in turn more difficult than confidence reduction.</p>"},{"location":"adversary-attacks/#adversarial-examples_1","title":"Adversarial Examples","text":"<p>In CS231n, we mainly focus on examples of adversarial images in the context of image classification. In an adversarial attack, the adversary attempts to modify the original input image by adding some carefully crafted perturbations, which can cause the image classification model to yield mispredictions. Oftentimes, the generated perturbations are either too small to be visually identified by human eyes, or small enough that humans consider them to be harmless, random noise. And yet, these perturbations can be \u201cmeaningful\u201d and misleading to the image classification model. Below, we discuss two methods to generate adversarial examples.</p> <p> An example of adversarial attack, in which a tiny amount of carefully crafted perturbations leads to misclassification. Here the perturbations are so small that they only become visible to humans after being magnified for about 30 times. </p>"},{"location":"adversary-attacks/#fast-gradient-sign-method-fgsm","title":"Fast Gradient Sign Method (FGSM)","text":"<p>The simplest yet highly efficient algorithm for generating adversarial examples is known as the Fast Gradient Sign Method (FGSM), which is a single step attack on images. Proposed by Goodfellow et al. in 2014, FGSM combines a white box approach with a misclassification goal. Using FGSM, a small perturbation is first generated in the direction of the sign of the gradients with respect to the input image. Next, the generated perturbations are added to the original image, resulting in an adversarial image. The equation for untargeted attack using FGSM is given by:</p> \\[ adv\\_x = x + \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y)) \\] <p>Here, $$ J $$ is the cost function (e.g. cross-entropy cost) of the trained model, $$ \\nabla_x $$ denotes the gradient of the model\u2019s loss function with respect to the original image $$ x $$. The thing to note here is we are calculating the gradient with respect to the pixels of the image. From the gradient of the model\u2019s loss function, we take the sign of each term in the gradient, reducing it to a matrix of 1s, 0s and -1s. The intuition here is that we nudge the pixels of the image in the direction that maximizes the loss. In other words, we perform gradient ascent instead of gradient descent, since the goal is to increase the error and let the model output the incorrect results.</p> <p>Having obtained the sign of the gradient, we then multiply the result with a tiny value, \u03f5, which controls the amount of perturbations (i.e. the perturbation\u2019s amplitude) to be added. The larger the value of epsilon, the more noticeable the perturbations are to humans. Recall that from the adversary\u2019s perspective, the goal is to ensure the corruption to the original image is imperceptible, while being able to fool the classification model. \u03f5 is a hyper-parameter to be chosen.</p> <p>FGSM can also be used for targeted misclassification attacks. In this case, the adversary aims to maximize the probability of some specific target class, which is unlikely to be the true class of the original image $$ x $$:</p> \\[ adv\\_x = x - \\epsilon*\\text{sign}(\\nabla_xJ(\\theta, x, y_{target})) \\] <p>The difference is in case of targeted attacks, we minimize the loss between the model\u2019s predicted class and the target class, whereas in case of untargeted attack we maximize the loss instead of minimize it.</p> <p>In addition to only applying the FGSM equation once, a straightforward extension is to develop an iterative procedure, and run FGSM multiple times. Here is what the iterative procedure might look like for untargeted attacks using FGSM, when implemented with TensorFlow:</p> <pre><code>import tensorflow as tf\nimport keras.backend as K\n\n# Get the true label of the iamge\ncorrect_label = get_correct_label()\ntotal_class_count = N\n\n# Initialize adversarial example with original input image\nx_adv = original_img\nx_adv = tf.convert_to_tensor(x_adv, dtype=tf.float32)\n\n# Initialize the perturbations\nnoise = np.zeros_like(original_img)\n\n# Epsilon is a hyper-parameter\nepsilon = 0.01\nepochs = 100\n\nfor i in range(epochs):\n    target = K.one_hot(correct_label, total_class_count)\n\n    with tf.GradientTape() as tape:\n        tape.watch(x_adv)\n        prediction = model(x_adv)\n        loss = K.categorical_crossentropy(target, prediction[0])\n\n    # Calculate the gradient\n    grads = tape.gradient(loss, x_adv)\n\n    # Get the sign of the gradient\n    delta = K.sign(grads[0])\n    noise = noise + delta\n\n    # Generate an adversarial example with FGSM\n    x_adv = x_adv + epsilon*delta\n\n    # Get the latest model output\n    preds = model.predict(x_adv, steps=1).squeeze()\n    pred = np.argmax(preds, axis=-1)\n\n    # Exit the procedure if model is fooled\n    if pred != correct_label:\n      break\n</code></pre> <p>In the example implementation above, we also employ early-stopping and exit the iterative procedure once the model is fooled. This helps minimize the amount of perturbations added, and may also improve the efficiency by reducing the time needed to fool the model. With the iterative approach, we can also obtain additional adversarial examples when the procedure is run for more iterations.</p> <p> An example of a \u201csuccessful\u201d adversarial attack in which the image classifier recognized a watermelon as a tomato. In this case, although the goal of misclassification is achieved, the unmagnified perturbations are large enough to be perceived by human eyes. </p> <p>In practice, FGSM attacks work particularly well for network architectures that favor linearity, such as logistic regression, maxout networks, LSTMs, networks that use the ReLU activation function, etc. While ReLU is non-linear, when \u03f5 is sufficiently small, the ReLU activation does not change the sign of the gradient with respect to the original image, and thus will not prevent the pixels of the image to be nudged in the direction that maximizes the loss. The authors of FGSM stated that changing to nonlinear model families such as RBF networks confer a significant reduction in a model\u2019s vulnerability to adversarial examples.</p>"},{"location":"adversary-attacks/#one-pixel-attack","title":"One-Pixel Attack","text":"<p>In order to fool a machine learning model, the Fast Gradient Sign Method discussed above requires many pixels of the original image to be changed, if only by a little. As shown in the example image above, sometimes the modifications might be excessive (i.e. the amount of modified pixels are fairly large) such that they become visually identifiable to human eyes. One may then wonder if it\u2019s possible to modify fewer pixels, while still keeping the model fooled? The answer is yes. In 2019, a method for generating one-pixel adversarial perturbations was proposed, in which an adversarial example can be generated by modifying just one pixel.</p> <p>The One-pixel attack uses differential evolution to find out which pixel is to be changed, and how. Differential evolution (DE) is a type of evolutionary algorithm (EA). It is a population based optimization algorithm for solving complex optimization problems. In specific, during each iteration, a set of candidate solutions (children) is generated according to the current population (parents). The candidate solutions are then compared with their corresponding parents, surviving if they are better candidate solutions according to some criterion. The process repeats until some stopping criterion are met.</p> <p>In one-pixel attack, each candidate solution encodes a pixel modification and is represented by a vector of five elements: the x and y coordinates, and the red, green and blue (RGB) values of the pixel. The search starts with 400 initial candidate solutions. In each iteration, another 400 candidate solutions (children) are generated using the following formula:</p> \\[ x_{i}(g+1) = x_{r1}(g) + F(x_{r2}(g) - x_{r3}(g)), \\] \\[ r1 \\neq r2 \\neq r3 \\] <p>where $$ x_{i} $$ is an element of the candidate solution, $$ g $$ is the current generation, $$ F $$ is the scale parameter set to be 0.5, and $$ r1 $$, $$ r2 $$, $$ r3 $$ are different random numbers. The search stops when one of the candidate solutions is an adversarial example that fools the model successfully, or if the maximum number of iterations specified has been reached.</p> <p>By using differential evolution, one-pixel attack has several advantages. Since DE doesn\u2019t use gradient information for optimization, it\u2019s not required for the objective function to be differentiable, as is the case with classical optimization methods such as gradient descent. Calculating the gradient requires much more information about the model to be exploited, therefore not needing gradient information makes conducting the attack more feasible. Finally, it\u2019s worth noting that one-pixel attack is a type of black-box attack, which assumes no information about the classification model; it is sufficient to just observe the model\u2019s output probabilities.</p>"},{"location":"adversary-attacks/#defense-strategies-against-adversarial-examples_1","title":"Defense Strategies against Adversarial Examples","text":"<p>Having discussed some techniques for generating adversarial examples, we now turn our attention to possible defense strategies against such adversarial attacks. While we go through each of the countermeasures, it\u2019s worth keeping in mind that none of them can act as a panacea for all challenges. Moreover, implementing such defense strategies may incur extra performance costs.</p>"},{"location":"adversary-attacks/#adversarial-training","title":"Adversarial Training","text":"<p>One of the most intuitive and effective defenses against adversarial attacks is adversarial training. The idea of adversarial training is to incorporate adversarial samples into the model training stage, and thus increase model robustness. In other words, since we know that the original training process leads to models that are vulnerable to adversarial examples, we just also train on adversarial examples so that the models have some \u201cimmunity\u201d over the adversarial examples.</p> <p>To perform adversarial training, the defender simply generates a lot of adversarial examples and include them in the training data. At training time, the model is trained to assign the same label to the adversarial example as to the original example. For example, upon seeing an adversarially perturbed training image, whose original label is cat, the model should learn the correct label for the perturbed image is still cat.</p> <p>The problem with adversarial training is that it is only effective in defending the models against the same attacks used to craft the examples originally included in the training pool. In black-box attacks, adversaries only need to find one crack in a system\u2019s defenses for an attack to go through. It can be likely that the attack method employed by the adversary is not anticipated by the defender at the time of model training, therefore leaving the adversarially trained model vulnerable to the unseen attacks.</p>"},{"location":"adversary-attacks/#defensive-distillation","title":"Defensive Distillation","text":"<p>Introduced in 2015 by Papernot et al., defensive distillation uses the idea of distillation and knowledge transfer to reduce the effectiveness of adversarial samples on deep neural networks. The term distillation was originally proposed as a way to transfer knowledge from a large neural network to a smaller one. Doing so can help reduce the computational complexity of deep neural networks, and facilitate the deployment of deep learning models in resource constrained devices. In defensive distillation, instead of transferring knowledge between models of different architectures, the knowledge is extracted from a model to then improve its own resilience to adversarial examples.</p> <p>Let's assume we are training a neural network for image classification tasks, and the network is designed with a softmax layer as the output layer. The key point in distillation is the addition of a temperature parameter T to the softmax operation:</p> \\[ F(X) = \\left[ \\frac{e^{z_i(x)/T}}{\\sum_{l=0}^{N-1} e^{z_l(x)/T}} \\right]_{i \\in 0 ... N-1} \\] <p>The authors showed that experimentally, a high empirical value of T gives a better distillation performance. During test time, T is set to 1, making the above equation equivalent to standard softmax operation.</p> <p>Defensive distillation is a two-step process. First, we train an initial network \\(\\(F\\)\\) on data \\(\\(X\\)\\). In this step, instead of letting the network output hard class labels, we take the probability vectors produced by the softmax layer. The benefit of using class probabilities (i.e. soft labels) instead of hard labels is that in addition to merely providing a sample\u2019s correct class, probabilities also encode the relative differences between classes. Next, we then use the probability vectors from the initial network as the labels to train another distilled network \\(\\(F\u2019\\)\\) with the same architecture on the same training data \\(\\(X\\)\\). During training, it is important to set the temperature parameter \\(\\(T\\)\\) for both networks to a value larger than 1. After training is completed, we will use the distilled network \\(\\(F\u2019\\)\\) with \\(\\(T\\)\\) set to 1 to make predictions at test time.</p> <p> Defense mechanism based on a transfer of knowledge contained in probability vectors through distillation. </p> <p>Why is defensive distillation a good idea? First, a large value of \\(\\(T\\)\\) has the effect of pushing the resulting probability distribution closer to uniform. This helps improve the model\u2019s ability to generalize outside of its training dataset, by avoiding situations where the model is forced to make an overly confident prediction in one class when a sample includes characteristics of two or more classes. The authors also argue that distillation at high temperatures reduces a model\u2019s sensitivity to small input variations, which are often found in adversarial examples. The model\u2019s sensitivity to input variation is quantified by its Jacobian:</p> \\[ \\frac{\\partial F_i(X)}{\\partial X_j} = \\frac{\\partial}{\\partial X_j}  \\left( \\frac{e^{z_i/T}}{\\sum_{l=0}^{N-1} e^{z_l/T}} \\right) \\\\ = \\frac{1}{T} \\frac{e^{z_i/T}}{g^2(X)} \\left( \\sum_{l=0}^{N-1} \\left(\\frac{\\partial z_i}{\\partial X_j} - \\frac{\\partial z_l}{\\partial X_j} \\right) e^{z_l/T} \\right) \\] <p>where</p> \\[ g(X) = \\sum_{l=0}^{N-1} e^{z_l(X)/T} \\] <p>From the above expression, it can be observed that the amplitude of the Jacobian is inversely proportional to the temperature value. During test time, although \\(\\(T\\)\\) is set to a relatively small value of 1, the model\u2019s sensitivity to small variations and perturbations will not be affected, since the weights learned at training time remain unchanged, and decreasing temperature only makes the class probability vector more discrete without changing the relative ordering of the classes.</p>"},{"location":"adversary-attacks/#denoising","title":"Denoising","text":"<p>Since adversarial examples are images with added perturbations (i.e. noise), one straightforward defense strategy is to have some mechanisms to denoise the adversarial samples. There can be two approaches to denoising: input denoising and feature denoising. Input denoising attempts to partially or fully remove the adversarial perturbations from the input images, whereas feature denoising aims to alleviate the effects of adversarial perturbations on high-level features.</p> <p>In their study, Chow et al. proposed a method for input denoising with ensembles of denoisers. The intuition of using ensembles for denoising is that there are various ways for adversaries to generate and add perturbations to images, and no single denoiser is guaranteed to be effective across all data corruption methods - a denoiser that excels at removing some types of noise may perform poorly on others. Therefore, it is often helpful to employ an ensemble of diverse denoisers, instead of relying only on a single denoiser.</p> <p>Autoencoders are used for training the denoisers. First, the denoising autoencoder takes a clean input image and transforms it into an adversarial example by adding some perturbations. Next, the noisy image is fed into the auto encoder, with a goal of reconstructing the original clean, uncorrupted image. Given N training examples, the denoising autoencoder is trained by backpropagation to minimize the reconstruction loss:</p> \\[ Loss = \\frac{1}{N}\\sum_{i=1}^{N}d(x_i, g_{\\theta'}(f_{\\theta}(x'_i))) + \\frac{\\lambda}{2}(\\|\\theta \\|_\\text{F}^2 + \\|\\theta' \\|_\\text{F}^2) \\] <p>where d is a distance function and \u03bb is a regularization hyperparameter penalizing the Frobenius norm of \u03b8 and \u03b8\u2019. $$ g^i $$ is the operation at the i-th decoding layer with weights $$ \\theta_i^' $$.</p> <p>For feature denoising as a defense strategy, one study was conducted by Xie et al, in which the authors incorporated denoising blocks at intermediate layers of a convolutional neural network. The authors argued that the adversarial perturbation of the features gradually increases as an image is propagated through the network, causing the model to eventually make the wrong predictions. Therefore, it can be helpful to add denoising blocks at intermediate layers of the network to combat feature noise.</p> <p> Defense mechanism based on a transfer of knowledge contained in probability vectors through distillation. </p> <p>The input to a denoising block can be any feature layer in the convolutional neural network. In the study, each denoising blocks performs one type of the following denoising operations: nonlocal means, bilateral filter, mean filter, and median filter. These are the techniques commonly used in computer vision tasks such as image processing and denoising. The denoising blocks are trained jointly with all layers of the network in an end-to-end manner using adversarial training. In their experiments, denoising blocks were added to the variants of ResNet models. The results showed that the proposed denoising method achieved 55.7 percent accuracy under white-box attacks on ImageNet, whereas previous state of the art was only 27.9 percent accuracy.</p>"},{"location":"adversary-attacks/#references","title":"References","text":"<p>Explaining and harnessing adversarial examples One pixel attack for fooling deep neural networks Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks Feature Denoising for Improving Adversarial Robustness Adversarial Attacks and Defences: A Survey</p>"},{"location":"adversary-attacks/#additional-resources","title":"Additional Resources","text":"<p>Adversarial Robustness - Theory and Practice (NeurIPS 2018 tutorial) CleverHans - An Python Library on Adversarial Example</p>"},{"location":"attention/","title":"Attention","text":"<p>Table of Contents:</p> <ul> <li>Motivation</li> <li>General Attention Layers<ul> <li>Operations</li> </ul> </li> <li>Self-Attention<ul> <li>Masked Self-Attention Layers</li> <li>Multi-Head Self-Attention Layers</li> </ul> </li> <li>Summary</li> <li>Additional References</li> </ul>"},{"location":"attention/#attention","title":"Attention","text":"<p>We discussed fundamental workhorses of modern deep learning such as Convolutional Neural Networks and Recurrent Neural Networks in previous sections. This section is devoted to yet another layer -- the attention layer -- that forms a new primitive for modern Computer Vision and NLP applications.</p> <p></p>"},{"location":"attention/#motivation","title":"Motivation","text":"<p>To motivate the attention layer, let us look at a sample application -- image captioning, and see what's the problem with using plain CNNs and RNNs there.</p> <p>The figure below shows a pipeline of applying such networks on a given image to generate a caption. It first uses a pre-trained CNN feature extractor to summarize the image, resulting in an image feature vector \\(c = h_0\\). It then applies a recurrent network to repeatedly generate tokens at each step. After five time steps, the image captioning model obtains the sentence: \"surfer riding on wave\".</p> <p>What is the problem here? Notice that the model relies entirely on the context vector \\(c\\) to write the caption -- everything it wants to say about the image needs to be compressed within this vector. What if we want to be very specific, and describe every nitty-gritty detail of the image, e.g. color of the surfer's shirt, facing direction of the waves? Obviously, a finite-length vector cannot be used to encode all such possibilities, especially if the desired number of tokens goes to the magnitude of hundreds or thousands.</p> <p>The central idea of the attention layer is borrowed from human's visual attention system: when humans like us are given a visual scene and try to understand a specific region of that scene, we focus our eyesight on that region. The attention layer simulates this process, and attends to different parts of the image while generating words to describe it.</p> <p>With attention in play, a similar diagram showing the pipeline for image captioning is as follows. What's the main difference? We incorporate two additional matrices: one for alignment scores, and the other for attention; and have different context vectors \\(c_i\\) at different steps. At each step, the model uses a multi-layer perceptron to digest the current hidden vector \\(h_i\\) and the input image features, to generate an alignment score matrix of shape \\(H \\times W\\). This score matrix is then fed into a softmax layer that converts it to an attention matrix with weights summing to one. The weights in the attention matrix are next multiplied element-wise with image features, allowing the model to focus on regions of the image differently. This entire process is differentiable and enables the model to choose its own attention weights.</p> <p></p>"},{"location":"attention/#general-attention-layers","title":"General Attention Layers","text":"<p>While the previous section details the application of an attention layer in image captioning, we next present a more general and principled formulation of the attention layer, de-contextualizing it from the image captioning and recurrent network settings. In a general setting, the attention layer is a layer with input and output vectors, and five major operations. These are illustrated in the following diagrams.</p> Left: A General Attention Layer. Right: A Self-Attention Layer. <p>As illustrated, inputs to an attention layer contain input vectors \\(X\\) and query vectors \\(Q\\). The input vectors, \\(X\\), are of shape \\(N \\times D_x\\) while the query vectors \\(Q\\) are of shape \\(M \\times D_k\\). In the image captioning example, input vectors are the image features while query vectors are the hidden states of the recurrent network. Outputs of an attention layer are the vectors \\(Y\\) of shape \\(M \\times D_k\\), at the top.</p> <p>The bulk of the attention operations are illustrated as the colorful grids in the middle, and contains two major types of operations: linear key-value maps; and align &amp; attend operations that we saw earlier in the image captioning example.</p> <p></p>"},{"location":"attention/#operations","title":"Operations","text":"<p>Linear Key and Value Transformations. These operations are linear transformations that convert the input vectors \\( X\\) to two alternative set of vectors:</p> <ul> <li>Key vectors \\(K\\): These vectors are obtained by using the linear equation \\(K = X W_k\\) where \\(W_k\\) is a   learnable weight matrix of shape \\(D_x \\times D_k\\), converting from input vector dimension \\(D_x\\) to key   dimension \\(D_k\\). The resulting keys have the same dimension as the query vectors, to enable alignment.</li> <li>Value vectors \\(V\\): Similarly, the equation to derive these vectors is the linear rule \\(V = X W_v\\)   where \\(W_v\\) is of shape \\(D_x \\times D_v\\). The value vectors have the same dimension as the output vectors.</li> </ul> <p>By applying these fully-connected layers on top of the inputs, the attention model achieves additional expressivity.</p> <p>Alignment. Core to the attention layer are two fundamental operations: alignment, and attention. In the alignment step, while more complex functions are possible, practitioners often opt for a simple function between vectors: pairwise dot products between key and query vectors.</p> <p>Moreover, for vectors with a larger dimensionality, more terms are multiplied and summed in the dot product and this usually implies a larger variance. Vectors with larger magnitude contribute more weights to the resulting softmax calculation, and many terms usually receive low attention. To deal with this issue, a scaling factor, the reciprocal of \\(\\sqrt{D_x}\\), is often incorporated to reduce the alignment scores. This scaling procedure reduces the effect of large magnitude terms, so that the resulting attention weights are more spread-out. The alignment computation can be summarized as the following equation:</p> \\[ e_{i,j} = \\frac{q_j \\cdot x_i}{\\sqrt{D_x}} \\] <p>Attention. The attention matrix is obtained by applying the softmax function column-wise to the alignment matrix.</p> \\[ \\mathbf{a} = \\text{softmax}(\\mathbf{e}) \\] <p>The output vectors are finally calculated as multiplications of the attention matrix and the input vectors:</p> \\[ y_j = \\sum_{i} a_{i,j} x_i \\] <p></p>"},{"location":"attention/#self-attention","title":"Self-Attention","text":"<p>While we explain the general attention layer above, the self-attention layer refers to the special case where similar to the key and value vectors, the query vectors \\(Q\\) are also expressed as a linear transformation of the input vectors: \\(Q = X W_q\\) where \\(W_q\\) is of shape \\(D_x \\times D_k\\). With this expression of query vectors as a linear function of the inputs, the attention layer is self-contained. This is illustrated on the right of the figure above.</p> <p>Permutation Invariance. It is worth noting that the self-attention layer is invariant to the order of the input vectors: if we apply a permutation to the input vectors, the outputs will be permuted in exactly the same way. This is illustrated in the following diagram.</p> Permutation Invariance of Self-Attention Layers. <p>Positional Encoding. While the self-attention layer is agnostic to the ordering of inputs, practical applications often require some notion of ordering. For example, in natural language sequences, the relative ordering of words often plays a pivotal role in differentiating the meaning of the entire sentence. This necessitates the inclusion of a positional encoding component into the self-attention module, to endow the model with the ability to determine the positions of its inputs. A number of desiderata are needed for this component:</p> <ul> <li>The positional encodings should be unique for each time step.</li> <li>The distance between any two consecutive encodings should be the same.</li> <li>The positional encoding function should generalize to arbitrarily long sequences.</li> <li>The function should be deterministic.</li> </ul> <p>While there exists a number of functions that satisfy the above criteria, a commonly used method makes use of mixed sine and cosine values. Concretely, the encoding function looks like the following:</p> \\[ p(t) = [\\sin(w_1 \\cdot t), \\cos(w_1 \\cdot t), \\sin(w_2 \\cdot t), \\cos(w_2 \\cdot t), \\cdots, \\sin(w_{d/2} \\cdot t), \\cos(w_{d/2} \\cdot t)] \\] <p>where the frequency \\(w_k = \\frac{1}{10000^{2k/d}}\\). What does this function encode? The following diagram is an intuitive explanation of the same phenomenon, but in the binary domain:</p> <p>The frequencies \\(w_k\\) are varied, to represent the relative positions of the inputs, in a similar vein as the 0s and 1s in the binary case. In practice, the positional encoding component concatenates additional information to the input vectors, before they are passed to the self-attention module:</p> <p>A Comparison Between General Attention vs. Self-Attention. The general attention layer has access to three sets of vectors: key, value, and query vectors. In comparison, the self-attention layer is entirely self-enclosed, and instead parameterizes the three sets of vectors as linear functions of the inputs.</p> <p></p>"},{"location":"attention/#masked-self-attention-layers","title":"Masked Self-Attention Layers","text":"<p>While the positional encoding layer integrates some positional information, in more critical applications, it may be necessary to distill into the model a clearer idea of relative input orderings and prevent it from looking-ahead at future vectors. To this end, the masked self-attention layer is created: it explicitly sets the lower-triangular part of the alignment matrix to negative infinity values, to ignore the corresponding, future vectors while the model processes earlier vectors.</p> <p></p>"},{"location":"attention/#multi-head-self-attention-layers","title":"Multi-Head Self-Attention Layers","text":"<p>Yet another possibility to increase the expressivity of the model is to exploit the notion of a multi-head attention. Instead of using one single self-attention layer, multi-head attention utilizes multiple, parallel attention layers. In some cases, to maintain the total computation, the key and value dimensions \\(D_k, D_v\\) may be reduced accordingly. The benefit of using multiple attention heads is to allow the model to focus on different aspects of the input vectors.</p> <p></p>"},{"location":"attention/#summary","title":"Summary","text":"<p>To summarize this section,</p> <ul> <li>We motivated and introduced a novel layer popular in deep learning, the attention layer.</li> <li>We introduced it in its general formulation and in particular, studied details of the align and attend operations.</li> <li>We then specialized to the case of a self-attention layer.</li> <li>We learned that self-attention layers are permutation-invariant to the input vectors.</li> <li>To retain some positional information, self-attention layers use a positional-encoding function.</li> <li>Moreover, we also studied two extensions of the vanilla self-attention layer: the masked attention layer, and   the multi-head attention. While the former layer prevents the model from looking ahead, the latter serves to   increase its expressivity.</li> </ul> <p></p>"},{"location":"attention/#additional-resources","title":"Additional Resources","text":"<ul> <li>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention   presents an application of the attention layer to image captioning.</li> <li>Women also Snowboard: Overcoming Bias in Captioning Models exploits the   attention layer to detect gender bias in image captioning models.</li> <li>Neural Machine Translation by Jointly Learning to Align and Translate applies   attention to natural language translation.</li> <li>Attention is All You Need is the seminal paper on attention-based   Transformers, that took the Vision and NLP communities by storm.</li> </ul>"},{"location":"aws-tutorial/","title":"AWS Tutorial","text":"<p>For GPU instances, we also have an Amazon Machine Image (AMI) that you can use to launch GPU instances on Amazon EC2. This tutorial goes through how to set up your own EC2 instance with the provided AMI. We do not currently  distribute AWS credits to CS231N students but you are welcome to use this  snapshot on your own budget.</p> <p>TL;DR for the AWS-savvy: Our image is <code>cs231n_caffe_torch7_keras_lasagne_v2</code>, AMI ID: <code>ami-125b2c72</code> in the us-west-1 region. Use a <code>g2.2xlarge</code> instance.  Caffe, Torch7, Theano, Keras and Lasagne are pre-installed. Python bindings of caffe are available. It has CUDA 7.5 and CuDNN v3.</p> <p>First, if you don't have an AWS account already, create one by going to the AWS homepage, and clicking on the yellow \"Sign In to the Console\" button. It will direct you to a signup page which looks like the following.</p> <p>Select the \"I am a new user\" checkbox, click the \"Sign in using our secure server\" button, and follow the subsequent pages to provide the required details. They will ask for a credit card information, and also a phone verification, so have your phone and credit card ready.</p> <p>Once you have signed up, go back to the AWS homepage, click on \"Sign In to the Console\", and this time sign in using your username and password.</p> <p>Once you have signed in, you will be greeted by a page like this:</p> <p>Make sure that the region information on the top right is set to N. California. If it is not, change it to N. California by selecting from the dropdown menu there.</p> <p>(Note that the subsequent steps requires your account to be \"Verified\" by  Amazon. This may take up to 2 hrs, and you may not be able to launch instances  until your account verification is complete.)</p> <p>Next, click on the EC2 link (first link under the Compute category). You will go to a dashboard page like this:</p> <p>Click the blue \"Launch Instance\" button, and you will be redirected to a page like the following:</p> <p>Click on the \"Community AMIs\" link on the left sidebar, and search for \"cs231n\" in the search box. You should be able to see the AMI <code>cs231n_caffe_torch7_keras_lasagne_v2</code> (AMI ID: <code>ami-125b2c72</code>). Select that AMI, and continue to the next step to choose your instance type.</p> <p>Choose the instance type <code>g2.2xlarge</code>, and click on \"Review and Launch\".</p> <p>In the next page, click on Launch.</p> <p>You will be then prompted to create or use an existing key-pair. If you already use AWS and have a key-pair, you can use that, or alternately you can create a new one by choosing \"Create a new key pair\" from the drop-down menu and giving it some name of your choice. You should then download the key pair, and keep it somewhere that you won't accidentally delete. Remember that there is NO WAY to get to your instance if you lose your key. </p> <p>Once you download your key, you should change the permissions of the key to user-only RW, In Linux/OSX you can do it by:</p> <p><pre><code>$ chmod 600 PEM_FILENAME\n</code></pre> Here <code>PEM_FILENAME</code> is the full file name of the .pem file you just downloaded.</p> <p>After this is done, click on \"Launch Instances\", and you should see a screen showing that your instances are launching:</p> <p>Click on \"View Instances\" to see your instance state. It should change to \"Running\" and \"2/2 status checks passed\" as shown below within some time. You are now ready to ssh into the instance.</p> <p>First, note down the Public IP of the instance from the instance listing. Then, do:</p> <pre><code>ssh -i PEM_FILENAME ubuntu@PUBLIC_IP\n</code></pre> <p>Now you should be logged in to the instance. You can check that Caffe is working by doing:</p> <pre><code>$ cd caffe\n$ ./build/tools/caffe time --gpu 0 --model examples/mnist/lenet.prototxt\n</code></pre> <p>We have Caffe, Theano, Torch7, Keras and Lasagne pre-installed. Caffe python bindings are also available by default. We have CUDA 7.5 and CuDNN v3 installed.</p> <p>If you encounter any error such as </p> <pre><code>Check failed: error == cudaSuccess (77 vs.  0)  an illegal memory access was encountered\n</code></pre> <p>you might want to terminate your instance and start over again. I have observed this rarely, and I am not sure what causes this.</p> <p>About how to use these instances:</p> <ul> <li>The root directory is only 12GB, and only ~ 3GB of that is free.</li> <li>There should be a 60GB <code>/mnt</code> directory that you can use to put your data, model checkpoints, models etc.</li> <li>Remember that the <code>/mnt</code> directory won't be persistent across reboots/terminations.</li> <li>Stop your instances when are done for the day to avoid incurring charges. GPU instances are costly. Use your funds wisely. Terminate them when you are sure you are done with your instance (disk storage also costs something, and can be significant if you have a large disk footprint).</li> <li>Look into creating custom alarms to automatically stop your instances when they are not doing anything.</li> <li>If you need access to a large dataset and don't want to download it every time you spin up an instance, the best way to go would be to create an AMI for that and attach that AMI to your machine when configuring your instance (before launching but after you have selected the AMI).</li> </ul>"},{"location":"choose-project/","title":"Taking a Course Project to Publication","text":"<p>This tutorial was originally contributed by Leila Abdelrahman, Amil Khanzada, Cong Kevin Chen, and Tom Jin with oversight and guidance from Professor Fei-Fei Li and Professor Ranjay Krishna.</p> <p>Taking a course project to publication is a challenging but rewarding endeavor. Starting in CS231n in Spring 2020, our team spent hundreds of hours over seven months to publish our project at the ACM 2020 International Conference on Multimodal Interaction! We aim to share tips on creating a great project and how you can take it to the next level through publication.</p> <p>Here is a link to our paper \ud83d\udcc4 and corresponding pdf for the 2020 ACM ICMI! </p>"},{"location":"choose-project/#table-of-contents","title":"Table of Contents \ud83d\udcd6","text":"<ul> <li>CS231n: Taking a Course Project to Publication</li> <li>Picking your Team \ud83e\udd1d</li> <li>Choosing your Project \ud83c\udf9b</li> <li>Building the Project \ud83d\udc77\ud83c\udffd</li> <li>Showcasing your Work \ud83d\udcfd</li> <li>Refining for Publication \ud83d\udcc4</li> <li>Maximizing your Impact \ud83d\ude4c</li> <li>Conclusion</li> </ul>"},{"location":"choose-project/#picking-your-team","title":"Picking your Team \ud83e\udd1d","text":"<p>Although working alone may seem faster and more comfortable, it is generally not recommended. One of the primary advantages of taking a course is building a \"dense network\" with classmates through spending hours together working on the project. Additionally, your talented classmates may actually be teachers in disguise! If done correctly, working with a team is a lot more fun and lends for greater creativity and impact through differing opinions.</p> <p>Investing good time to choose your teammates is CRITICAL, as they will shape your thinking, dictate your sanity, join your lifelong network, and ultimately determine the success of your project. Start your search as early as possible (e.g., before the course starts) so that you have enough time to consider different ideas and hit the ground running.</p> <p>Have a sense of what you want to do and who you want to work with. Post your interest on Piazza and evaluate others. Join a Slack channel for your area of interest (e.g., #bio_healthcare). Interview folks through project brainstorming video calls. Consider their experience in machine learning, motivation, and personality. Specifically, when evaluating your potential team members' deep learning background, consider factors such as prior work experience, projects/publications, and AI courses in topics related to your intended project. Ask questions such as what kind of networks they would use to tackle your intended project, how they would go about finding data sources, and what tooling they are comfortable with (e.g., Google Colab or AWS). A good list of deep learning interview questions can be found on Workera.ai.</p> <p>Finally, be flexible and pick your group. Always keep in mind that plans can change, and some students do end up dropping the course. Be open in your communication and watch out for unexpected changes in your group before the course drop deadline.</p>"},{"location":"choose-project/#picking-your-mentors","title":"Picking your Mentors","text":"<p>Do note that mentorship and support are invaluable in ensuring your success. Find the TA, professor, and/or industry mentors best suited to guide you and meet with them often. Course staff and mentors are generally interested in helping YOU succeed and may even write you a recommendation letter in the future!</p>"},{"location":"choose-project/#what-we-did","title":"What We Did","text":"<p>We looked at the profiles of all the TAs and went to office hours. We were lucky to find Christina Yuan, who had done similar research work and soon became a kind mentor for us. Professor Ranjay Krishna was also very kind to guide us in taking our project to publication.</p>"},{"location":"choose-project/#choosing-your-project","title":"Choosing your Project \ud83c\udf9b","text":"<p>What you do is very important, as it determines your enjoyment and builds your foundation for future research.</p>"},{"location":"choose-project/#sample-categories-of-cs231n-projects","title":"Sample Categories of CS231n Projects","text":"<p>Past projects can be found here on the the course website and include categories such as:</p> <ol> <li>Image Classification</li> <li>Image Segmentation</li> <li>Video Processing</li> <li>3D Deep Learning</li> <li>Sentiment Analysis</li> <li>Object Detection</li> <li>Data Augmentation</li> <li>Audio Analysis with CNNs and Spectrogram Images</li> </ol>"},{"location":"choose-project/#consider-smart-goals","title":"Consider SMART Goals","text":""},{"location":"choose-project/#specific","title":"Specific","text":"<p>What question are you exactly trying to answer, or what concept do you want to address in-depth? Specific projects are suitable for focus, but make sure it has some generalizability.</p> <p>Good Example: Generate Deep Fakes</p> <p>Bad Example: Generating Deep Fakes on lung CT scans to improve medical image segmentation.</p>"},{"location":"choose-project/#measurable","title":"Measurable","text":"<p>Make sure there are performance metrics, qualitative assessments, and other benchmarks you can use to measure your work's success. Examples include accuracy on a test dataset and performance relative to a human.</p> <p>Bad Example: Create high-fidelity image reconstructions.</p> <p>Good Example: Create high-fidelity image reconstructions with low mean-squared error difference compared to original images.</p>"},{"location":"choose-project/#attainable","title":"Attainable","text":"<p>Do you have the resources, tools, and background experience to accomplish the project's tasks? Choosing grandiose projects beyond reach can only bring disappointment, be realistic. Ask yourself if you have the computing power (GPUs, disk space), the data (is it open-access, private, do you have to curate it yourself), and the bandwidth (are you juggling a busy semester/quarter).</p> <p>If you are currently doing CV related research, you can incorporate that into your project, and your supervisor may be willing to lend you data or compute resources. If not, there is usually a short list of faculty-sponsored projects looking for students.</p> <p>Bad Example: Detection of cancer from facial videos. This is not attainable because there is unlikely to be a large enough dataset that is open-access.</p> <p>Good Example: Projects that can leverage open-source repos and APIs or work with pre-trained or fine-tuned Deep Learning models.</p>"},{"location":"choose-project/#relevant","title":"Relevant","text":"<p>CS231n requires you to process some form of 2D pixelated data with a CNN.   Discussing your idea with the TAs is good to know how relevant the project is to the class. Make sure the project also aligns with your values and interests. Think long term, and ask yourself five years after the project ends if it enriched your interests and career direction.</p> <p>Bad Example: Chirp classification with principal component analysis.</p> <p>Good Example: Object recognition using CNNs. We incorporated feature detection tasks like scene analysis and image captioning for emotion recognition in videos in our project.</p>"},{"location":"choose-project/#timely","title":"Timely","text":"<p>CS231n only lasts for 8-10 weeks. Make sure you can deliver your objectives along the class's timeline. Roadmap your project to see what needs to get done--and when! This is perhaps the most critical.   Allow yourself 2-3 weeks to find a team and formulate a proposal, 3-4 to collect/preprocess data and train your first model against an established baseline (milestone), and the rest to run additional models, tune hyperparameters, and format your report.   It's helpful to log everything in Overleaf and repurpose your milestone for your final submission so that everything doesn't catch up to you in the last few days.</p> <p>Bad Example: A project that requires two years to collect, clean, and organize raw data.</p> <p>Good Example: A project where you can work with existing datasets or spend a few weeks collecting raw data. For our EmotiW project, an existing dataset was provided to us, so we were able to spend more time on algorithms.</p>"},{"location":"choose-project/#advice-from-professor-fei-fei-li","title":"Advice from Professor Fei-Fei Li","text":"<ol> <li>Maximize learning: Find a project that can enable your maximal learning.</li> <li>Feasibility: Make sure it is executable within the short amount of time in this quarter, especially making sure that data, evaluation metric, and compute resources are adequate for what you want to do.</li> <li>Think long term: Executing a good project regardless of the topic would benefit your job application. Companies look for people with a solid technical background and the ability to execute and work as a team. I don't think they will be very narrow-minded on the specific project topics.</li> </ol>"},{"location":"choose-project/#building-the-project","title":"Building the Project \ud83d\udc77\ud83c\udffd","text":"<p>When working on the project, consider these tips to maximize the potential for impact.</p>"},{"location":"choose-project/#work-in-the-cloud","title":"Work in the Cloud","text":"<p>Store documentation, code, datasets, and model files in services such as Google Drive, Dropbox, or GitHub.</p> <p>The course offers AWS and GCP credits for your projects. Consider taking advantage of these resources, especially when it comes to heavy computing.</p> <p>If you plan to use Colab, consider saving your model checkpoints directly to your Stanford Google Drive folder to avoid losing files. Colab is also great to parallelize your work as you can run multiple experiments simultaneously. Be careful not to let your work session time out, or you could lose your progress!</p>"},{"location":"choose-project/#work-collaboratively","title":"Work Collaboratively","text":"<p>Brainstorm your projects on live-editors like Google Sheets and Google Docs so others can work in the same workspace. Overleaf is excellent for polishing the final report as a team.</p> <p>Set up regular weekly or semi-weekly meetings with your teammates. This establishes a cadence and helps keep everyone on track, even if no progress has been made.</p>"},{"location":"choose-project/#document-everything","title":"Document Everything","text":"<p>Write your code as if you will be showing it to someone who has no idea about your project. This is great for making your methods transparent and reproducible.</p> <p>Record all experiments you run (e.g., hyperparameter tuning) even if they don't work out, as you can write about them in your report.</p>"},{"location":"choose-project/#be-organized","title":"Be Organized","text":"<p>Create clean file structures. Give folders and scripts precise names. Organization saves your team hours, if not days, in the long run.</p> <p>Write your notebooks, so they are executable top-to-bottom. That way, if you suffer a catastrophic data loss or discover a mistake in your code, you can smoothly go back and replicate your entire setup.</p>"},{"location":"choose-project/#showcasing-your-work","title":"Showcasing your Work \ud83d\udcfd","text":"<p>Communication makes the project memorable. Come up with a fun title, and make sure to present your work in multiple ways. Beyond the written report, create a slide deck, record a video, and practice talking about your work with TAs, professors, and industry experts. Articulation skills in oral and visual presentations ensure that the audience remembers your work!</p> <p>Before completing the project, take the time to clean up your GitHub repository, or make one if you have not done so already. Add a strong README.md file with a clear description of your project and a \"how-to\" guide complete with examples. Doing so makes your project accessible and encourages others to build upon your work.</p> <p>If you are submitting to a journal or conference, consider submitting a copy of your work to a preprint service like arXiv. arXiv does not require peer-review and is commonly used by researchers within the CS field to disseminate their work. Publishing a preprint is a quick way to add to your list of publications on Google Scholar. You may also want to take 5 minutes to register and ORCID.</p>"},{"location":"choose-project/#our-challenges","title":"Our Challenges","text":"<p>With four people working simultaneously, our Github repository had naturally become a little hard to navigate since everyone used a slightly different naming scheme. Multiple cloned notebooks were also leftover from other experiments. In our case, our goal had always been to submit a grand challenge paper based on our results, so organizing public-facing repository by project completion was critical.</p> <p>In 2020, many conferences were entirely virtual, which made it even more important to have a polished video presentation as this would end up as part of conference proceedings.</p>"},{"location":"choose-project/#what-we-did_1","title":"What We Did","text":"<p>We made sure our Github repository reflected our results, and we took the opportunity to embed multiple figures into the README to make it visually appealing and easily understandable.</p> <p>We chose the playful title \"Fusical\" and created a one-figure visual that summarized our work at a high-level. This made it easy for people to reference our work during the conference, allowing us to talk about our project without confusing attendees with different slides.</p>"},{"location":"choose-project/#refining-for-publication","title":"Refining for Publication \ud83d\udcc4","text":"<p>When refining for publication, ask yourself: What am I bringing to the conversations that is new? This can come from novelty or exceeding state-of-the-art methods. Reinforcing this question throughout refinement is critical for successfully publishing.</p> <p>The next step is to find the right journal or conference to publish to. Unlike most science fields, CS and AI tend to value conference submissions over journal submissions due to the publication turnaround time and higher visibility within the field. Conferences are fun and allow you to network with other like-minded individuals.</p> <p>In Computer Vision, you'll find conferences of all sizes happening at different times throughout the year. Top conferences include CVPR, ICCV, and ECCV, but due to their popularity, it might be challenging to get a successful submission. Because many students choose to do an application-based project, consider niche conferences depending on your project's topic. For instance, teams who developed a model to determine student engagement in the classroom might consider submitting to AIED. Talking to your mentors might also help narrow down the specific method or venue of publication. Keep in mind that Stanford does offer some travel grants.</p> <p>Iterating over the work is essential for creating publication-quality work. Improving the writing, optimizing the code, and making quality figures are vital points to consider. Reference the work of experts and reach out for advice. Dr. Juan Carlos Niebles' publications page is a good example.</p> <p>Publications are a lengthy process, partly due to peer review. Review your submission for possible ambiguous areas or claims that are unsubstantiated. Ensure you are clear with your experiment parameters and ask someone outside your team to review the paper.</p> <p>Writing for a peer-reviewed journal or conference proceedings is very different than preparing a project report, yet the project helps guide what goes into the final paper.</p>"},{"location":"choose-project/#our-challenges_1","title":"Our Challenges","text":"<p>While we made advances during the CS231n course, we didn't believe our models were novel enough for publication just yet. Based on guidance from our mentors, we looked for opportunities to differentiate our work from others. Because our project was part of the EmotiW grand challenge, one of the motivating factors was to continue exploring additional methods to improve our validation performance before the submission deadline.</p>"},{"location":"choose-project/#what-we-did_2","title":"What We Did","text":"<p>Based on a literature search, we found common patterns in the modalities used by other groups for sentiment classification (e.g. audio, pose). As a result, we focused on exploring two novel modalities that we felt had value: image captioning (to look for certain objects or settings, such as protests, that are correlated with a specific degree of sentiment) and laughter detection (since the presence of laughter in an audio clip quite often indicates a positive sentiment). Using modified saliency maps and ablation studies, we tried to demonstrate the usefulness of all of these modalities for a more convincing argument.</p> <p>Overall, we attained a test accuracy of approximately 64%. Because there was no public leaderboard, there was initial uncertainty about how we fared compared to other groups, but we were confident in our approach as it beat the baseline by a large margin. Although we ultimately ranked below the top three winning scores, we were still able to publish our work at the conference due to our novel findings, demonstrating that the test data performance is only of many criteria stressed for publication.</p>"},{"location":"choose-project/#differences-between-our-course-project-and-publication-paper","title":"Differences Between our Course Project and Publication Paper","text":""},{"location":"choose-project/#course-project","title":"Course Project","text":"<ul> <li>More focus on error analysis and experiments that went wrong.</li> <li>Tendency towards experimental approaches, as the course timeframe was too short for pretraining and auxiliary data.</li> <li>Significant time spent on \"invisible\" setup work for the project's infrastructure (preprocessing, APIs, and overall backbone).</li> <li>Our original CS231n original project submission can be found here.</li> </ul>"},{"location":"choose-project/#publication-paper","title":"Publication Paper","text":"<ul> <li>More focus on the novelty of our approach and emphasis on beating the competition baseline.</li> <li>The video presentation was longer and required more detail.</li> <li>Ran more experiments and hyperparameter tuning.</li> <li>Our official published paper can be found here and as a pdf.</li> </ul> <p>If you don't get the results you were hoping for, don't be scared to submit it anyway - the worst reviewers can do is say no, and feedback from experts in the field is always valuable!</p>"},{"location":"choose-project/#maximizing-your-impact","title":"Maximizing your Impact \ud83d\ude4c","text":"<p>Your project has worth and impact. Presenting at global conferences (we presented at the 2020 ICMI) is a great way to share your results and findings with other academic and industry experts, but there are other options. Do you think your idea can be deployed in practice? Stanford has resources and opportunities to help students continue to pursue their ideas, including the Startup Garage, a hands-on project-based course to develop and test out new business concepts, and the Vision Lab.</p> <p>If your idea is more research-focused, seek out professors who might work in the domain area and pitch them your thoughts. You might have opportunities to continue your project as an independent study project or apply your idea to more extensive and significant datasets or pivot to similar research areas.</p>"},{"location":"choose-project/#conclusion","title":"Conclusion","text":"<p>It was hard work and fun because we had a good team! We hope you all enjoy these tips on the process of taking a project from brainstorming to impactful presentations and publications. These tips are scalable, and we hope you use them in other endeavors as well!</p>"},{"location":"convnet-tips/","title":"Convnet tips","text":""},{"location":"convnet-tips/#addressing-overfitting","title":"Addressing Overfitting","text":""},{"location":"convnet-tips/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Flip the training images over x-axis</li> <li>Sample random crops / scales in the original image</li> <li>Jitter the colors</li> </ul>"},{"location":"convnet-tips/#dropout","title":"Dropout","text":"<ul> <li>Dropout is just as effective for Conv layers. Usually people apply less dropout right before early conv layers since there are not that many parameters there compared to later stages of the network (e.g. the fully connected layers).</li> </ul>"},{"location":"generative-modeling/","title":"Generative Modeling","text":"<p>Table of Contents - Motivation and Overview - Pixel RNN/CNN     - Explicit density model     - Pixel RNN     - Pixel CNN - Variational Autoencoder     - Overview of Variational Autoencoder     - Autoencoders v.s. Variational Autoencoders     - VAE Mathematical Explanation     - VAE training process - Generative Adversarial Networks     - Overview of Generative Adversarial Networks      - Generative Adversarial Nets - 2014 original version         - Discriminator network         - Generator network     - GAN Mathematical explanation     - Evaluation         - Inception Scores         - Nearest Neighbours         - HYPE - Human eye perceptual Evaluation     - Challenges         - Optimization         - Mode Collapse     - Case Studies         - DCGAN         - CycleGAN         - StyleGAN     - Summary</p> <p></p>"},{"location":"generative-modeling/#motivation-and-overview","title":"Motivation and Overview","text":"<p>In the first half of the quarter, we studied several supervised learning methods, which learn functions to map input images to labels. However, labeling the training data may be expensive because it requires much time and effort. Thus, we are introducing unsupervised learning methods. In unsupervised learning methods, training data is relatively cheaper because the methods don't need labeling from the huge dataset. The goal is to learn the underlying hidden structures or feature representations from raw data directly.</p> <p>This table compares supervised and unsupervised learning:  |          | Supervised Learning | Unsupervised Learning | | -------- | -------- | -------- | | Data         | has label y         |    no labels      | | Goal         |  input data -&gt; output label        |    Learn some underlying hidden strcture of the data      | | Examples     | Classification, regression, object detection, semantic segmentation, image captioning, etc     |   Clustering, dimensionality reduction, feature learning, density estimation, etc.   |</p> <p>Generative modeling is in the class of unsupervised learning. The goal of generative modeling is to generate new samples from the same distribution. In the application of image generation, we want to make sure that the quality of generated images aligns with the raw data image distributions. Thus, during the training process, there are two objectives: 1. Learn \\(p_{model} (x)\\) that approximates \\(p_{data}(x)\\)  2. Sampling new data \\(x\\) from \\(p_{model}(x)\\) </p> <p>Within the first objective (how \\(p_{model} (x)\\) approximates \\(p_{data}(x)\\)), we can categorize generative model into two types:  1. Explicit density estimation 2. Implicit density estimation</p> <p></p> <p>In this document, we will talk about 3 most popular types:  1. Pixel RNN/CNN - Explicit density estimation 2. Variational Autoencoder - Approximate density 3. Generative Adversarial Networks - Implicit density</p>"},{"location":"generative-modeling/#pixel-rnncnn","title":"Pixel RNN/CNN","text":""},{"location":"generative-modeling/#explicit-density-model","title":"Explicit density model","text":"<p>Pixel RNN/CNN is an explicit density estimation method, which means that we explicitly define and solve for \\(p_{model}(x)\\). For example, given an input image \\(x\\), we can calculate the joint likelihood of each pixel in the image, in order to predict the of the pixel values of the image \\(x\\). Being able to explicitly calculate the joint likelihood is why we called it explicitly defined method. </p> <p>However, estimating the joint likelihood of pixels directly can be difficult. A trick borrowed from probability is to rewrite the joint likelihood as a product of the conditional likelihoods on the previous pixels. This uses the chain rule to decompose the likelihood of an image \\(x\\) into products of 1-dimensional densities. Our objective is then to maximize the likelihood of training data.</p> \\[ \\begin{aligned} p(x) = p(x_1, x_2, ..., x_n)      &amp;= \\prod_{i=1}^n p(x_i | x_1, ..., x_{i - 1}). \\end{aligned} \\]"},{"location":"generative-modeling/#pixel-rnn","title":"Pixel RNN","text":"<p>You may notice that the distribution of \\(p(x)\\) is very complex because each pixel is conditioned on hundreds of thousands of pixels, making the computation very expensive. How can we resolve this problem? </p> <p>Recall RNN that we learn in the previous lecture. RNN has an \u201cinternal state\u201d that is updated as a sequence is processed and allows previous outputs to be used as inputs. We can treat the conditional distribution of each pixel as a sequence of data, and apply RNN to model the joint likelihood function. More specifically, we can model the dependency of one pixel on all the previous pixels by keeping the hidden state of all the previous inputs. We use the hidden state to express the dependency of generating a new pixel from the previous pixel. In the beginning, we have the default hidden state,  first pixel \\(x_1\\), and the image we want to generate. We will use the first pixel to generate the second pixel and repeat. Then it becomes a sequential generating process: feeding the previously predicted pixel back to the network to generate the next pixel.</p> <p>The process described in the Pixel RNN paper is as follows: Starting from the corner, each pixel is conditional on the pixel from the left and the pixel above. Repeat the sequential generating process until the whole image is generated.</p> <p></p>  Pixel RNN Sequential Generating Process"},{"location":"generative-modeling/#pixel-cnn","title":"Pixel CNN","text":"<p>One drawback of Pixel RNN is that the sequential generation is slow and we need to process the pixels one at a time. Is there a way to process more pixels at a time? In the same paper, the authors proposed another method, Pixel CNN, which allows parallelizaton among pixels. Specifically, Pixel CNN uses a masked convolution over context region. Different from regular square receptive field in the convolutional layer, the receptive field of masked convolution need not be a square. </p> <p>You may wonder: are we able to generate the whole image with masked convolution? In fact, if we stack enough layers of this kind of masked convolution, we can achieve the same effective receptive field as the pixel generation that conditional on all of the previous pixels (pixel RNN).</p> <p></p>  Pixel CNN Generating Example"},{"location":"generative-modeling/#variational-autoencoder","title":"Variational Autoencoder","text":""},{"location":"generative-modeling/#overview-of-variational-autoencoder","title":"Overview of Variational Autoencoder","text":"<p>The modeling procedure of Pixel RNN is still slow because it's a sequential generation process. What if we make a little bit of trade-off but we can generate all pixels at the same time and model a simpler data distribution? Instead of optimizing the expensive tractable density function directly, we can derive and optimize the lower bound on likelihood instead. This is called Approximate density estimation.</p> <p>We can re-write the probability density function as \\(p_{\\theta}(x) = \\int p_{\\theta}(z)p_{\\theta}(x|z)dz\\). We introduce a new latent variable \\(z\\) to decompose the data likelihood as the marginal distribution of the conditional likelihood w.r.t this latent variable. The latent variable \\(z\\) represents the underlying structure of the data distribution. </p> <p>This method is called Variational Autoencoders (VAE). There is no dependency among the pixels. All pixels are conditional on this variable z. We can generate all pixels at the same time. The drawback is we need to integrate all possible values of z. In reality, \\(p_\\theta(z)\\) is low dimensional and \\(p_\\theta(x|z)\\) is often times complex, making it impossible to integrate \\(p_\\theta(x)\\) with an analytical solution. Thus, we cannot directly optimize this function as a result. We will discuss how to resolve this issue by approximating the unknown posterior distribution from only the observed data \\(x\\) in the following section. </p>"},{"location":"generative-modeling/#autoencoders-vs-variational-autoencoders","title":"Autoencoders v.s. Variational Autoencoders","text":"<p>Autoencoder Before diving into Variational Autoencoders, let's take a look at Autoencoder, a model that encodes input by reconstructing the input itself. An Autoencoder contains an encoder and a decoder, with the goal of learning a low-dimensional feature representation from the input (unlabeled) training data. The encoder compresses the input data to low-dimensional feature vector z, while the decoder decomposes \\(z\\) to the same shape as the input data.</p> <p>The idea of Autoencoder is to compress input images such that each vector in z contains meaningful factors of variation in data. For example, if the inputs are different faces, the dimensions in z could be facial expressions, poses, different degrees of smile, etc.</p> <p>However, we cannot generate new images from an autoencoder because we don't know the distributional space of z. VAE makes Autoencoders generative and allows us to sample from the model to generate data. VAE estimates the latent z representation so that we can generate more realistic images from the sampling. The intuition is z space shall reflect the factors of the variations. Assume that each image x is generated by sampling a new z with a slightly different factor of variations. Overall, z is used to conditionally generate the x.</p>"},{"location":"generative-modeling/#vae-mathematical-explanation","title":"VAE Mathematical Explanation","text":"<p>We need two things to represent the model: 1. choose a proper \\(p(z)\\): Gaussian distribution is a reasonable choice for latent attributes. We can interpret every expression as a variation of the average neutral expression. 2. conditional distribution \\(p(x|z)\\) is represented with the neural network: We want to be able to generate a high-dimensional image from the simple low-dimension Gaussian distribution.</p> <p>Intractability To train the model, we can learn model parameters to maximize likelihood of training data: \\(p_{\\theta}(x) = \\int p_{\\theta}(z)p_{\\theta}(x|z)dz\\). However, this likelihood expression is intractable to evaluate or to optimize because we cannot compute \\(p(x|z)\\) for every z in the intergral. We may also try to estimate posterior density \\(p_{\\theta}(z|x) = p_{\\theta}(x|z)p_{\\theta}(z)/p_{\\theta}(x)\\) but it's also intractable due to the \\(p_{\\theta}(x)\\) term. Alternatively, in the paper, the authors proposed that we can approximate the true posterior \\(p_{\\theta}(z|x)\\) with \\(q_{\\phi}(z|x)\\), which is a lower bound on the data likelihood and can be optimized. </p> <p>The goal is to maximize the log-likelihood of \\(p_{\\theta} (x^{(i)})\\). Since \\(p_{\\theta} (x^{(i)})\\) does not depend on z, we can re-write as the expectation of z w.r.t \\(q_{\\phi}(z|x^{(i)})\\) and further derive (from lecture 12 slide): $$ \\begin{aligned} \\log p_{\\theta} (x^{(i)}) &amp;= \\mathbb{E}{z \\sim q{\\phi}(z|x^{(i)})} [\\log p_{\\theta}(x^{(i)})] \\ &amp;= \\mathbb{E}z [\\log \\frac{p{\\theta}(x^{(i)} | z)p_{\\theta}(z)}{p_{\\theta}(z | x^{(i)})}] \\ &amp;= \\mathbb{E}{z} [\\log \\frac{p{\\theta}(x^{(i)} | z)p_{\\theta}(z) q_{\\phi}(z | x^{(i)}) }{p_{\\theta}(z | x^{(i)}) q_{\\phi}(z | x^{(i)}) }] \\ &amp;= \\mathbb{E}z [\\log p{\\theta} (x^{(i)} | z)] - \\mathbb{E}z [\\log \\frac{q{\\phi}(z | x^{(i)})}{p_{\\theta}(z)}] + \\mathbb{E}z [\\log \\frac{q{\\phi}(z | x^{(i)})}{p_{\\theta}(z | x_{(i)})}] \\ &amp;=  \\mathbb{E}z [\\log p{\\theta} (x^{(i)} | z)] - D_{KL}(q_{\\phi}(z | x^{(i)}) || p_{\\theta}(z)) + D_{KL}(q_{\\phi}(z | x^{(i)})|| p_{\\theta}(z | x^{(i)})) \\end{aligned} $$</p> <p>The estimate of \\(\\mathbb{E}_z [\\log p_{\\theta} (x^{(i)} | z)]\\) can be computed through sampling. \\(D_{KL}(q_{\\phi}(z | x^{(i)}) || p_{\\theta}(z))\\) has closed-form solution. \\(D_{KL}(q_{\\phi}(z | x^{(i)})|| p_{\\theta}(z | x^{(i)}))\\) would be always larger than or equal to zero. Thus, we have tractable lower bound. </p>"},{"location":"generative-modeling/#vae-training-process","title":"VAE training process","text":"<p>\\(q_{\\phi}(z | x^{(i)})\\) is the encoder network in this process. The goal of \\(D_{KL}(q_{\\phi}(z | x^{(i)}) || p_{\\theta}(z))\\) is to estimate a posterior distribution close to prior distribution. On the other hand, \\(p_{\\theta} (x^{(i)} | z)\\) is the decoder network and reconstruct the input data. We compute them in the forward pass for every minibatch of input data and then perform back-propagation. </p>"},{"location":"generative-modeling/#generative-adversarial-networks","title":"Generative Adversarial Networks","text":""},{"location":"generative-modeling/#overview-of-generative-adversarial-networks","title":"Overview of Generative Adversarial Networks","text":"<p>While implicit modeling is proven useful in generating data, it has the drawback of needing to estimate a probability distribution. What if we give up on explicitly modeling density, and just want the ability to sample? For Generative Adversarial Networks, we don't model the likelihood function \\(p(x)\\) at all but only care about generating high-quality pictures.</p> <p>From VAE, we learn that we can map a simple Gaussian distribution to a complex image distribution. We could leverage the same idea by mapping low dimensional noise to high dimensional image distribution. We can think of the decoder network as a generative network. The goal of Generative Adversarial Networks is to directly generate samples from a high-dimensional training distribution.</p>"},{"location":"generative-modeling/#generative-adversarial-nets-2014-original-version","title":"Generative Adversarial Nets - 2014 original version","text":"<p>You may be curious: if we don't model z's distribution and don't know which sample z maps to which training image, how can we learn by reconstructing training images?</p> <p>The general objective is to generate images that should look \"real\". To achieve that, Generative Adversarial Net trains another network that learns to tell the difference between real and fake images and whether the generated image from a generator network looks like the one coming from the real distribution.</p>"},{"location":"generative-modeling/#discriminator-network","title":"Discriminator network","text":"<p>The network that tells whether the image is real or fake is called the Discriminator network. We refer to images from the training distribution as real and the generated images from the generator network as fake. The discriminator network is essentially performing a supervised binary-class classification task. The discriminator uses the real/fake information to compute the gradient and backpropagate to the generation network, to make the generative examples more 'real'.</p> <p>In the beginning, the discriminator can tell whether an image is a real input image or a generated image easily. Over time, as the generator network improves, images become more and more realistic. The discriminator has to change the decision boundary gradually to fit the new distribution better and better.</p> <p>Python code example:  <pre><code>logits_real = D(real_data)\n\nrandom_noise = sample_noise(batch_size, noise_size)\nfake_images = G(random_noise)\nlogits_fake = D(fake_images.view(batch_size, 1, size, size))\n\nd_total_error = discriminator_loss(logits_real, logits_fake)\n</code></pre></p>"},{"location":"generative-modeling/#generator-network","title":"Generator network","text":"<p>On the other side, the network that maps low dimensional noise to high dimensional image distribution is called the Generator. The goal of the generator is to fool the discriminator by generating real-looking images. In the beginning, the generator will generate random tensors that don't look like real images at all. However, the signal from the discriminator would inform the generator how it should improve the generated image to look more real. Over time, the generator would learn to generate more and more realistic samples.</p> <p>python code example:  <pre><code>random_noise = sample_noise(batch_size, noise_size)\nfake_images = G(random_noise)\n\ngen_logits_fake = D(fake_images.view(batch_size, 1, size, size))\ng_error = generator_loss(gen_logits_fake)\n</code></pre></p>"},{"location":"generative-modeling/#gan-mathematical-explanation","title":"GAN Mathematical explanation","text":"<p>Due to the coexistence of two networks, GAN is a two-player/min-max game that balances the optimization between Generator and Discriminator network. </p> <p>Objective function:  $$ \\begin{aligned} \\min_{\\theta_g} \\max_{\\theta_d} [\\mathbb{E}{x \\sim p{data}} \\log D_{\\theta_d} (x) + \\mathbb{E}{z \\sim p(z)} \\log(1 - D{\\theta_d}(G_{\\theta_g}(z)))] \\end{aligned} $$</p> <p>Generator Objective: \\(\\min_{\\theta_g}\\) find weights that minimize this objective. \\(\\mathbb{E}_{x \\sim p_{data}} \\log D_{\\theta_d} (x)\\) is the expectation of score predicted by discriminator, given on the training set. The generator would try to maximize this term because the generator tries to fool the discriminator by generating more realistic images.</p> <p>Discriminator Objective: \\(\\max_{\\theta_d}\\) find weights that maximize this objective.</p> <p>During the training, generator transforms noise z to tensor and then the generated image is fed to the discriminator. Thus, \\(D_{\\theta_d}(G_{\\theta_g}(z))\\) is the generated image score predicted by discriminator. Discriminator tries to tell the difference between real and fake images by moving this term to 0 and maximizing \\(1 - D_{\\theta_d}(G_{\\theta_g}(z))\\). </p> <p>The training process is to alternate between  1. Gradient ascent on discriminator  2. Gradient descent on generator</p> <p>Problem of 2 is the gradient dominated by the region sample is already good. Training is very slow and unstable at the beginning. One solution is to change to use gradient ascent on generator and modify the different objective.</p> <p> </p>  Generative Adversaial Nets training flow"},{"location":"generative-modeling/#evaluation","title":"Evaluation","text":"<p>Recall that there are two objectives in Generative Modeling:  1. Learn \\(p_{model} (x)\\) that approximates \\(p_{data}(x)\\) 2. Sampling new data \\(x\\) from \\(p_{model}(x)\\) </p> <p>When evaluating the GAN output, we want to make sure the two objectives are taken care of.</p>"},{"location":"generative-modeling/#inception-scores","title":"Inception Scores","text":"<p>Inception score was a popular evaluation metric, which evaluates the quality of generated images. Inception Score uses the Inception V3 pre-trained model on ImageNet to observe the distribution of generated images. If the generated image is easily recognized by the discriminator, the classification score (i.e. \\(p(y|x)\\)) would be large, which leads to \\(p(y|x)\\) having low entropy. Meanwhile, we also want the marginal distribution \\(p(y)\\) to have high entropy. The inception score can be calculated as follows: $$ \\begin{aligned} IS(x) &amp;= \\exp(\\mathbb{E}{x \\sim p_g}[D{KL}[p(y|x) || p(y)]]) \\ &amp;= \\exp(\\mathbb{E}_{x \\sim p_g, y \\sim p(y | x)} [\\log p(y | x) - \\log p(y)]) \\ &amp;= \\exp(H(y) - H(y | x)) \\end{aligned} $$ where a high inception score indicates better-quality generated images. However, the inception score has some drawbacks and is fooled over the years. Thus, people started using measurements such as FID in recent years. </p>"},{"location":"generative-modeling/#nearest-neighbours","title":"Nearest Neighbours","text":"<p>A simpler evaluation method is to visualize a sample of generated images to tell how realistic the generated images are. We can also leverage Nearest Neighbours to compare real images and generated images. The idea is to sample some real images from the training set and calculate the distance between the sampled generated images. If the generated images are real-looking, the distances should be small.</p>"},{"location":"generative-modeling/#hype-human-eye-perceptual-evaluation","title":"HYPE - Human eye perceptual Evaluation","text":"<p>HYPE is a new evaluation method introduced in 2019. It evaluates GAN by a social computing method: the website invites users to evaluate GAN and try to build metrics on top of it. The goal is to ensure the evaluation is consistent while evaluating different types of GANs.</p>"},{"location":"generative-modeling/#challenges","title":"Challenges","text":""},{"location":"generative-modeling/#optimization","title":"Optimization","text":"<p>It's not easy to train GAN because the process has many challenges. Often times, the generator and discriminator loss keeps oscillating during GAN training. There is also no stopping criterion in practice. Also, when the discriminator is very confidently classifying fake samples, the generator training may fail due to vanishing gradients.  </p>"},{"location":"generative-modeling/#mode-collapse","title":"Mode collapse","text":"<p>Mode collapse happens when the generator learns to fool the discriminator by producing a single class from the whole training dataset. Often time the training dataset is multi-modal, which means the probability density distribution over features has multiple peaks. If data is imbalanced or some other problems happen during the training process, the generating image may collapse into one mode or few modes while other modes are disappearing. For example, the discriminator classifies a lot of generated images incorrectly. The generator takes the feedback and only generates images that are the same or similar to the ones that fool the discriminator. Eventually, the generated images collapse into single-mode or fewer modes.</p>"},{"location":"generative-modeling/#case-studies","title":"Case Studies","text":""},{"location":"generative-modeling/#dcgan","title":"DCGAN","text":"<p>The idea of DCGAN is to use a convolutional neural network in GAN. Here are some architecture guidelines DCGAN gave in their paper:</p> <ol> <li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).</li> <li>Use batch norm in both the generator and the discriminator.</li> <li>Remove fully connected hidden layers for deeper architectures</li> <li>Use ReLU activation in generator for all layers except for the output, which uses Tanh.</li> <li>Use LeakyReLU activation in the discriminator for all layers.</li> </ol>"},{"location":"generative-modeling/#cyclegan","title":"CycleGAN","text":"<p>Image-to-image translation is a class of problems where the goal is to map an image to another image within the same pair. For example, one may wish to map an image of a location during the Spring season to an image of the same location but during the Fall season. However, paired images are not always availabe. The goal of CycleGAN is then to learn a mapping \\(G\\) such that the distribution \\(G(X)\\) is as similar to the distribution of \\(Y\\) as possible, where \\(X\\) and \\(Y\\) are the input and output images respectively. </p>"},{"location":"generative-modeling/#stylegan","title":"StyleGAN","text":"<p>StyleGAN is an extension of GAN that aims to improve the generator's ability to generate a wider variety of images. The main modifications to the architecture of GAN's generator is by having two sources of randomness (instead of one): a mapping network that controls the style of the output image, and an additional noise that adds variability to the image. Applications of StyleGAN include human-face generation, anime character generations, new fonts, etc. </p>"},{"location":"generative-modeling/#summary","title":"Summary","text":"<p>Comparision between the methods</p> Pixel RNN/CNN Variational AutoEncoders Generative Adversial Modeling Pros <ul><li>Can explicitly compute likelihood p(x) </li> <li> Easy to optimize </li> <li>Good samples </li></ul> <ul><li>principled approach to generative models </li> <li>interpretable latent space </li> <li> allows inference of q(z|x) </li> <li>can be useful feature representation for other tasks  </li></ul> Beautiful, state-of-the-art samples! Cons slow sequential generation <ul><li>Maximizes lower bound of likelihood which is not as good evaluation as PixelRNN/PixelCNN </li> <li> Samples blurrier and lower quality compared to state-of-the-art (GANs) </li></ul> <ul><li>Trickier/more unstable to train </li> <li> cannot solve inference queries such as p(x) p(z|x)  </li></ul>"},{"location":"generative-modeling/#further-reading","title":"Further Reading","text":"<p>These readings are optional and contain pointers of interest. </p> <p>PixelRNN/CNN: https://arxiv.org/pdf/1601.06759.pdf Variational Auto-Encoders: https://arxiv.org/pdf/1312.6114.pdf Generative Adversial Net: https://arxiv.org/pdf/1406.2661.pdf DCGAN: https://arxiv.org/pdf/1511.06434.pdf CycleGAN: https://arxiv.org/pdf/1703.10593.pdf StyleGAN: https://arxiv.org/pdf/1812.04948.pdf Mode collapse: https://www.coursera.org/lecture/build-basic-generative-adversarial-networks-gans/mode-collapse-Terkm HYPE: https://arxiv.org/pdf/1904.01121.pdf</p>"},{"location":"generative-models/","title":"Generative Modeling","text":"<p>With generative modeling, we aim to learn how to generate new samples from the same distribution of the given training data. Specifically, there are two major objectives: </p> <ul> <li>Learn \\(p_{\\text{model}}(x)\\) that approximates true data distribution \\(p_{\\text{data}}(x)\\)</li> <li>Sampling new \\(x\\) from \\(p_{\\text{model}}(x)\\)</li> </ul> <p>The former can be structured as learning how likely a given sample is drawn from a true data distribution; the latter means the model should be able to produce new samples that are similar but not exactly the same as the training samples. One way to judge if the model has learned the correct underlying representation of the training data distribution is the quality of the new samples produced by the trained model. </p> <p>These objectives can be formulated as density estimation problems. There are two different approaches: </p> <ul> <li>Explicit density estimation: explicitly define and solve for \\(p_{\\text{model}}(x)\\)</li> <li>Implicit density estimation: learn model that can sample from \\(p_{\\text{model}}(x)\\) without explicitly define it </li> </ul> <p>The explicit approach can be challenging because it is generally difficult to find an expression for image likelihood function from a high dimensional space. The implicit approach may be preferable in situations that the only interest is to generate new samples. In this case, instead of finding the specific expression of the density function, we can simply training the model to directly sample from the data distribution without going through the process of explicit modeling. </p> <p></p> <p>Generative models are widely used in various computer vision tasks. For instance, they are used in super-resolution applications in which the model fills in the details of the low resolution inputs and generates higher resolution images. They are also used for colorization in which greyscale images get converted to color images. </p>"},{"location":"generative-models/#pixelrnn-and-pixelcnn","title":"PixelRNN and PixelCNN","text":"<p>PixelRNN and PixelCNN [van den Oord et al., 2016] are examples of a fully visible belief network (FVBN) in which data likelihood function \\(p(x)\\) is explicitly modeled given image input \\(x\\): </p> <p>\\( p(x) = p(x_1, x_2, \\cdots, x_n) \\)</p> <p>where \\(x_1, x_2, \\cdots\\) are each pixel in the image. In other words, the likelihood of an image (LHS) is the joint likelihood of each pixel in the image (RHS). We then use chain rule to decompose the joint likelihood into product of 1-d distributions: </p> <p>\\( \\displaystyle p(x) = \\prod_{i=1}^{n} p(x_i \\mid x_1, \\cdots, x_{i-1}) \\)</p> <p>where each distribution in the product gives the probability of \\(i\\)<sup>th</sup> pixel value given all previous pixels. Choice of pixel ordering (hence \"previous\") might have implications on computational efficiencies in training and inference. Following is an example of ordering in which each pixel \\(x_i\\) (in red) is conditioned on all the previously generated pixels left and above of \\(x_i\\) (in blue): </p> <p></p> <p>To train the model, we could try to maximize the defined likelihood of the training data. </p>"},{"location":"generative-models/#pixelrnn","title":"PixelRNN","text":"<p>The problem with above naive approach is that the conditional distributions can be extremely complex. To mitigate the difficulty, the PixelRNN instead expresses conditional distribution of each pixel as sequence modeling problem. It uses RNNs (more specifically LSTMs) to model the joint likelihood function \\(p(x_i \\mid x_1, \\cdots, x_{i-1})\\). The main idea is that we model the dependencies of one pixel on all previous pixels by the hidden state of an RNN. Then the process of feeding previous predicted pixel back to the network to get the next pixel allows us to sequentially generate all pixels of an image. </p> <p></p> <p>Another thing that the PixelRNN model does slightly differently is that it defines pixel order diagonally. This allows some level of parallelization, which makes training and generation a bit faster. With this ordering, generation process starts from the top-left corner, and then makes its way down and right until the entire image is produced. </p> <p></p>"},{"location":"generative-models/#pixelcnn","title":"PixelCNN","text":"<p>Because the generation process even with this diagonal ordering is still largely sequential, it is expensive to train such model. To achieve further parallelization, instead of taking all previous pixels into consideration, we could instead only model dependencies on pixels in a context region. This gives rise to the PixelCNN model in which a context region is defined by a masked convolution. The receptive field of a masked convolution is an incomplete square around a central pixel (darker blue squares). This ensures that each pixel only depends on the already generated pixels in the region. The paper shows that with enough masked convolutional layers, the effective receptive field is the same as the pixel generation process that directly models dependencies on all previous pixels (all blue squares), like the PixelRNN model. </p> <p></p> <p>Because context region values are known from training images, PixelCNN is faster in training thanks to convolution parallelizations. However, generation is still slow as the process is inherently sequential. For instance, for a \\(32 \\times 32\\) image, the model needs to perform forward pass \\(1024\\) times to generate a single image. </p> <p></p> <p>From the generation samples on <code>CIFAR-10</code> (left) and <code>ImageNet</code> (right), we see these models are able to capture the distribution of training data to some extent, yet the generated samples do not look like natural images. Later models like flow based deep generative models are able to strike a better balance between training and generation efficiencies, and generate better quality images. </p> <p>In summary, PixelRNN and PixelCNN models explicitly compute likelihood, and thus are relatively easy to optimize. The major drawback of these models is the sequential generation process which is time consuming. There have been follow-up efforts on improving PixelCNN performance, ranging from architecture changes to training tricks. </p>"},{"location":"generative-models/#variational-autoencoder","title":"Variational Autoencoder","text":"<p>We introduce a new latent variable \\(z\\) that allows us to decompose the data likelihood as the marginal distribution of the conditional data likelihood with respect to this latent variable \\(z\\): </p> <p>\\( \\displaystyle p_{\\theta}(x) = \\int p_{\\theta}(z) \\cdot p_{\\theta}(x \\mid z) ~dz \\)</p> <p>In other words, all pixels of an image are independent with each other given latent variable \\(z\\). This makes simultaneous generation of all pixels possible. However, we cannot directly optimize this likelihood expression. Instead, we optimize a lower bound of this expression to approximate the optimization we'd like to perform. </p>"},{"location":"generative-models/#autoencoder","title":"Autoencoder","text":"<p>On a high-level, the goal of an autoencoder is to learn a lower-dimensional feature representation from un-labeled training data. The \"encoder\" component of an autoencoder aims at compressing input data into a lower-dimensional feature vector \\(z\\). Then the \"decoder\" component decodes this feature vector and converts it back to the data in the original dimensional space. </p> <p></p> <p>The idea of the dimensionality reduction step is that we want every dimension of the feature vector \\(z\\) captures meaningful factors of variation in data. We feed the feature vector into the decoder network and have it learn how to reconstruct the original input data with some pre-defined pixel-wise reconstruction loss (L2 is one of the most common choices). By training an autoencoder model, we hope feature vector \\(z\\) eventually encodes the most essential information about possible variables of the data. </p> <p>Now the autoencoder gives a way to effectively represent the underlying structure of the data distribution, which is one of the objectives of generative modeling. However, since do not know the entire latent space that \\(z\\) is in (not every latent feature in the latent space can be decoded into a meaningful image), we are unable to arbitrarily generate new images from an autoencoder. </p>"},{"location":"generative-models/#variational-autoencoder_1","title":"Variational Autoencoder","text":"<p>To be able to sample from the latent space, we take a probabilistic approach to autoencoder models. Assume training data \\(\\big\\{x^{(i)}\\big\\}_{i=1}^{N}\\) is generated from the distribution of unobserved latent representation \\(z\\). So \\(x\\) follows the conditional distribution given \\(z\\); that is, \\(p_{\\theta^{\\ast}}(x \\mid z^{(i)})\\). And \\(z^{(i)}\\) follows the prior distribution \\(p_{\\theta^{\\ast}}(z)\\). In other words, we assume each image \\(x\\) is generated by first sampling a new \\(z\\) that has a slight different factors of variation and then sampling the image conditionally on that chosen variable \\(z\\). </p> <p></p> <p>With variational autoencoder [Kingma and Welling, 2014], we would like to estimate true parameters \\(\\theta^{\\ast}\\) of both the prior and conditional distributions of the training data. We choose prior \\(p_{\\theta}(z)\\) to be a simple distribution (e.g. a diagonal/isotropic Gaussian distribution), and use a neural network, denoted as decoder network, to decode a latent sample from prior \\(pp_{\\theta}(z)\\) to a conditional distribution of the image \\(p_{\\theta}(x \\mid z)\\). We have the data likelihood: </p> <p>\\(\\displaystyle p_{\\theta}(x) = \\int p_{\\theta}(z) \\cdot p_{\\theta}(x \\mid z) ~dz \\)</p> <p>We note that to train the model, we need to compute the integral which involves computing \\(p(x \\mid z)\\) for every possible \\(z\\). Hence it is intractably to directly optimize the likelihood expression. We could use Monte Carlo estimation technique but there will incur high variance because of the high dimensionality nature of the density function. If we look at the posterior distribution using the Baye's rule: </p> <p>\\( p_{\\theta}(z \\mid x) = \\dfrac{p_{\\theta}(x \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(x)} \\)</p> <p>we see it is still intractable to compute because \\(p_{\\theta}(x)\\) shows up in the denominator. </p> <p>To make it tractable, we instead learn another distribution \\(q_{\\phi}(z \\mid x)\\) that approximates the true posterior distribution \\(p_{\\theta}(z \\mid x)\\). We denote this approximate distribution as probabilistic encoder because given an input image, it produces a distribution over the possible values of latent feature vector \\(z\\) from which the image could have been sampled from. This approximate posterior distribution \\(q_{\\phi}(z \\mid x)\\) allows us to derive a lower bound on the data likelihood. We then can optimize the tractable lower bound instead. The goal of the variational inference is to approximate the unknown posterior distribution \\(p_{\\theta}(z \\mid x)\\) from only the observed data. </p>"},{"location":"generative-models/#tractable-lower-bound","title":"Tractable Lower Bound","text":"<p>To derive the tractable lower bound, we start from the log likelihood of an observed example: </p> <p>\\( \\begin{aligned}   \\log p_{\\theta}(x^{(i)}) &amp;= \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x^{(i)})} \\Big[\\log p_{\\theta}(x^{(i)})\\Big] \\quad \\cdots \\small\\mathsf{(1)} \\\\   &amp;= \\mathbb{E}_{z} \\bigg[\\log \\frac{p_{\\theta}(x^{(i)} \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(z \\mid x^{(i)})}\\bigg] \\quad \\cdots \\small\\mathsf{(2)} \\\\   &amp;= \\mathbb{E}_{z} \\bigg[\\log \\bigg(\\frac{p_{\\theta}(x^{(i)} \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(z \\mid x^{(i)})} \\cdot \\frac{q_{\\phi}(z \\mid x^{(i)})}{q_{\\phi}(z \\mid x^{(i)})}\\bigg)\\bigg] \\quad \\cdots \\small\\mathsf{(3)} \\\\   &amp;= \\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big] - \\mathbb{E}_{z} \\bigg[\\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z)}\\bigg] + \\mathbb{E}_{z} \\bigg[\\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z \\mid x^{(i)})}\\bigg] \\quad \\cdots \\small\\mathsf{(4)} \\\\   &amp;= \\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big] - D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z)\\Big) + D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z \\mid x^{(i)})\\Big) \\quad \\cdots \\small\\mathsf{(5)} \\end{aligned} \\)</p> <ul> <li> <p>Step \\(\\mathrm{(1)}\\): the true data distribution is independent of the estimated posterior \\(q_{\\phi}(z \\mid x^{(i)})\\); moreover, since \\(q_{\\phi}(z \\mid x^{(i)})\\) is represented by a neural network, we are able to sample from distribution \\(q_{\\phi}\\). </p> </li> <li> <p>Step \\(\\mathrm{(2)}\\): by the Baye's rule: </p> </li> </ul> <p>\\( \\begin{aligned}   &amp; p_{\\theta}(z \\mid x) = \\dfrac{p_{\\theta}(x \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(x)} \\\\   \\Longrightarrow \\quad &amp; p_{\\theta}(x) = \\dfrac{p_{\\theta}(x \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(z \\mid x)} \\end{aligned} \\)</p> <ul> <li> <p>Step \\(\\mathrm{(3)}\\): multiplying the expression by \\(1 = \\dfrac{q_{\\phi}(z \\mid x^{(i)})}{q_{\\phi}(z \\mid x^{(i)})}\\)</p> </li> <li> <p>Step \\(\\mathrm{(4)}\\): by logarithm properties as well as linearity of expectation: </p> </li> </ul> <p>\\( \\begin{aligned}   &amp;~ \\mathbb{E}_{z} \\bigg[\\log \\bigg(\\frac{p_{\\theta}(x^{(i)} \\mid z) \\cdot p_{\\theta}(z)}{p_{\\theta}(z \\mid x^{(i)})} \\cdot \\frac{q_{\\phi}(z \\mid x^{(i)})}{q_{\\phi}(z \\mid x^{(i)})}\\bigg)\\bigg] \\\\   =&amp;~ \\mathbb{E}_{z} \\bigg[\\log \\bigg(p_{\\theta}(x^{(i)} \\mid z) \\cdot \\frac{p_{\\theta}(z)}{q_{\\phi}(z \\mid x^{(i)})} \\cdot \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z \\mid x^{(i)})}\\bigg)\\bigg] \\\\   =&amp;~ \\mathbb{E}_{z} \\bigg[\\log p_{\\theta}(x^{(i)} \\mid z) + \\log \\frac{p_{\\theta}(z)}{q_{\\phi}(z \\mid x^{(i)})} + \\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z \\mid x^{(i)})}\\bigg] \\\\   =&amp;~ \\mathbb{E}_{z} \\bigg[\\log p_{\\theta}(x^{(i)} \\mid z) - \\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z)} + \\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z \\mid x^{(i)})}\\bigg] \\\\   =&amp;~ \\mathbb{E}_{z} \\bigg[\\log p_{\\theta}(x^{(i)} \\mid z)\\bigg] - \\mathbb{E}_{z}\\bigg[\\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z)}\\bigg] + \\mathbb{E}_{z}\\bigg[\\log \\frac{q_{\\phi}(z \\mid x^{(i)})}{p_{\\theta}(z \\mid x^{(i)})}\\bigg] \\\\ \\end{aligned} \\)</p> <ul> <li>Step \\(\\mathrm{(5)}\\): by definition of the Kullback\u2013Leibler divergence. The KL divergence gives a measure of the \u201cdistance\u201d between two distributions. </li> </ul> <p>We see the first term \\(\\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big]\\) involves \\(p_{\\theta}(x^{(i)} \\mid z)\\) that is given by the decoder network. With some tricks, this term can be estimated through sampling. </p> <p>The second term is the KL divergence between the approximate posterior and the prior (a Gaussian distribution). Assuming the approximate posterior posterior takes on a Gaussian form with diagonal covariance matrix, the KL divergence then has an analytical closed form solution. </p> <p>The third term is the KL divergence between between the approximate posterior and the true posterior. Even though we it is intractable to computer, by non-negativity of KL divergence, we know this term is non-negative. </p> <p>Therefore we obtain the tractable lower bound of log likelihood of the data: </p> <p>\\( \\log p_{\\theta}(x^{(i)}) = \\underbrace{\\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big] - D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z)\\Big)}_{\\mathcal{L}(x^{(i)}; \\theta, \\phi)} + \\underbrace{D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z \\mid x^{(i)})\\Big)}_{\\geqslant 0} \\)</p> <p>We note that \\(\\mathcal{L}(x^{(i)}; \\theta, \\phi)\\), as known as the evidence lower bound (ELBO), is differentiable, hence we can apply gradient descent methods to optimize the lower bound. </p> <p>The lower bound can also be interpreted as encoder component \\(D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z)\\Big)\\) that seeks to approximate posterior distribution close to prior; and decoder component \\(\\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big]\\) that concerns with reconstructing the original input data. </p>"},{"location":"generative-models/#training","title":"Training","text":"<p>For a given input, we first use the encoder network to to generate the mean \\(\\mu_{z \\mid x}\\) and variance \\(\\Sigma_{z \\mid x}\\) of the approximate posterior Gaussian distribution. Notice that here \\(\\Sigma_{z \\mid x}\\) is represented by a vector instead of a matrix because the approximate posterior is assumed to have a diagonal covariance matrix. Then we can compute the gradient of the KL divergence term \\(D_{\\mathrm{KL}} \\Big(q_{\\phi}(z \\mid x^{(i)}) \\parallel p_{\\theta}(z)\\Big)\\) as it has an analytical solution. </p> <p>Next we compute the gradient of the expectation term \\(\\mathbb{E}_{z} \\Big[\\log p_{\\theta}(x^{(i)} \\mid z) \\Big]\\). Since \\(p_{\\theta}(x^{(i)} \\mid z)\\) is represented by the decoder network, we need to sample \\(z\\) from the approximate posterior \\(\\mathcal{N}(\\mu_{z \\mid x}, \\Sigma_{z \\mid x})\\). However, since \\(z\\) is not part of the computation graph, we won't be able to find the gradient of the expression that entails the sampling process. To solve this problem, we perform reparameterization. Specifically, we take advantage of the Gaussian distribution assumption, and sample \\(\\varepsilon \\sim \\mathcal{N}(0, I)\\). Then we represent \\(z\\) as \\(z = \\mu_{z \\mid x} + \\varepsilon \\Sigma_{z \\mid x}\\). This way \\(z\\) has the same Gaussian distribution as before; moreover, since \\(\\varepsilon\\) is seen as an input to the computation graph, and both \\(\\mu_{z \\mid x}\\) and \\(\\Sigma_{z \\mid x}\\) are part of the computation graph, the sampling process now becomes differentiable. </p> <p>Lastly, we use the decoder network to produce the pixel-wise conditional distribution \\(p_{\\theta}(x \\mid z)\\). We are now able to perform the maximum likelihood of the original input. In practice, L2 distance between the predicted image and the actual input image is commonly used. </p> <p>For every minibatch of input data, we compute the forward pass and then perform the back-propagation. </p>"},{"location":"generative-models/#inference","title":"Inference","text":"<p>We take a sample \\(z\\) from the prior distribution \\(p_{\\theta}(z)\\) (e.g. a Gaussian distribution), then feed the sample into the trained decoder network to obtain the conditional distributions \\(p_{\\theta}(x \\mid z)\\). Lastly we sample a new image from the conditional distribution. </p>"},{"location":"generative-models/#generated-samples","title":"Generated Samples","text":"<p>Since we assumed diagonal prior for \\(z\\), components of latent variable are independent of each other. This means different dimensions of \\(z\\) encode interpretable factors of variation. </p> <p></p> <p>After training the model using a \\(2\\)-dimensional latent variable \\(z\\) on <code>MNIST</code>, we discover that varying the samples of \\(z\\) would induce interpretable variations in the image space. For instance, one possible interpretation would be that \\(z_1\\) morphs digit <code>6</code> to <code>9</code> through <code>7</code>, and \\(z_2\\) is related to the orientation of digits. </p> <p></p> <p>Similarly, we also find that dimensions of latent variable \\(z\\) can be interpretable after training the model on head pose dataset. For instance, it appears \\(z_1\\) encodes degree of smile and \\(z_2\\) encodes head pose orientation. </p> <p></p> <p>From above generation samples on <code>CIFAR-10</code> (left) and labeled face images (right), we see newly generated images are similar to the original ones. However, these generated images are still blurry and generating high quality images is an active area for research. </p>"},{"location":"generative-models/#generative-adversarial-networks-gans","title":"Generative Adversarial Networks (GANs)","text":"<p>We would like to train a model to directly generate high quality samples without modeling any explicit density function \\(p(x)\\). With GAN [Goodfellow et al., 2014], our goal is to train a generator network to learn transformation from a simple distribution (e.g. random noise) that we can easily sample from to the high-dimensional training distribution followed by the data. The challenge is that because we do not model any data distribution, we don't have the mapping between random sample \\(z\\) to a training image \\(x\\). This means we cannot directly train the model with supervised reconstruction loss. </p> <p>To overcome this challenge, we recognize the general objective that all the images generated from the latent space of \\(z\\) should exhibit \"realness\". In other words, all the generated images should look like they belong to the original training data. To formulate this general objective into a learning objective, we introduce another discriminator network that learns to identify whether an image is from the training data distribution or not. Specifically, the discriminator network performs a two-class classification task in which an input image feeds into the network and a label indicating if the input image is from the training data distribution or is produced by the generated network. </p> <p>We then can use the output from the discriminator network to compute gradient and perform back-propagation to the generator network to gradually improve the image generation process. Overtime, learning signal from the discriminator will inform the generator on how to produce more \"realistic\" samples. Similarly, as generated images from the generator become more and more close to the real training data, the discriminator adapt its decision boundary to fit the training data distribution better. The discriminator effectively learns to model the data distribution without explicitly defining it. </p> <p></p> <p>In summary: </p> <ul> <li>discriminator network: try to distinguish between real and fake images</li> <li>generator network: try to fool the discriminator by generating real-looking images</li> </ul>"},{"location":"generative-models/#training-gans","title":"Training GANs","text":"<p>Training GAN can be formulated as the minimax optimization of a two-player adversarial game. Assume that the discriminator outputs likelihood in \\((0,1)\\) of real image, the objective function is the following: </p> <p>\\( \\displaystyle \\min_{\\theta_g} \\max_{\\theta_d} \\Big\\{\\mathbb{E}_{x \\sim p_{\\mathrm{data}}}\\big[\\log \\underbrace{D_{\\theta_d}(x)}_{\\mathsf{(1)}}\\big] + \\mathbb{E}_{z \\sim p(z)}\\big[\\log\\big(1 - \\underbrace{D_{\\theta_d}(G_{\\theta_g}(z))\\big)}_{\\mathsf{(2)}}\\big]\\Big\\} \\)</p> <ul> <li>\\(\\mathsf{(1)}\\): \\(D_{\\theta_d}(x)\\) is the discriminator output (score) for real data \\(x\\)</li> <li>\\(\\mathsf{(2)}\\): \\(D_{\\theta_d}(G_{\\theta_g}(z))\\big)\\) is the discriminator output (score) for generated fake data \\(G(z)\\)</li> </ul> <p>The inner maximization is the discriminator objective. The discriminator aims to find maximizer \\(\\theta_g\\) such that real data \\(D(x)\\) is close to \\(1\\) (real) while generated fake data \\(D(G(z))\\) is close to \\(0\\) (fake). </p> <p>The outer minimization is the generator objective. The generator aims to find minimizer \\(\\theta_g\\) such that generated fake data \\(D(G(z))\\) is close to \\(1\\) (real). This means the generator seeks to fool discriminator into thinking that generated fake data \\(D(G(z))\\) is real. </p> <p>Naively, we could alternate between maximization and minimization by performing gradient ascent on discriminator: </p> <p>\\( \\displaystyle \\max_{\\theta_d} \\Big\\{\\mathbb{E}_{x \\sim p_{\\mathrm{data}}}\\big[\\log D_{\\theta_d}(x)\\big] + \\mathbb{E}_{z \\sim p(z)}\\big[\\log \\big(1 - D_{\\theta_d}(G_{\\theta_g}(z))\\big)\\big]\\Big\\} \\)</p> <p>and gradient descent on generator: </p> <p>\\( \\displaystyle \\min_{\\theta_g} \\Big\\{\\mathbb{E}_{z \\sim p(z)}\\big[\\log \\big(1 - D_{\\theta_d}(G_{\\theta_g}(z))\\big)\\big]\\Big\\} \\)</p> <p>However, we note that when a sample is likely fake\u2014hence \\(D(G(z))\\) is small, expression \\(\\log \\big(1 - D_{\\theta_d}(G_{\\theta_g}(z))\\big)\\) in the generator objective function has small derivative with respect to \\(D(G(z))\\). This means in the beginning of the training, the gradient of the generator objective function is small; that is updates to parameters \\(\\theta_g\\) are small. Conversely, the updates are large (strong gradient signal) when samples are already realistic (\\(D(G(z))\\) is large). This creates an unfavorable situation as ideally we would hope the generator is able to learn fast when the discriminator outsmarts the generator. </p> <p></p> <p>To remedy this problem, we now maximize likelihood of the discriminator being wrong, as opposed to minimizing the likelihood of it being correct: </p> <p>\\( \\displaystyle \\max{\\theta_g} \\Big\\{\\mathbb{E}_{z \\sim p(z)}\\big[\\log D_{\\theta_d}\\big(G_{\\theta_g}(z)\\big)\\big]\\Big\\} \\)</p> <p>The objective remains unchanged, yet there will be higher gradient signal to the generator for unrealistic samples (in the eyes of the discriminator), which improves training performance. </p> <p>for number of training iterations do: for \\(k\\) steps do: \u00a0\u00a0\u00a0\u00a0 - Sample minibatch of \\(m\\) noise samples \\(\\{z^{(1)}, \\cdots, z^{(m)}\\}\\) from noise prior \\(p(z)\\) \u00a0\u00a0\u00a0\u00a0 - Sample minibatch of \\(m\\) samples \\(\\{x^{(1)}, \\cdots, x^{(m)}\\}\\) from data generating distribution \\(p_{\\mathrm{data}}(x)\\) \u00a0\u00a0\u00a0\u00a0 - Update the discriminator by ascending its stochastic gradient: </p> <p>\\( \\displaystyle \\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^{m} \\Big[\\log D_{\\theta_d}(x^{(i)}) + \\log \\big(1 - D_{\\theta_d}(G_{\\theta_g}(z^{(i)}))\\big)\\Big] \\) end for \u00a0\u00a0 - Sample minibatch of \\(m\\) samples \\(\\{x^{(1)}, \\cdots, x^{(m)}\\}\\) from data generating distribution \\(p_{\\mathrm{data}}(x)\\) \u00a0\u00a0 - Update the generator by ascending its stochastic gradient of the improved objective: </p> <p>\\( \\displaystyle \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^{m} \\log D_{\\theta_d}(G_{\\theta_g}(z^{(i)})) \\) end for </p> <p>Here \\(k \\geqslant 1\\) is the hyper-parameter and there is not best rule as to the value of \\(k\\). In general, GANs are difficult to train, and followup work like Wasserstein GAN [Arjovsky et al., 2017] and BEGAN [Berthelot et al., 2017] sets out to achieve better training stability. </p>"},{"location":"generative-models/#inference_1","title":"Inference","text":"<p>After training, we use the generator network to generate images. Specifically, we first draw a sample \\(z\\) from noise prior \\(p(z)\\); then we feed the sampled \\(z\\) into the generator network. The output from the network gives us an image that is similar to the training images. </p>"},{"location":"generative-models/#generated-samples_1","title":"Generated Samples","text":"<p>From the generated samples, we see GAN can generate high quality samples, indicating the model does not simply memorize exact images from the training data. Training sets from left to right: <code>MNIST</code>, <code>Toronto Face Dataset (TFD)</code>, <code>CIFAR-10</code>. The highlighted columns show the nearest training example of the neighboring generated sample. </p> <p>There have been numerous followup studies on improving sample quality, training stability, and other aspects of GANs. The ICLR 2016 paper [Radford et al., 2015] proposed deep convolutional networks and other architecture features (deep convolutional generative adversarial networks, or DCGANs) to achieve better image quality and training stability: </p> <ul> <li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).</li> <li>Use batchnorm in both the generator and the discriminator.</li> <li>Remove fully connected hidden layers for deeper architectures.</li> <li>Use ReLU activation in generator for all layers except for the output, which uses Tanh.</li> <li>Use LeakyReLU activation in the discriminator for all layers.</li> </ul> <p></p> <p>Generated samples from DCGANs trained on <code>LSUN</code> bedrooms dataset show promising improvements as the model can produce high resolution and high quality images without memorizing (overfitting) training examples. </p> <p>Similar to VAE, we are also able to find structures in the latent space and meaningfully interpolate random points in the latent space. This means we observe smooth semantic changes to the image generations along any direction of the manifold, which suggests that model has learned relevant  representations (as opposed to memorization). </p> <p></p> <p>The above figure shows smooth transitions between a series of \\(9\\) random points in the latent space. Every image in the interpolation reasonably looks like a bedroom. For instance, generated samples in the \\(6\\)<sup>th</sup> row exhibit transition from a room without a window to a room with a large window; in the \\(10\\)<sup>th</sup>, an TV-alike object morphs into a window. </p> <p>Additionally, we can also perform arithmetic on \\(z\\) vectors in the latent space. By averaging the \\(z\\) vector for three exemplary generated samples of different visual concepts, we see consistent and stable generations that semantically obeyed the arithmetic. </p> <p></p> <p></p> <p>Arithmetic is performed on the mean vectors and the resulting vector feeds into the generator to produce the center sample on the right hand side. The remaining samples around the center are produced by adding uniform noise in \\([-0.25, 0.25]\\) to the vector. </p> <p></p> <p></p> <p>We note that same arithmetic performed pixel-wise in the image space does not behave similarly, as it only yields in noise overlap due to misalignment. Therefore latent representations learned by the model and associated vector arithmetic have the potential to compactly model conditional generative process of complex image distributions.</p>"},{"location":"generative-models/#other-variants","title":"Other Variants","text":"<ul> <li>new loss function (LSGAN): Mao et al., Least Squares Generative Adversarial Networks, 2016</li> <li>new training methods: <ul> <li>Wasserstein GAN: Arjovsky et al., Wasserstein GAN, 2017</li> <li>Improved Wasserstein GAN: Gulrajani et al., Improved Training of Wasserstein GANs, 2017</li> <li>Progressive GAN: Karras et al., Progressive Growing of GANs for Improved Quality, Stability, and Variation, 2017</li> </ul> </li> <li>source-to-target domain transfer (CycleGAN): Zhu et al., Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017</li> <li>text-to-image synthesis: Reed et al., Generative Adversarial Text to Image Synthesis, 2016</li> <li>image-to-image translation (Pix2pix): Isola et al., Image-to-Image Translation with Conditional Adversarial Networks, 2016</li> <li>high-resolution and high-quality generations (BigGAN): Brock et al., Large Scale GAN Training for High Fidelity Natural Image Synthesis, 2018</li> <li>scene graphs to GANs: Johnson et al., Image Generation from Scene Graphs, 2018</li> <li>benchmark for generative models: Zhou, Gordon, Krishna et al., HYPE: Human eYe Perceptual Evaluations, 2019</li> <li>many more: \"the GAN zoo\"</li> </ul>"},{"location":"jupyter-colab-tutorial/","title":"Jupyter Notebook / Google Colab Tutorial","text":"<p>A Jupyter notebook lets you write and execute Python code locally in your web browser. Jupyter notebooks make it very easy to tinker with code and execute it in bits and pieces; for this reason they are widely used in scientific computing. Colab on the other hand is Google's flavor of Jupyter notebooks that is particularly suited for machine learning and data analysis and that runs entirely in the cloud. Colab is basically Jupyter notebook on steroids: it's free, requires no setup, comes preinstalled with many packages, is easy to share with the world, and benefits from free access to hardware accelerators like GPUs and TPUs (with some caveats).</p> <p>To get yourself familiar with Python and notebooks, we'll be running a short tutorial as a standalone Jupyter or Colab notebook. If you wish to use Colab, click the <code>Open in Colab</code> badge below.</p> <p>If you wish to run the notebook locally make sure your virtual environment was installed correctly (as per the setup instructions), activate it, then run <code>pip install notebook</code> to install Jupyter notebook. Next, open the notebook and download it to a directory of your choice by right-clicking on the page and selecting <code>Save Page As</code>. Then <code>cd</code> to that directory and run the following in your terminal:</p> <pre><code>jupyter notebook\n</code></pre> <p>Once your notebook server is up and running, point your web browser to <code>http://localhost:8888</code> to start using your notebooks. If everything worked correctly, you should see a screen like this, showing all available notebooks in the current directory:</p> <p>Click <code>jupyter-notebook-tutorial.ipynb</code> and follow the instructions in the notebook. Enjoy!</p>"},{"location":"jupyter-notebook-tutorial/","title":"Jupyter notebook tutorial","text":"<p>This tutorial was originally written by Justin Johnson for cs231n and adapted as a Jupyter notebook for cs228 by Volodymyr Kuleshov and Isaac Caswell.</p> <p>This current version has been adapted as a Jupyter notebook with Python3 support by Kevin Zakka for the Spring 2020 edition of cs231n.</p> <p>A Jupyter notebook is made up of a number of cells. Each cell can contain Python code. There are two main types of cells: <code>Code</code> cells and <code>Markdown</code> cells. This particular cell is a <code>Markdown</code> cell. You can execute a particular cell by double clicking on it (the highlight color will switch from blue to green) and pressing <code>Shift-Enter</code>. When you do so, if the cell is a <code>Code</code> cell, the code in the cell will run, and the output of the cell will be displayed beneath the cell, and if the cell is a <code>Markdown</code> cell, the markdown text will get rendered beneath the cell.</p> <p>Go ahead and try executing this cell.</p> <p>The cell below is a <code>Code</code> cell. Go ahead and click it, then execute it.</p> In\u00a0[101]: Copied! <pre>x = 1\nprint(x)\n</pre> x = 1 print(x) <pre>1\n</pre> <p>Global variables are shared between cells. Try executing the cell below:</p> In\u00a0[102]: Copied! <pre>y = 2 * x\nprint(y)\n</pre> y = 2 * x print(y) <pre>2\n</pre> <p>You can restart a notebook and clear all cells by clicking <code>Kernel -&gt; Restart &amp; Clear Output</code>. If you don't want to clear cell outputs, just hit <code>Kernel -&gt; Restart</code>.</p> <p>By convention, Jupyter notebooks are expected to be run from top to bottom. Failing to execute some cells or executing cells out of order can result in errors. After restarting the notebook, try running the <code>y = 2 * x</code> cell 2 cells above and observe what happens.</p> <p>After you have modified a Jupyter notebook for one of the assignments by modifying or executing some of its cells, remember to save your changes! You can save with the <code>Command/Control + s</code> shortcut or by clicking <code>File -&gt; Save and Checkpoint</code>.</p> <p>This has only been a brief introduction to Jupyter notebooks, but it should be enough to get you up and running on the assignments for this course.</p> <p>Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.</p> <p>We expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.</p> <p>Some of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).</p> <p>In this tutorial, we will cover:</p> <ul> <li>Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes</li> <li>Numpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting</li> <li>Matplotlib: Plotting, Subplots, Images</li> <li>IPython: Creating notebooks, Typical workflows</li> </ul> In\u00a0[2]: Copied! <pre>!python --version\n</pre> !python --version <pre>Python 3.7.6\r\n</pre> <p>Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:</p> In\u00a0[3]: Copied! <pre>def quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nprint(quicksort([3,6,8,10,1,2,1]))\n</pre> def quicksort(arr):     if len(arr) &lt;= 1:         return arr     pivot = arr[len(arr) // 2]     left = [x for x in arr if x &lt; pivot]     middle = [x for x in arr if x == pivot]     right = [x for x in arr if x &gt; pivot]     return quicksort(left) + middle + quicksort(right)  print(quicksort([3,6,8,10,1,2,1])) <pre>[1, 1, 2, 3, 6, 8, 10]\n</pre> <p>Integers and floats work as you would expect from other languages:</p> In\u00a0[4]: Copied! <pre>x = 3\nprint(x, type(x))\n</pre> x = 3 print(x, type(x)) <pre>3 &lt;class 'int'&gt;\n</pre> In\u00a0[5]: Copied! <pre>print(x + 1)   # Addition\nprint(x - 1)   # Subtraction\nprint(x * 2)   # Multiplication\nprint(x ** 2)  # Exponentiation\n</pre> print(x + 1)   # Addition print(x - 1)   # Subtraction print(x * 2)   # Multiplication print(x ** 2)  # Exponentiation <pre>4\n2\n6\n9\n</pre> In\u00a0[6]: Copied! <pre>x += 1\nprint(x)\nx *= 2\nprint(x)\n</pre> x += 1 print(x) x *= 2 print(x) <pre>4\n8\n</pre> In\u00a0[7]: Copied! <pre>y = 2.5\nprint(type(y))\nprint(y, y + 1, y * 2, y ** 2)\n</pre> y = 2.5 print(type(y)) print(y, y + 1, y * 2, y ** 2) <pre>&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n</pre> <p>Note that unlike many languages, Python does not have unary increment (x++) or decrement (x--) operators.</p> <p>Python also has built-in types for long integers and complex numbers; you can find all of the details in the documentation.</p> <p>Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols (<code>&amp;&amp;</code>, <code>||</code>, etc.):</p> In\u00a0[8]: Copied! <pre>t, f = True, False\nprint(type(t))\n</pre> t, f = True, False print(type(t)) <pre>&lt;class 'bool'&gt;\n</pre> <p>Now we let's look at the operations:</p> In\u00a0[9]: Copied! <pre>print(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n</pre> print(t and f) # Logical AND; print(t or f)  # Logical OR; print(not t)   # Logical NOT; print(t != f)  # Logical XOR; <pre>False\nTrue\nFalse\nTrue\n</pre> In\u00a0[10]: Copied! <pre>hello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n</pre> hello = 'hello'   # String literals can use single quotes world = \"world\"   # or double quotes; it does not matter print(hello, len(hello)) <pre>hello 5\n</pre> In\u00a0[11]: Copied! <pre>hw = hello + ' ' + world  # String concatenation\nprint(hw)\n</pre> hw = hello + ' ' + world  # String concatenation print(hw) <pre>hello world\n</pre> In\u00a0[12]: Copied! <pre>hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n</pre> hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting print(hw12) <pre>hello world 12\n</pre> <p>String objects have a bunch of useful methods; for example:</p> In\u00a0[13]: Copied! <pre>s = \"hello\"\nprint(s.capitalize())  # Capitalize a string\nprint(s.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(s.rjust(7))      # Right-justify a string, padding with spaces\nprint(s.center(7))     # Center a string, padding with spaces\nprint(s.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n</pre> s = \"hello\" print(s.capitalize())  # Capitalize a string print(s.upper())       # Convert a string to uppercase; prints \"HELLO\" print(s.rjust(7))      # Right-justify a string, padding with spaces print(s.center(7))     # Center a string, padding with spaces print(s.replace('l', '(ell)'))  # Replace all instances of one substring with another print('  world '.strip())  # Strip leading and trailing whitespace <pre>Hello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n</pre> <p>You can find a list of all string methods in the documentation.</p> <p>Python includes several built-in container types: lists, dictionaries, sets, and tuples.</p> <p>A list is the Python equivalent of an array, but is resizeable and can contain elements of different types:</p> In\u00a0[14]: Copied! <pre>xs = [3, 1, 2]   # Create a list\nprint(xs, xs[2])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n</pre> xs = [3, 1, 2]   # Create a list print(xs, xs[2]) print(xs[-1])     # Negative indices count from the end of the list; prints \"2\" <pre>[3, 1, 2] 2\n2\n</pre> In\u00a0[15]: Copied! <pre>xs[2] = 'foo'    # Lists can contain elements of different types\nprint(xs)\n</pre> xs[2] = 'foo'    # Lists can contain elements of different types print(xs) <pre>[3, 1, 'foo']\n</pre> In\u00a0[16]: Copied! <pre>xs.append('bar') # Add a new element to the end of the list\nprint(xs)\n</pre> xs.append('bar') # Add a new element to the end of the list print(xs)   <pre>[3, 1, 'foo', 'bar']\n</pre> In\u00a0[17]: Copied! <pre>x = xs.pop()     # Remove and return the last element of the list\nprint(x, xs)\n</pre> x = xs.pop()     # Remove and return the last element of the list print(x, xs) <pre>bar [3, 1, 'foo']\n</pre> <p>As usual, you can find all the gory details about lists in the documentation.</p> <p>In addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:</p> In\u00a0[18]: Copied! <pre>nums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n</pre> nums = list(range(5))    # range is a built-in function that creates a list of integers print(nums)         # Prints \"[0, 1, 2, 3, 4]\" print(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\" print(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\" print(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\" print(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\" print(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\" nums[2:4] = [8, 9] # Assign a new sublist to a slice print(nums)         # Prints \"[0, 1, 8, 9, 4]\" <pre>[0, 1, 2, 3, 4]\n[2, 3]\n[2, 3, 4]\n[0, 1]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n[0, 1, 8, 9, 4]\n</pre> <p>You can loop over the elements of a list like this:</p> In\u00a0[19]: Copied! <pre>animals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n</pre> animals = ['cat', 'dog', 'monkey'] for animal in animals:     print(animal) <pre>cat\ndog\nmonkey\n</pre> <p>If you want access to the index of each element within the body of a loop, use the built-in <code>enumerate</code> function:</p> In\u00a0[20]: Copied! <pre>animals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n</pre> animals = ['cat', 'dog', 'monkey'] for idx, animal in enumerate(animals):     print('#{}: {}'.format(idx + 1, animal)) <pre>#1: cat\n#2: dog\n#3: monkey\n</pre> <p>When programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:</p> In\u00a0[21]: Copied! <pre>nums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n</pre> nums = [0, 1, 2, 3, 4] squares = [] for x in nums:     squares.append(x ** 2) print(squares) <pre>[0, 1, 4, 9, 16]\n</pre> <p>You can make this code simpler using a list comprehension:</p> In\u00a0[22]: Copied! <pre>nums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n</pre> nums = [0, 1, 2, 3, 4] squares = [x ** 2 for x in nums] print(squares) <pre>[0, 1, 4, 9, 16]\n</pre> <p>List comprehensions can also contain conditions:</p> In\u00a0[23]: Copied! <pre>nums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n</pre> nums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print(even_squares) <pre>[0, 4, 16]\n</pre> <p>A dictionary stores (key, value) pairs, similar to a <code>Map</code> in Java or an object in Javascript. You can use it like this:</p> In\u00a0[24]: Copied! <pre>d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n</pre> d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data print(d['cat'])       # Get an entry from a dictionary; prints \"cute\" print('cat' in d)     # Check if a dictionary has a given key; prints \"True\" <pre>cute\nTrue\n</pre> In\u00a0[25]: Copied! <pre>d['fish'] = 'wet'    # Set an entry in a dictionary\nprint(d['fish'])      # Prints \"wet\"\n</pre> d['fish'] = 'wet'    # Set an entry in a dictionary print(d['fish'])      # Prints \"wet\" <pre>wet\n</pre> In\u00a0[26]: Copied! <pre>print(d['monkey'])  # KeyError: 'monkey' not a key of d\n</pre> print(d['monkey'])  # KeyError: 'monkey' not a key of d <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-26-78fc9745d9cf&gt; in &lt;module&gt;\n----&gt; 1 print(d['monkey'])  # KeyError: 'monkey' not a key of d\n\nKeyError: 'monkey'</pre> In\u00a0[27]: Copied! <pre>print(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\"\nprint(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\"\n</pre> print(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\" print(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\" <pre>N/A\nwet\n</pre> In\u00a0[28]: Copied! <pre>del d['fish']        # Remove an element from a dictionary\nprint(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\"\n</pre> del d['fish']        # Remove an element from a dictionary print(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\" <pre>N/A\n</pre> <p>You can find all you need to know about dictionaries in the documentation.</p> <p>It is easy to iterate over the keys in a dictionary:</p> In\u00a0[29]: Copied! <pre>d = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A {} has {} legs'.format(animal, legs))\n</pre> d = {'person': 2, 'cat': 4, 'spider': 8} for animal, legs in d.items():     print('A {} has {} legs'.format(animal, legs)) <pre>A person has 2 legs\nA cat has 4 legs\nA spider has 8 legs\n</pre> <p>Dictionary comprehensions: These are similar to list comprehensions, but allow you to easily construct dictionaries. For example:</p> In\u00a0[30]: Copied! <pre>nums = [0, 1, 2, 3, 4]\neven_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\nprint(even_num_to_square)\n</pre> nums = [0, 1, 2, 3, 4] even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0} print(even_num_to_square) <pre>{0: 0, 2: 4, 4: 16}\n</pre> <p>A set is an unordered collection of distinct elements. As a simple example, consider the following:</p> In\u00a0[31]: Copied! <pre>animals = {'cat', 'dog'}\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' in animals)  # prints \"False\"\n</pre> animals = {'cat', 'dog'} print('cat' in animals)   # Check if an element is in a set; prints \"True\" print('fish' in animals)  # prints \"False\"  <pre>True\nFalse\n</pre> In\u00a0[32]: Copied! <pre>animals.add('fish')      # Add an element to a set\nprint('fish' in animals)\nprint(len(animals))       # Number of elements in a set;\n</pre> animals.add('fish')      # Add an element to a set print('fish' in animals) print(len(animals))       # Number of elements in a set; <pre>True\n3\n</pre> In\u00a0[33]: Copied! <pre>animals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals))       \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))\n</pre> animals.add('cat')       # Adding an element that is already in the set does nothing print(len(animals))        animals.remove('cat')    # Remove an element from a set print(len(animals))        <pre>3\n2\n</pre> <p>Loops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:</p> In\u00a0[35]: Copied! <pre>animals = {'cat', 'dog', 'fish'}\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n</pre> animals = {'cat', 'dog', 'fish'} for idx, animal in enumerate(animals):     print('#{}: {}'.format(idx + 1, animal)) <pre>#1: dog\n#2: fish\n#3: cat\n</pre> <p>Set comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:</p> In\u00a0[36]: Copied! <pre>from math import sqrt\nprint({int(sqrt(x)) for x in range(30)})\n</pre> from math import sqrt print({int(sqrt(x)) for x in range(30)}) <pre>{0, 1, 2, 3, 4, 5}\n</pre> <p>A tuple is an (immutable) ordered list of values. A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:</p> In\u00a0[37]: Copied! <pre>d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n</pre> d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys t = (5, 6)       # Create a tuple print(type(t)) print(d[t])        print(d[(1, 2)]) <pre>&lt;class 'tuple'&gt;\n5\n1\n</pre> In\u00a0[38]: Copied! <pre>t[0] = 1\n</pre> t[0] = 1 <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-38-c8aeb8cd20ae&gt; in &lt;module&gt;\n----&gt; 1 t[0] = 1\n\nTypeError: 'tuple' object does not support item assignment</pre> <p>Python functions are defined using the <code>def</code> keyword. For example:</p> In\u00a0[39]: Copied! <pre>def sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\nfor x in [-1, 0, 1]:\n    print(sign(x))\n</pre> def sign(x):     if x &gt; 0:         return 'positive'     elif x &lt; 0:         return 'negative'     else:         return 'zero'  for x in [-1, 0, 1]:     print(sign(x)) <pre>negative\nzero\npositive\n</pre> <p>We will often define functions to take optional keyword arguments, like this:</p> In\u00a0[40]: Copied! <pre>def hello(name, loud=False):\n    if loud:\n        print('HELLO, {}'.format(name.upper()))\n    else:\n        print('Hello, {}!'.format(name))\n\nhello('Bob')\nhello('Fred', loud=True)\n</pre> def hello(name, loud=False):     if loud:         print('HELLO, {}'.format(name.upper()))     else:         print('Hello, {}!'.format(name))  hello('Bob') hello('Fred', loud=True) <pre>Hello, Bob!\nHELLO, FRED\n</pre> <p>The syntax for defining classes in Python is straightforward:</p> In\u00a0[41]: Copied! <pre>class Greeter:\n\n    # Constructor\n    def __init__(self, name):\n        self.name = name  # Create an instance variable\n\n    # Instance method\n    def greet(self, loud=False):\n        if loud:\n          print('HELLO, {}'.format(self.name.upper()))\n        else:\n          print('Hello, {}!'.format(self.name))\n\ng = Greeter('Fred')  # Construct an instance of the Greeter class\ng.greet()            # Call an instance method; prints \"Hello, Fred\"\ng.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\"\n</pre> class Greeter:      # Constructor     def __init__(self, name):         self.name = name  # Create an instance variable      # Instance method     def greet(self, loud=False):         if loud:           print('HELLO, {}'.format(self.name.upper()))         else:           print('Hello, {}!'.format(self.name))  g = Greeter('Fred')  # Construct an instance of the Greeter class g.greet()            # Call an instance method; prints \"Hello, Fred\" g.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\" <pre>Hello, Fred!\nHELLO, FRED\n</pre> <p>Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. If you are already familiar with MATLAB, you might find this tutorial useful to get started with Numpy.</p> <p>To use Numpy, we first need to import the <code>numpy</code> package:</p> In\u00a0[42]: Copied! <pre>import numpy as np\n</pre> import numpy as np <p>A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.</p> <p>We can initialize numpy arrays from nested Python lists, and access elements using square brackets:</p> In\u00a0[43]: Copied! <pre>a = np.array([1, 2, 3])  # Create a rank 1 array\nprint(type(a), a.shape, a[0], a[1], a[2])\na[0] = 5                 # Change an element of the array\nprint(a)\n</pre> a = np.array([1, 2, 3])  # Create a rank 1 array print(type(a), a.shape, a[0], a[1], a[2]) a[0] = 5                 # Change an element of the array print(a)                   <pre>&lt;class 'numpy.ndarray'&gt; (3,) 1 2 3\n[5 2 3]\n</pre> In\u00a0[44]: Copied! <pre>b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array\nprint(b)\n</pre> b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array print(b) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[45]: Copied! <pre>print(b.shape)\nprint(b[0, 0], b[0, 1], b[1, 0])\n</pre> print(b.shape) print(b[0, 0], b[0, 1], b[1, 0]) <pre>(2, 3)\n1 2 4\n</pre> <p>Numpy also provides many functions to create arrays:</p> In\u00a0[46]: Copied! <pre>a = np.zeros((2,2))  # Create an array of all zeros\nprint(a)\n</pre> a = np.zeros((2,2))  # Create an array of all zeros print(a) <pre>[[0. 0.]\n [0. 0.]]\n</pre> In\u00a0[47]: Copied! <pre>b = np.ones((1,2))   # Create an array of all ones\nprint(b)\n</pre> b = np.ones((1,2))   # Create an array of all ones print(b) <pre>[[1. 1.]]\n</pre> In\u00a0[48]: Copied! <pre>c = np.full((2,2), 7) # Create a constant array\nprint(c)\n</pre> c = np.full((2,2), 7) # Create a constant array print(c) <pre>[[7 7]\n [7 7]]\n</pre> In\u00a0[49]: Copied! <pre>d = np.eye(2)        # Create a 2x2 identity matrix\nprint(d)\n</pre> d = np.eye(2)        # Create a 2x2 identity matrix print(d) <pre>[[1. 0.]\n [0. 1.]]\n</pre> In\u00a0[50]: Copied! <pre>e = np.random.random((2,2)) # Create an array filled with random values\nprint(e)\n</pre> e = np.random.random((2,2)) # Create an array filled with random values print(e) <pre>[[0.5293933  0.83232089]\n [0.54040558 0.42955453]]\n</pre> <p>Numpy offers several ways to index into arrays.</p> <p>Slicing: Similar to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array:</p> In\u00a0[51]: Copied! <pre>import numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\nprint(b)\n</pre> import numpy as np  # Create the following rank 2 array with shape (3, 4) # [[ 1  2  3  4] #  [ 5  6  7  8] #  [ 9 10 11 12]] a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])  # Use slicing to pull out the subarray consisting of the first 2 rows # and columns 1 and 2; b is the following array of shape (2, 2): # [[2 3] #  [6 7]] b = a[:2, 1:3] print(b) <pre>[[2 3]\n [6 7]]\n</pre> <p>A slice of an array is a view into the same data, so modifying it will modify the original array.</p> In\u00a0[52]: Copied! <pre>print(a[0, 1])\nb[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\nprint(a[0, 1])\n</pre> print(a[0, 1]) b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1] print(a[0, 1])  <pre>2\n77\n</pre> <p>You can also mix integer indexing with slice indexing. However, doing so will yield an array of lower rank than the original array. Note that this is quite different from the way that MATLAB handles array slicing:</p> In\u00a0[53]: Copied! <pre># Create the following rank 2 array with shape (3, 4)\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint(a)\n</pre> # Create the following rank 2 array with shape (3, 4) a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) print(a) <pre>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</pre> <p>Two ways of accessing the data in the middle row of the array. Mixing integer indexing with slices yields an array of lower rank, while using only slices yields an array of the same rank as the original array:</p> In\u00a0[54]: Copied! <pre>row_r1 = a[1, :]    # Rank 1 view of the second row of a  \nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nrow_r3 = a[[1], :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)\nprint(row_r2, row_r2.shape)\nprint(row_r3, row_r3.shape)\n</pre> row_r1 = a[1, :]    # Rank 1 view of the second row of a   row_r2 = a[1:2, :]  # Rank 2 view of the second row of a row_r3 = a[[1], :]  # Rank 2 view of the second row of a print(row_r1, row_r1.shape) print(row_r2, row_r2.shape) print(row_r3, row_r3.shape) <pre>[5 6 7 8] (4,)\n[[5 6 7 8]] (1, 4)\n[[5 6 7 8]] (1, 4)\n</pre> In\u00a0[55]: Copied! <pre># We can make the same distinction when accessing columns of an array:\ncol_r1 = a[:, 1]\ncol_r2 = a[:, 1:2]\nprint(col_r1, col_r1.shape)\nprint()\nprint(col_r2, col_r2.shape)\n</pre> # We can make the same distinction when accessing columns of an array: col_r1 = a[:, 1] col_r2 = a[:, 1:2] print(col_r1, col_r1.shape) print() print(col_r2, col_r2.shape) <pre>[ 2  6 10] (3,)\n\n[[ 2]\n [ 6]\n [10]] (3, 1)\n</pre> <p>Integer array indexing: When you index into numpy arrays using slicing, the resulting array view will always be a subarray of the original array. In contrast, integer array indexing allows you to construct arbitrary arrays using the data from another array. Here is an example:</p> In\u00a0[56]: Copied! <pre>a = np.array([[1,2], [3, 4], [5, 6]])\n\n# An example of integer array indexing.\n# The returned array will have shape (3,) and \nprint(a[[0, 1, 2], [0, 1, 0]])\n\n# The above example of integer array indexing is equivalent to this:\nprint(np.array([a[0, 0], a[1, 1], a[2, 0]]))\n</pre> a = np.array([[1,2], [3, 4], [5, 6]])  # An example of integer array indexing. # The returned array will have shape (3,) and  print(a[[0, 1, 2], [0, 1, 0]])  # The above example of integer array indexing is equivalent to this: print(np.array([a[0, 0], a[1, 1], a[2, 0]])) <pre>[1 4 5]\n[1 4 5]\n</pre> In\u00a0[57]: Copied! <pre># When using integer array indexing, you can reuse the same\n# element from the source array:\nprint(a[[0, 0], [1, 1]])\n\n# Equivalent to the previous integer array indexing example\nprint(np.array([a[0, 1], a[0, 1]]))\n</pre> # When using integer array indexing, you can reuse the same # element from the source array: print(a[[0, 0], [1, 1]])  # Equivalent to the previous integer array indexing example print(np.array([a[0, 1], a[0, 1]])) <pre>[2 2]\n[2 2]\n</pre> <p>One useful trick with integer array indexing is selecting or mutating one element from each row of a matrix:</p> In\u00a0[58]: Copied! <pre># Create a new array from which we will select elements\na = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nprint(a)\n</pre> # Create a new array from which we will select elements a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) print(a) <pre>[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n</pre> In\u00a0[59]: Copied! <pre># Create an array of indices\nb = np.array([0, 2, 0, 1])\n\n# Select one element from each row of a using the indices in b\nprint(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n</pre> # Create an array of indices b = np.array([0, 2, 0, 1])  # Select one element from each row of a using the indices in b print(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\" <pre>[ 1  6  7 11]\n</pre> In\u00a0[60]: Copied! <pre># Mutate one element from each row of a using the indices in b\na[np.arange(4), b] += 10\nprint(a)\n</pre> # Mutate one element from each row of a using the indices in b a[np.arange(4), b] += 10 print(a) <pre>[[11  2  3]\n [ 4  5 16]\n [17  8  9]\n [10 21 12]]\n</pre> <p>Boolean array indexing: Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:</p> In\u00a0[61]: Copied! <pre>import numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n</pre> import numpy as np  a = np.array([[1,2], [3, 4], [5, 6]])  bool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;                     # this returns a numpy array of Booleans of the same                     # shape as a, where each slot of bool_idx tells                     # whether that element of a is &gt; 2.  print(bool_idx) <pre>[[False False]\n [ True  True]\n [ True  True]]\n</pre> In\u00a0[62]: Copied! <pre># We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n</pre> # We use boolean array indexing to construct a rank 1 array # consisting of the elements of a corresponding to the True values # of bool_idx print(a[bool_idx])  # We can do all of the above in a single concise statement: print(a[a &gt; 2]) <pre>[3 4 5 6]\n[3 4 5 6]\n</pre> <p>For brevity we have left out a lot of details about numpy array indexing; if you want to know more you should read the documentation.</p> <p>Every numpy array is a grid of elements of the same type. Numpy provides a large set of numeric datatypes that you can use to construct arrays. Numpy tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explicitly specify the datatype. Here is an example:</p> In\u00a0[63]: Copied! <pre>x = np.array([1, 2])  # Let numpy choose the datatype\ny = np.array([1.0, 2.0])  # Let numpy choose the datatype\nz = np.array([1, 2], dtype=np.int64)  # Force a particular datatype\n\nprint(x.dtype, y.dtype, z.dtype)\n</pre> x = np.array([1, 2])  # Let numpy choose the datatype y = np.array([1.0, 2.0])  # Let numpy choose the datatype z = np.array([1, 2], dtype=np.int64)  # Force a particular datatype  print(x.dtype, y.dtype, z.dtype) <pre>int64 float64 int64\n</pre> <p>You can read all about numpy datatypes in the documentation.</p> <p>Basic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module:</p> In\u00a0[64]: Copied! <pre>x = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\n\n# Elementwise sum; both produce the array\nprint(x + y)\nprint(np.add(x, y))\n</pre> x = np.array([[1,2],[3,4]], dtype=np.float64) y = np.array([[5,6],[7,8]], dtype=np.float64)  # Elementwise sum; both produce the array print(x + y) print(np.add(x, y)) <pre>[[ 6.  8.]\n [10. 12.]]\n[[ 6.  8.]\n [10. 12.]]\n</pre> In\u00a0[65]: Copied! <pre># Elementwise difference; both produce the array\nprint(x - y)\nprint(np.subtract(x, y))\n</pre> # Elementwise difference; both produce the array print(x - y) print(np.subtract(x, y)) <pre>[[-4. -4.]\n [-4. -4.]]\n[[-4. -4.]\n [-4. -4.]]\n</pre> In\u00a0[66]: Copied! <pre># Elementwise product; both produce the array\nprint(x * y)\nprint(np.multiply(x, y))\n</pre> # Elementwise product; both produce the array print(x * y) print(np.multiply(x, y)) <pre>[[ 5. 12.]\n [21. 32.]]\n[[ 5. 12.]\n [21. 32.]]\n</pre> In\u00a0[67]: Copied! <pre># Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\nprint(np.divide(x, y))\n</pre> # Elementwise division; both produce the array # [[ 0.2         0.33333333] #  [ 0.42857143  0.5       ]] print(x / y) print(np.divide(x, y)) <pre>[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n</pre> In\u00a0[68]: Copied! <pre># Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n</pre> # Elementwise square root; produces the array # [[ 1.          1.41421356] #  [ 1.73205081  2.        ]] print(np.sqrt(x)) <pre>[[1.         1.41421356]\n [1.73205081 2.        ]]\n</pre> <p>Note that unlike MATLAB, <code>*</code> is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. dot is available both as a function in the numpy module and as an instance method of array objects:</p> In\u00a0[69]: Copied! <pre>x = np.array([[1,2],[3,4]])\ny = np.array([[5,6],[7,8]])\n\nv = np.array([9,10])\nw = np.array([11, 12])\n\n# Inner product of vectors; both produce 219\nprint(v.dot(w))\nprint(np.dot(v, w))\n</pre> x = np.array([[1,2],[3,4]]) y = np.array([[5,6],[7,8]])  v = np.array([9,10]) w = np.array([11, 12])  # Inner product of vectors; both produce 219 print(v.dot(w)) print(np.dot(v, w)) <pre>219\n219\n</pre> <p>You can also use the <code>@</code> operator which is equivalent to numpy's <code>dot</code> operator.</p> In\u00a0[70]: Copied! <pre>print(v @ w)\n</pre> print(v @ w) <pre>219\n</pre> In\u00a0[71]: Copied! <pre># Matrix / vector product; both produce the rank 1 array [29 67]\nprint(x.dot(v))\nprint(np.dot(x, v))\nprint(x @ v)\n</pre> # Matrix / vector product; both produce the rank 1 array [29 67] print(x.dot(v)) print(np.dot(x, v)) print(x @ v) <pre>[29 67]\n[29 67]\n[29 67]\n</pre> In\u00a0[72]: Copied! <pre># Matrix / matrix product; both produce the rank 2 array\n# [[19 22]\n#  [43 50]]\nprint(x.dot(y))\nprint(np.dot(x, y))\nprint(x @ y)\n</pre> # Matrix / matrix product; both produce the rank 2 array # [[19 22] #  [43 50]] print(x.dot(y)) print(np.dot(x, y)) print(x @ y) <pre>[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n</pre> <p>Numpy provides many useful functions for performing computations on arrays; one of the most useful is <code>sum</code>:</p> In\u00a0[73]: Copied! <pre>x = np.array([[1,2],[3,4]])\n\nprint(np.sum(x))  # Compute sum of all elements; prints \"10\"\nprint(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\"\n</pre> x = np.array([[1,2],[3,4]])  print(np.sum(x))  # Compute sum of all elements; prints \"10\" print(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\" print(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\" <pre>10\n[4 6]\n[3 7]\n</pre> <p>You can find the full list of mathematical functions provided by numpy in the documentation.</p> <p>Apart from computing mathematical functions using arrays, we frequently need to reshape or otherwise manipulate data in arrays. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of an array object:</p> In\u00a0[74]: Copied! <pre>print(x)\nprint(\"transpose\\n\", x.T)\n</pre> print(x) print(\"transpose\\n\", x.T) <pre>[[1 2]\n [3 4]]\ntranspose\n [[1 3]\n [2 4]]\n</pre> In\u00a0[75]: Copied! <pre>v = np.array([[1,2,3]])\nprint(v )\nprint(\"transpose\\n\", v.T)\n</pre> v = np.array([[1,2,3]]) print(v ) print(\"transpose\\n\", v.T) <pre>[[1 2 3]]\ntranspose\n [[1]\n [2]\n [3]]\n</pre> <p>Broadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array.</p> <p>For example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this:</p> In\u00a0[76]: Copied! <pre># We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = np.empty_like(x)   # Create an empty matrix with the same shape as x\n\n# Add the vector v to each row of the matrix x with an explicit loop\nfor i in range(4):\n    y[i, :] = x[i, :] + v\n\nprint(y)\n</pre> # We will add the vector v to each row of the matrix x, # storing the result in the matrix y x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = np.empty_like(x)   # Create an empty matrix with the same shape as x  # Add the vector v to each row of the matrix x with an explicit loop for i in range(4):     y[i, :] = x[i, :] + v  print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>This works; however when the matrix <code>x</code> is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix <code>x</code> is equivalent to forming a matrix <code>vv</code> by stacking multiple copies of <code>v</code> vertically, then performing elementwise summation of <code>x</code> and <code>vv</code>. We could implement this approach like this:</p> In\u00a0[77]: Copied! <pre>vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\nprint(vv)                # Prints \"[[1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]]\"\n</pre> vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other print(vv)                # Prints \"[[1 0 1]                          #          [1 0 1]                          #          [1 0 1]                          #          [1 0 1]]\" <pre>[[1 0 1]\n [1 0 1]\n [1 0 1]\n [1 0 1]]\n</pre> In\u00a0[78]: Copied! <pre>y = x + vv  # Add x and vv elementwise\nprint(y)\n</pre> y = x + vv  # Add x and vv elementwise print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>Numpy broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:</p> In\u00a0[79]: Copied! <pre>import numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  # Add v to each row of x using broadcasting\nprint(y)\n</pre> import numpy as np  # We will add the vector v to each row of the matrix x, # storing the result in the matrix y x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = x + v  # Add v to each row of x using broadcasting print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>The line <code>y = x + v</code> works even though <code>x</code> has shape <code>(4, 3)</code> and <code>v</code> has shape <code>(3,)</code> due to broadcasting; this line works as if v actually had shape <code>(4, 3)</code>, where each row was a copy of <code>v</code>, and the sum was performed elementwise.</p> <p>Broadcasting two arrays together follows these rules:</p> <ol> <li>If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.</li> <li>The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.</li> <li>The arrays can be broadcast together if they are compatible in all dimensions.</li> <li>After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.</li> <li>In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension</li> </ol> <p>If this explanation does not make sense, try reading the explanation from the documentation or this explanation.</p> <p>Functions that support broadcasting are known as universal functions. You can find the list of all universal functions in the documentation.</p> <p>Here are some applications of broadcasting:</p> In\u00a0[80]: Copied! <pre># Compute outer product of vectors\nv = np.array([1,2,3])  # v has shape (3,)\nw = np.array([4,5])    # w has shape (2,)\n# To compute an outer product, we first reshape v to be a column\n# vector of shape (3, 1); we can then broadcast it against w to yield\n# an output of shape (3, 2), which is the outer product of v and w:\n\nprint(np.reshape(v, (3, 1)) * w)\n</pre> # Compute outer product of vectors v = np.array([1,2,3])  # v has shape (3,) w = np.array([4,5])    # w has shape (2,) # To compute an outer product, we first reshape v to be a column # vector of shape (3, 1); we can then broadcast it against w to yield # an output of shape (3, 2), which is the outer product of v and w:  print(np.reshape(v, (3, 1)) * w) <pre>[[ 4  5]\n [ 8 10]\n [12 15]]\n</pre> In\u00a0[81]: Copied! <pre># Add a vector to each row of a matrix\nx = np.array([[1,2,3], [4,5,6]])\n# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n# giving the following matrix:\n\nprint(x + v)\n</pre> # Add a vector to each row of a matrix x = np.array([[1,2,3], [4,5,6]]) # x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3), # giving the following matrix:  print(x + v) <pre>[[2 4 6]\n [5 7 9]]\n</pre> In\u00a0[82]: Copied! <pre># Add a vector to each column of a matrix\n# x has shape (2, 3) and w has shape (2,).\n# If we transpose x then it has shape (3, 2) and can be broadcast\n# against w to yield a result of shape (3, 2); transposing this result\n# yields the final result of shape (2, 3) which is the matrix x with\n# the vector w added to each column. Gives the following matrix:\n\nprint((x.T + w).T)\n</pre> # Add a vector to each column of a matrix # x has shape (2, 3) and w has shape (2,). # If we transpose x then it has shape (3, 2) and can be broadcast # against w to yield a result of shape (3, 2); transposing this result # yields the final result of shape (2, 3) which is the matrix x with # the vector w added to each column. Gives the following matrix:  print((x.T + w).T) <pre>[[ 5  6  7]\n [ 9 10 11]]\n</pre> In\u00a0[83]: Copied! <pre># Another solution is to reshape w to be a row vector of shape (2, 1);\n# we can then broadcast it directly against x to produce the same\n# output.\nprint(x + np.reshape(w, (2, 1)))\n</pre> # Another solution is to reshape w to be a row vector of shape (2, 1); # we can then broadcast it directly against x to produce the same # output. print(x + np.reshape(w, (2, 1))) <pre>[[ 5  6  7]\n [ 9 10 11]]\n</pre> In\u00a0[84]: Copied! <pre># Multiply a matrix by a constant:\n# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n# these can be broadcast together to shape (2, 3), producing the\n# following array:\nprint(x * 2)\n</pre> # Multiply a matrix by a constant: # x has shape (2, 3). Numpy treats scalars as arrays of shape (); # these can be broadcast together to shape (2, 3), producing the # following array: print(x * 2) <pre>[[ 2  4  6]\n [ 8 10 12]]\n</pre> <p>Broadcasting typically makes your code more concise and faster, so you should strive to use it where possible.</p> <p>This brief overview has touched on many of the important things that you need to know about numpy, but is far from complete. Check out the numpy reference to find out much more about numpy.</p> <p>Matplotlib is a plotting library. In this section give a brief introduction to the <code>matplotlib.pyplot</code> module, which provides a plotting system similar to that of MATLAB.</p> In\u00a0[85]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt <p>By running this special iPython command, we will be displaying plots inline:</p> In\u00a0[86]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>The most important function in <code>matplotlib</code> is plot, which allows you to plot 2D data. Here is a simple example:</p> In\u00a0[87]: Copied! <pre># Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\n</pre> # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x)  # Plot the points using matplotlib plt.plot(x, y) Out[87]: <pre>[&lt;matplotlib.lines.Line2D at 0x121741990&gt;]</pre> <p>With just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels:</p> In\u00a0[89]: Copied! <pre>y_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\n</pre> y_sin = np.sin(x) y_cos = np.cos(x)  # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel('x axis label') plt.ylabel('y axis label') plt.title('Sine and Cosine') plt.legend(['Sine', 'Cosine']) Out[89]: <pre>&lt;matplotlib.legend.Legend at 0x121939950&gt;</pre> <p>You can plot different things in the same figure using the subplot function. Here is an example:</p> In\u00a0[90]: Copied! <pre># Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n</pre> # Compute the x and y coordinates for points on sine and cosine curves x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x)  # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1)  # Make the first plot plt.plot(x, y_sin) plt.title('Sine')  # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title('Cosine')  # Show the figure. plt.show() <p>You can read much more about the <code>subplot</code> function in the documentation.</p>"},{"location":"jupyter-notebook-tutorial/#cs231n-python-tutorial-with-jupyter-notebook","title":"CS231n Python Tutorial With Jupyter Notebook\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#what-is-a-jupyter-notebook","title":"What is a Jupyter Notebook?\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#keyboard-shortcuts","title":"Keyboard Shortcuts\u00b6","text":"<p>There are a few keyboard shortcuts you should be aware of to make your notebook experience more pleasant. To escape editing of a cell, press <code>esc</code>. Escaping a <code>Markdown</code> cell won't render it, so make sure to execute it if you wish to render the markdown. Notice how the highlight color switches back to blue when you have escaped a cell.</p> <p>You can navigate between cells by pressing your arrow keys. Executing a cell automatically shifts the cell cursor down 1 cell if one exists, or creates a new cell below the current one if none exist.</p> <ul> <li>To place a cell below the current one, press <code>b</code>.</li> <li>To place a cell above the current one, press <code>a</code>.</li> <li>To delete a cell, press <code>dd</code>.</li> <li>To convert a cell to <code>Markdown</code> press <code>m</code>. Note you have to be in <code>esc</code> mode.</li> <li>To convert it back to <code>Code</code> press <code>y</code>. Note you have to be in <code>esc</code> mode.</li> </ul> <p>Get familiar with these keyboard shortcuts, they really help!</p>"},{"location":"jupyter-notebook-tutorial/#python-tutorial","title":"Python Tutorial\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#a-brief-note-on-python-versions","title":"A Brief Note on Python Versions\u00b6","text":"<p>As of Janurary 1, 2020, Python has officially dropped support for <code>python2</code>. We'll be using Python 3.7 for this iteration of the course.</p> <p>You should have activated your <code>cs231n</code> virtual environment created in the Setup Instructions before calling <code>jupyter notebook</code>. If that is the case, the cell below should print out a major version of 3.7.</p>"},{"location":"jupyter-notebook-tutorial/#basics-of-python","title":"Basics of Python\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#basic-data-types","title":"Basic data types\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#numbers","title":"Numbers\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#booleans","title":"Booleans\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#strings","title":"Strings\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#containers","title":"Containers\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#lists","title":"Lists\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#slicing","title":"Slicing\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#loops","title":"Loops\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#list-comprehensions","title":"List comprehensions\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#dictionaries","title":"Dictionaries\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#sets","title":"Sets\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#tuples","title":"Tuples\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#functions","title":"Functions\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#classes","title":"Classes\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#numpy","title":"Numpy\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#arrays","title":"Arrays\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#array-indexing","title":"Array indexing\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#datatypes","title":"Datatypes\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#array-math","title":"Array math\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#matplotlib","title":"Matplotlib\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#plotting","title":"Plotting\u00b6","text":""},{"location":"jupyter-notebook-tutorial/#subplots","title":"Subplots\u00b6","text":""},{"location":"nerf/","title":"Tl;dr What is NeRF and what does it do","text":"<p>NeRF stands for Neural Radiance Fields. It solves for view interpolation, which is taking a set of input views (in this case a sparse set) and synthesizing novel views of the same scene. Current RGB volume rendering models are great for optimization, but require extensive storage space (1-10GB). One side benefit of NeRF is the weights generated from the neural network are \\(\\sim\\)6000 less in size than the original images.</p>"},{"location":"nerf/#helpful-terminology","title":"Helpful Terminology","text":"<p>Rasterization: Computer graphics use this technique to display a 3D object on a 2D screen. Objects on the screen are created from virtual triangles/polygons to create 3D models of the objects. Computers convert these triangles into pixels, which are assigned a color. Overall, this is a computationally intensive process.\\ Ray Tracing: In the real world, the 3D objects we see are illuminated by light. Light may be blocked, reflected, or refracted. Ray tracing captures those effects. It is also computationally intensive, but creates more realistic effects. Ray: A ray is a line connected from the camera center, determined by camera position parameters, in a particular direction determined by the camera angle.\\ NeRF uses ray tracing rather than rasterization for its models.\\ Neural Rendering As of 2020/2021, this terminology is used when a neural network is a black box that models the geometry of the world and a graphics engine renders it. Other terms commonly used are scene representations, and less frequently, implicit representations. In this case, the neural network is just a flexible function approximator and the rendering machine does not learn at all.</p>"},{"location":"nerf/#approach","title":"Approach","text":"<p>A continuous scene is represented as a 3D location x = (x, y, z) and 2D viewing direction \\((\\theta,\\phi)\\) whose output is an emitted color c = (r, g, b) and volume density \\(\\sigma\\). The density at each point acts like a differential opacity controlling how much radiance is accumulated in a ray passing through point x. In other words, an opaque surface will have a density of \\(\\infty\\) while a transparent surface would have \\(\\sigma = 0\\). In layman terms, the neural network is a black box that will repeatedly ask what is the color and what is the density at this point, and it will provide responses such as \u201cred, dense.\u201d\\ This neural network is wrapped into volumetric ray tracing where you start with the back of the ray (furthest from you) and walk closer to you, querying the color and density. The equation for expected color \\(C(r)\\) of a camera ray \\(r(t) = o + td\\) with near and far bounds \\(t_n\\) and \\(t_f\\) is calculated using the following:</p> <p>\\(C(r) = \\[ \\int_{t_n}^{t_f} T(t)\\sigma(r(t))c(r(t),d) \\,dt \\] where     \\[T(t) = exp(-\\int_{t_n}^{t}\\sigma(r(s))\\, ds)\\]\\)</p> <p>\\ To actually calculate this, the authors used a stratified sampling approach where they partition \\([t_n, t_f]\\) into N evenly spaced bins and then drew one sample uniformly from each bin:</p> \\[\\hat{C}(r) = \\sum_{i = 1}^{N}T_{i}(1-exp(-\\sigma_{i}\\delta_{i}))c_{i}, where T_{i} = exp(-\\sum_{j=1}^{i-1}\\sigma_{j}\\delta_{j})\\] <p>Where \\(\\delta_{i} = t_{i+1} - t_{i}\\) is the distance between adjacent samples. The volume rendering is differentiable. You can then train the model by minimizing rendering loss.</p> \\[min_{\\theta}\\sum_{i}\\left\\| render_{i}(F_{\\Theta}-I_{i}\\right\\|^{2}\\] <p> [fig:Figure 1]</p> <p>\\ In practice, the Cartesian coordinates are expressed as vector d. You can approximate this representation through MLP with \\(F_\\Theta = (x, d) \\rightarrow (c, \\sigma)\\).\\ Why does NeRF use MLP rather than CNN? Multilayer perceptron (MLP) is a feed forward neural network. The model doesn\u2019t need to conserve every feature, therefore a CNN is not necessary.\\</p>"},{"location":"nerf/#common-issues-and-mitigation","title":"Common issues and mitigation","text":"<p>The naive implementation of a neural radiance field creates blurry results. To fix this, the 5D coordinates are transformed into positional encoding (terminology borrowed from transformer literature). \\(F_\\Theta\\) is a composition of two formulas: \\(F_\\Theta = F'_\\Theta \\cdot \\gamma\\) which significantly improves performance.</p> \\[\\gamma(p) = (sin(2^{0}\\pi p), cos(2^{0}\\pi p),...,sin(2^{L-1}\\pi p), cos(2^{L-1} \\pi p)\\] <p>L determines how many levels there are in the positional encoding and it is used for regularizing NeRF (low L = smooth). This is also known as a Fourier feature, and it turns your MLP into an interpolation tool. Another way of looking at this is your Fourier feature based neural network is just a tiny look up table with extremely high resolution. Here is an example of applying Fourier feature to your code:\\</p> <pre><code>B = SCALE * np.random.normal(shape = (input_dims, NUM_FEATURES))\nx = np.concatenate([np.sin(x @ B), np.cos(x @ B)], axis = -1)\nx = nn.Dense(x, features = 256)\n</code></pre> <p> [fig:Figure 2]</p> <p>NeRF also uses hierarchical volume sampling: coarse sampling and the fine network. This allows NeRF to more efficiently run their model and deprioritize areas of the camera ray where there is free space and occlusion. The coarse network uses \\(N_{c}\\) sample points to evaluate the expected color of the ray with the stratified sampling. Based on these results, they bias the samples towards more relevant parts of the volume.</p> \\[\\hat{C}_c(r) = \\sum_{i=1}^{N_{c}}w_{i}c_{i}, w_{i}=T_{i}(1-exp(-\\sigma_{i}\\delta_{i}))\\] <p>A second set of \\(N_{f}\\) locations are sampled from this distribution using inverse transform sampling. This method allocates more samples to regions where we expect visual content.</p>"},{"location":"nerf/#results","title":"Results","text":"<p>The paper goes in depth on quantitative measures of the results, which NeRF outperforms existing models. A visual assessment is shared below:</p> <p> [fig:Figure 3]</p>"},{"location":"nerf/#additional-references","title":"Additional references","text":"<p>What\u2019s the difference between ray tracing and rasterization? Self explanatory title, excellent write-up helping reader differentiate between two concepts.\\ Matthew Tancik NeRF ECCV 2020 Oral Videos showcasing NeRF produced images.\\ NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis Simple and alternative explanation for NeRF.\\ NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis arxiv paper\\ CS 231n Spring 2021 Jon Barron Guest Lecture</p>"},{"location":"overview/","title":"Overview of Computer Vision and Visual Recognition","text":"<p>Our introductory lecture covered the history of Computer Vision and many of its current applications, to help you understand the context in which this class is offered. See the slides for details.</p>"},{"location":"pixelrnn/","title":"PixelRNN","text":"<p>We now give a brief overview of PixelRNN. PixelRNNs belongs to a family of explicit density models called fully visible belief networks (FVBN). We can represent our model with the following equation: \\(\\(p(x) = p(x_1, x_2, \\dots, x_n),\\)\\) where the left hand side \\(p(x)\\) represents the likelihood of an entire image \\(x\\), and the right hand side represents the joint likelihood of each pixel in the image. Using the Chain Rule, we can decompose this likelihood into a product of 1-dimensional distributions: \\(\\(p(x) = \\prod_{i = 1}^n p(x_i \\mid x_1, \\dots, x_{i - 1}).\\)\\) Maximizing the likelihood of training data, we obtain our models PixelRNN.</p>"},{"location":"pixelrnn/#introduction","title":"Introduction","text":"<p>PixelRNN, first introduced in van der Oord et al. 2016, uses an RNN-like structure, modeling the pixels one-by-one, to maximize the likelihood function given above. One of the more difficult tasks in generative modeling is to create a model that is tractable, and PixelRNN seeks to address that. It does so by tractably modeling a joint distribution of the pixels in the image, casting it as a product of conditional distributions. The factorization turns the joint modeling problem into one that relates to sequences, i.e., we have to predict the next pixel given all the previously generated pixels. Thus, we use Recurrent Neural Networks for this tasks as they learn sequentially. Those same principles apply here; more precisely, we generate image pixels starting from the top left corner, and we model each pixel\u2019s dependency on previous pixels using an RNN (LSTM).</p> <p>Specifically, the PixelRNN framework is made up of twelve two-dimensional LSTM layers, with convolutions applied to each dimension of the data. There are two types of layers here. One is the Row LSTM layer where the convolution is applied along each row. The second type is the Diagonal BiLSTM layer where the convolution is applied to the diagonals of the image. In addition, the pixel values are modeled as discrete values using a multinomial distribution implemented with a softmax layer. This is in contrast to many previous approaches, which model pixels as continuous values.</p>"},{"location":"pixelrnn/#model","title":"Model","text":"<p>The approach of the PixelRNN is as follows. The RNN scans the each individual pixel, going row-wise, predicting the conditional distribution over the possible pixel values given what context the network has. As mentioned before, PixelRNN uses a two-dimensional LSTM network which begins scanning at the top left of the image and makesits way to the bottom right. One of the reasons an LSTM is used is that it can better capture some longer range dependencies between pixels - this is essential for understanding image composition. The reason a two-dimensional structure is used is to ensure that the signals propagate in the left-to-right and top-to-bottom directions well.\\</p> <p>The input image to the network is represented by a 1D vector of pixel values \\(\\{x_1,..., x_{n^2}\\}\\) for an \\(n\\)-by-\\(n\\) sized image, where \\(\\{x_1,..., x_{n}\\}\\) represents the pixels from the first row. Our goal is to use these pixel values to find a probability distribution \\(p(X)\\) for each image \\(X\\). We define this probability as: \\(\\(p(x) = \\prod_{i = 1}^{n^2} p(x_i \\mid x_1, \\dots, x_{i - 1}).\\)\\)</p> <p>This is the product of the conditional distributions across all the pixels in the image - for pixel \\(x_i\\), we have \\(p(x_i \\mid x_1, \\dots, x_{i - 1})\\). In turn, each of these conditional distributions is determined by three values, associated with each of the color channels present in the image (red, green and blue). In other words:</p> \\[p(x_i \\mid x_1, \\dots, x_{i - 1}) =  p(x_{i,R} \\mid \\textbf{x}_{&lt;i}) \\cdot p(x_{i,G} \\mid \\textbf{x}_{&lt;i}, x_{i,R}) \\cdot p(x_{i,B} \\mid \\textbf{x}_{&lt;i}, x_{i,R}, x_{i,G}).\\] <p>In the next section we will see how these distributions are calculated and used within the Recurrent Neural Network framework proposed in PixelRNN.</p>"},{"location":"pixelrnn/#architecture","title":"Architecture","text":"<p>As we have seen, there are two distinct components to the \u201ctwo-dimensional\u201d LSTM, the Row LSTM and the Diagonal BiLSTM. Figure 2 illustrates how each of these two LSTMs operates, when applied to an RGB image.</p> <p> </p> <p>Row LSTM is a unidirectional layer that processes the image row by row from top to bottom computing features for a whole row at once using a 1D convolution. As we can see in the image above, the Row LSTM captures a triangle-shaped context for a given pixel. An LSTM layer has an input-to-state component and a recurrent state-to-state component that together determine the four gates inside the LSTM core. In the Row LSTM, the input-to-state component is computed for the whole two-dimensional input map with a one-dimensional convolution, row-wise. The output of the convolution is a 4h \u00d7 n \u00d7 n tensor, where the first dimension represents the four gate vectors for each position in the input map (h here is the number of output feature maps). Below are the computations for this state-to-state component, using the previous hidden state (\\(h_{i-1}\\)) and previous cell state (\\(c_{i-1}\\)). \\(\\([o_i, f_i, i_i, g_i]  = \\sigma(\\textbf{K}^{ss} \\circledast h_{i-1} + \\textbf{K}^{is} \\circledast  \\textbf{x}_{i})\\)\\) \\(\\(c_i  = f_i \\odot c_{i-1} + i_i \\odot g_i\\)\\) \\(\\(h_i  = o_i \\odot \\tanh(c_{i})\\)\\)</p> <p>Here, \\(\\textbf{x}_i\\) is the row of the input representation and \\(\\textbf{K}^{ss}\\), \\(\\textbf{K}^{is}\\) are the kernel weights for state-to-state and input-to-state respectively. \\(g_i, o_i, f_i\\) and \\(i_i\\) are the content, output, forget and input gates. \\(\\sigma\\) represents the activation function (tanh activation for the content gate, and sigmoid for the rest of the gates).\\</p> <p>Diagonal BiLSTM The Diagonal BiLSTM is able to capture the entire image context by scanning along both diagonals of the image, for each direction of the LSTM. We first compute the input-to-state and state-to-state components of the layer. For each of the directions, the input-to-state component is simply a 1\u00d71 convolution \\(K^{is}\\), generating a \\(4h \u00d7 n \u00d7 n\\) tensor (Here again the dimension represents the four gate vectors for each position in the input map where h is the number of output feature maps). The state-to-state is calculated using the \\(K^{ss}\\) that has a kernel of size 2 \u00d7 1. This step takes the previous hidden and cell states, combines the contribution of the input-to-state component and produces the next hidden and cell states, as explained in the equations for Row LSTM above. We repeat this process for each of the two directions.</p>"},{"location":"pixelrnn/#performance","title":"Performance","text":"<p>When originally presented, the PixelRNN model\u2019s performance was tested on some of the most prominent datasets in the computer vision space - ImageNet and CIFAR-10. The results in some cases were state-of-the-art. On the ImageNet data set, achieved an NLL score of 3.86 and 3.63 on the the 32x32 and 64x64 image sizes respectively. On CiFAR-10, it achievied a NLL score of 3.00, which was state-of-the-art at the time of publication.</p>"},{"location":"pixelrnn/#references","title":"References","text":"<p>1) CS231n Lecture 11 'Generative Modeling' 2) Pixel Recurrent Neural Networks (Oord et. al.) 2016</p>"},{"location":"poster-2018/","title":"CS231n Poster Session 2018","text":"<ul> <li>Location: Jen-Hsun Huang Engineering Center</li> <li>Parking: Parking information can be found here</li> <li>Sponsor Setup Time: 11:15am - 12:00pm</li> <li>Poster Session: 12:00pm - 3:15pm</li> <li>Award Ceremony: 3:15pm - 3:30pm</li> </ul> <p>The 2018 Stanford CS231N poster session will showcase projects in Convolutional Neural Networks for Visual Recognition that students have worked on over the past quarter. This year, 650 students will be presenting over 300 projects.  The topics range from Generative Adversarial Networks (GANs), healthcare and medical imaging, art and style transfer, satellite imaging, self-driving cars, video understanding and more! See the list below for the projects that will be presented.</p> <p>Catered food and refreshments will be made available over the course of the event.</p> <p>This poster session has been made possible thanks to generous donations from Waymo, Bloomberg, Zoox, and NVIDIA!</p>"},{"location":"poster-2018/#list-of-projects-3-digit-number-is-id-to-locate-the-poster","title":"List of Projects (3 digit number is ID to locate the poster)","text":"<ul> <li>101: Enhance! Image Super-Resolution</li> <li>102: Super Resolution using CNNs and GANS</li> <li>103: Modern Approaches for Real-Time Neural Style Transfer</li> <li>104: A convolutional classification approach to colorization</li> <li>105: Face swapping and harmonization using neural nets</li> <li>106: Defendr</li> <li>107: Automatic Graphic Generation from Text using GANs for Artists and Engineers</li> <li>108: Generating Novel Images of Landmarks with WGANs</li> <li>109: Generating Artwork with GANs</li> <li>110: Audio2GIF</li> <li>111: End-to-End Super Resolution Object Detection Networks</li> <li>112: Conditional Face Generation with Deep Convolutional Generative Adversarial Networks</li> <li>113: Learning a Meta-Discriminator for GANs</li> <li>115: Exploring Adversarial Input Spaces for Convolutional Neural Network Defense</li> <li>118: Contour-Aware Image-to-Image Translation</li> <li>119: Human Portrait Colorization</li> <li>120: Adversarial Image Segmentation </li> <li>121: Artwork Classification and Style Transfer</li> <li>122: Automatic Colorization through Transfer Learning</li> <li>123: CNN models for classifying emotions in art images</li> <li>124: Deep Learning for Image Completion</li> <li>125: Computer Generated Art </li> <li>127: Filter discriminators: adversarial loss for convolutional kernels</li> <li>128: Anime Character Generation with GAN</li> <li>129: Modifying the AttnGAN Discriminator for Improved Feature-Specific Text-to-Image Synthesis</li> <li>130: Data Genie </li> <li>131: Fully Unsupervised Aerial-to-Map Translation with Cycle-GAN</li> <li>132: NeuralHash: An Adversarial Steganographic Method For Robust, Imperceptible Watermarking</li> <li>133: Mapping Decision Boundaries of Convolutional Neural Network Classifiers</li> <li>134: Distributionally Robust Adversarial Training</li> <li>135: Generating realistic morphologies of neurons in rodent hippocampus with DGCAN</li> <li>136: Text2Mesh using StackGAN</li> <li>137: Detection and removal of biases using adversarial architectures</li> <li>138: Cloud Removal in Hyperspectral Satellite Images using Generative Adversarial Networks</li> <li>140: In your face! GAN Face Generation</li> <li>141: Anonymity as a Service: Addressing Perceptual Biases by Anonymizing Facial Features</li> <li>144: Global an Local Neural Style Transfer</li> <li>146: End-to-End Neural Color Transfer of Subjects in Portrait Photography</li> <li>147: Text to Artistic Image Generation with GANs</li> <li>148: Be Creative: Style Transfer Mix &amp; Match</li> <li>149: Determine the Origin of Historical Artifacts</li> <li>150: Style Transfer Incorporating Shape Deformability for Animation Genre Replication</li> <li>200: Dog Breed Identification</li> <li>203: Convolutional Neural Networks for Landmark Recognition</li> <li>204: World Landmark Recognition</li> <li>205: Distracted Driver Detection</li> <li>206: Deep Shopping: Object Detection Based Fashion Recommendation</li> <li>208: Capsule Networks on Modified Real-World Datasets</li> <li>209: Aircraft Type Recognition with Convolutional Neural Networks</li> <li>210: FloraDex: A Fine-Grained Flower Recognizer</li> <li>212: Handing Noisy Bounding-Box Annotations with Faster R-CNN</li> <li>213: A Convolutional Recurrent Attention Model for Fine-Grained Classification</li> <li>215: Probabilistic Convolutional Neural Network for Probabilistic Inference</li> <li>217: Optical Music Recognition based on Convolutional Neural Networks</li> <li>218: Fine Grain Image Classification on Dog Breeds</li> <li>219: TINE-CNN Augmentation: Automatic data augmentation for any image classification task</li> <li>220: Fashion Product Recognition in Fine-Grained Visual Categorization</li> <li>222: One-Shot Learning of Cosmetic Objects</li> <li>223: Exploring Movie Poster Classification and Generation with Deep Convolutional Neural Networks</li> <li>224: Classifying Electromagnetic Showers from Calorimeter Images with CNNs</li> <li>228: Deep Stereo Fusion: Deep Learning for Disparity Map Estimation and Image Fusion with Dual Camera Phone Imagery</li> <li>229: Automobile-Based Dual-Camera Object Segmentation</li> <li>230: Fine-grained Classification of Furniture and Home Goods Images</li> <li>231: Fine-grained Image Classification with Batch Contrastive Loss</li> <li>232: Don't Judge a Movie by its Poster</li> <li>233: Product multi-class classification using pretrained CNN models</li> <li>234: SIFT++</li> <li>237: Spatio-temporal large traffic network speed prediction using CNN</li> <li>238: Shazam for Fashion</li> <li>239: Composite Architecture for High Performance Image Classification</li> <li>240: Traffic monitoring from seismic data</li> <li>241: Image-based Merged Di-photon Identification for the ATLAS Experiment at the Large Hadron Collider</li> <li>242: DeepClean:  Deep Bayesian Restoration of Interferometric Images</li> <li>243: 3D GAN Object Generation and Reconstruction</li> <li>244: Image Segmentation for Autonomous Cars</li> <li>245: Multi-Task Facial Landmark Detection with CNN</li> <li>247: 3DNetView: Training Neural Networks to Perform 3D Image Reconstruction</li> <li>248: Architecture for Automatic Fashion Product Labeling</li> <li>249: Understanding Deforestation in the Amazon Basin with Neural Networks</li> <li>250: Gotta Train 'Em All: Classifying Pok\u00e9mon using Deep Learning</li> <li>252: Movie Recommendation System Enhanced by Image Data and ConvNet</li> <li>253: Biomedical Image Segmentation for Nuclei Detection</li> <li>254: Self Attention Generative Adversarial Networks for High-Dimensional Scene Representations from Single 2D Images</li> <li>255: Learning to Detect Light Field Features</li> <li>300: Neural Techniques for Pose Guided Image Generation</li> <li>301: Conversational Group Detection With Deep Convolutional Networks</li> <li>302: Apparent Age Estimation From Facial Images</li> <li>303: American Sign Language Gesture Detection</li> <li>306: Human Body Pose Estimation</li> <li>307: Learning to Feel: Training a CNN to recognize emotion</li> <li>308: Monocular 3D Human Bounding Box Estimation</li> <li>309: Decrypting human emotions</li> <li>310: Semantic Segmentation(will update correct title)</li> <li>311: Predicting Human Emotions from Images and Captions</li> <li>312: Transformation Generalizability of Novel Objects in Human and Computational Vision</li> <li>314: Classification of Dance Styles Using CNNs</li> <li>316: Detecting Assaults in Surveillance Videos</li> <li>317: Video Hand Gesture Recognition</li> <li>319: Improving Affectnet: Emotion Classification</li> <li>320: Video Gesture Classification Using Combined RGB and Depth Features</li> <li>321: Predicting Hand Pose and Gesture from Monocular RGB Images</li> <li>402: Medical Image Super-Resolution using GANs</li> <li>403: 3D Reconstruction and Alignment of MRIs for Improved Medical Diagnostics</li> <li>405: Learning electrode probing strategy in retinal prosthesis systems</li> <li>408: PineappLeNet: PineappLeNet: Synthesizing Dynamic Contrast-Enhanced (DCE) Magnetic Resonance Data</li> <li>409: Classifying Dementia Ratings from MRI Imaging Data</li> <li>413: Protein Functional Site Detection Using Amino Acid Contact Maps</li> <li>415: Using Deep Learning for UIP Classification and Cyst Volume Calculation</li> <li>417: BoneNet: Convolutional Methods for Abnormality Detection in the Lower Extremities </li> <li>418: Does it look good? Evaluating GANs for Medical Imaging Applications</li> <li>419: Detecting Unburnt Skin Using Deep Learning</li> <li>423: Heirarchical Semantic Segmentation of Brain Tumors in MRIs using Convolutional Neural Networks</li> <li>424: Cross-Institute Histopathology Image Stain Normalization with Deep Convolutional Neural Networks</li> <li>426: Automated Burn Prognosis of Percent Total Body Surface Area</li> <li>429: Neural Stain Normalization and Unsupervised Classification of Cell Nuclei in Histopathological Breast Cancer Images</li> <li>430: Segmentation of Stroke Lesion in T1-Weighted MRIs</li> <li>431: Applying Deep Learning to Chest X-Rays for Pneumonia Detection</li> <li>432: 3D Brain Tumor Segmentation</li> <li>434: Thoracic Imaging Temporal Interpolation</li> <li>437: Automated Iris Detection</li> <li>438: Using Transfer Learning to classify Brain Tumors from Pathology Images</li> <li>440: Preprocessing Histopathology Stains with Deep Learning</li> <li>442: U-Net Architecture for Segmenting Nuclei in Medical Images</li> <li>444: Diagnosing Retinal Pathology from Optical Coherence Tomography Using VGG19 and InceptionV3</li> <li>445: Peak finding for crystallography</li> <li>446: Reconstruction of multi-shot diffusion-weighted MRI using deep learning</li> <li>450: Super-resolution MRIs with Semi-Supervised GANs</li> <li>451: Classification and Segmentation of Brain Lesions after Stroke</li> <li>452: Neural Networks for the Identification of Viable Eggs in In-vitro Fertilization</li> <li>471: Dense Convolutional Networks for Abnormality Classification in Mammograms</li> <li>455: 3D BrainNet: Brain Tumor Segmentation with Convolutional Neural Networks and Fully Connected CRFs</li> <li>456: Lung Nodule Candidate Generation and Cancer Prediction</li> <li>457: Anonymizing MRI Images via U-Net Image Segmentation</li> <li>458: Tackling Diabetic Retinopathy with Visual Recognition</li> <li>460: Unpaired Magnetic Resonance Image-to-Image Translation for Prostate using Cycle-Consistent Adversarial Networks</li> <li>461: Rethinking Radiology: An Analysis of Different Approaches to BraTS</li> <li>462: Classification Techniques for White Blood Cell Types</li> <li>470: Segmentation of Stroke Lesions in T1-Weighted Brain MRIs using Deep Learning</li> <li>500: D4QN: Distributed Deep Reinforcement Learning in the Arcade Learning Environment</li> <li>501: Single Shot Robotic Grasp Pose Detection</li> <li>502: Occupancy Grid Mapping for Robust and Efficient Learning</li> <li>504: Robotic Grasping Planning using Convolutional Neural Networks</li> <li>508: RevNet: An End-to-End Model for training self-driving simulated vehicles </li> <li>509: LiDAR &amp; RGB Fusion for 3D Bounding Box Estimation</li> <li>510: Autonomous Driving Video Segmentation with Deep Learning</li> <li>511: Recent Tesla Model X Autopilot Accident Analysis and Possible Solution via  Traffic Sign  and Lan Detection</li> <li>512: Smaller is Better: Image Compression using Deep Learning</li> <li>513: Multi-Task Learning in Visual Attribute Recognition</li> <li>514: A look at the topology of convolutional neural networks</li> <li>517: Deep Learning on LIDAR  Point Cloud for 3D Object Detection</li> <li>518: Semantic Segmentation in the Traffic Environment</li> <li>519: Suction Affordance Maps for Grasping of Diverse Objects</li> <li>521: Comparing Deep Neuroevolution to RL algorithms on Atari and Mujoco Environments</li> <li>524: Using Human Gameplay to Augment Deep Q-Networks for Crypt of the NecroDancer</li> <li>525: Comparing Learning Algorithms with Pac-Man </li> <li>526: Twist to Grasp: Rotational Regional Proposals for Grasp Detection</li> <li>529: Learning to Convolve</li> <li>530: Adversarial Examples for YOLO Object Detection</li> <li>531: High Fidelity Image Compression Using CNNs</li> <li>533: Video Object Segmentation for Autonomous Vehicles</li> <li>534: Object Detection in Autonomous Driving</li> <li>535: Improving 3D Data Resolution: Using CNNs to Upscale Resolution of LIDAR Data</li> <li>536: Semantic Segmentation on Autonomous Vehicle Data </li> <li>537: Traffic Sign Detection and Classification using Capsule Network</li> <li>539: Using t-SNE to Visualize Network Dynamics</li> <li>540: Quantization Methodology for Efficient CNN Inference</li> <li>600: Identifying Modes of Fishing from Ship Images</li> <li>601: Enhancing Climate Data Resolution using Residual Networks</li> <li>602: Detection of Cryogenic tanks</li> <li>604: Tents Density Mapping of Refugee Camp via High-Resolution Remote Sensing Imagery</li> <li>607: Convolutional Neural Networks for Radargram Segmentation</li> <li>609: Power Plant Type Classification and Numerical Prediction Using Satellite Imagery Data</li> <li>612: Semi-Supervised Image Segmentation for Satellite Imagery</li> <li>700: DeepGIFs: Using Deep Learning to Understand and Synthesize Motion</li> <li>701: Conventional Video Object Segmentation using Mask R-CNN and Online Learning</li> <li>702: Moments in Time Challenge: Spatiotemporal information extraction from sequential video inputs using joint CNN-LSTM framework</li> <li>703: Automated Visual Weak Supervision for Object Recognition in Videos</li> <li>704: Using CNNs to Identify Player Actions in Basketball Videos</li> <li>705: Video Compression with 3D CNNs</li> <li>706: Action Recognition in the Moments In Time Dataset</li> <li>707: Convolutional Neural Networks for MLB Pitch Recognition</li> <li>711: Video Classification on the YouTube-8M Dataset</li> <li>712: A Multi-Faceted Approach to Video Action Classification</li> <li>713: Basketball Shot Quality Assessment with Deep Learning</li> <li>714: Moments in Time: Deep Action Recognition</li> <li>800: Spotting and Transcribing Structured Nutrition Information from Product Images</li> <li>802: A Novel Approach using Weak Supervision to Label Digital Screenshots\\ for Behavioral Analysis with Transfer Learning</li> <li>803: End-To-End Trainable Model for Text Recognition</li> <li>804: Converting Handwriting to Latex</li> <li>807: Layerwise Quantization for Neural Networks</li> <li>808: Image Retrieval and Image-Text Feature Alignment</li> <li>809: Emoji-Language Image Captioning</li> <li>810: Optimizing Reddit Posts</li> <li>811: Culinary Images' Feature-Extraction And Recipe Generation Using Deep Convolutional Neural Network</li> <li>812: Automatic Code Generation from UI Screenshots</li> <li>813: GUCCI GAN: Grouped Unique Contextually-Classified Inpainting GAN</li> <li>814: Adversarial Visual Question Answering</li> <li>816: Handwritten Mathematics to LaTeX Code</li> <li>817: Image captioning with attention</li> <li>818: Identifying and Solving CAPTCHAs with Deep Learning</li> <li>819: Mobile-Focused Networks for Classification of Chinese Text in the Wild</li> <li>820: End-to-end Dense Video Captioning with Reinforcement Learning</li> <li>823: Image Captioning using Policy Gradient Method</li> <li>825: Chinese Character Synthesization From Components</li> <li>826: Photo Quality Assessment with Deep CNNs</li> <li>827: Bridging 3D shape and natural language</li> <li>828: Neural Image Captioning on MSCOCO Dataset</li> </ul>"},{"location":"poster/","title":"2017 Stanford CS231n Poster Session","text":"<ul> <li>Location: Bing Concert Hall</li> <li>Parking: Parking information can be found here</li> <li>Sponsor Setup Time: 11:15 am - 12:00 pm</li> <li>Poster Session: 12:00 pm - 2:45 pm</li> <li>Award Ceremony: 2:55 pm - 3:15 pm</li> </ul> <p>The 2017 Stanford CS231N poster session will showcase projects in Convolutional Neural Networks for Visual Recognition that students have worked on over the past quarter. This year, 750 students will be presenting over 350 projects.  The topics range from Generative Adversarial Networks (GANs), healthcare and medical imaging, art and style transfer, satellite imaging, self-driving cars, video understanding and more! See the complete list of projects and poster session map below to find the location of a specific poster. We will be awarding 10+ awards to the top posters! The top prizes will be $500+ in value! Stanford affiliates (faculty, staff, students, alumni) and their guests are welcome to attend. Catered food and refreshments will be made available over the course of the event. This poster session is made possible through the generous support of Benchmark, Andreessen Horowitz, Nvidia and Apple! </p>"},{"location":"poster/#bing-map-see-the-project-ids-in-the-list-below-and-find-them-on-the-map","title":"Bing Map: See the Project IDs in the list below and find them on the map","text":""},{"location":"poster/#list-of-projects-3-digit-number-is-id-to-locate-the-poster","title":"List of Projects (3 Digit Number is ID to Locate the Poster)","text":"<ul> <li>101       Invasive Species Detection                                  </li> <li>102       Scene Classification with Convolutional Neural Networks                                 </li> <li>104       Adaptive Regularization for Neural Networks                                 </li> <li>105       Image-based Product Recommendation System with Convolutional Neural Networks                                    </li> <li>107       Metric Learning for Clustering Images from Unknown Classes                                  </li> <li>108       Intra-Class and Inter-Class Feature Learning through Deep Metric Learning for Large Scale Image Retrieval                                   </li> <li>110       Neural Combinatorial Optimization for Solving Jigsaw Puzzles                                    </li> <li>112       Faster R-CNN with RoI Refinement                                    </li> <li>113       CNNs for Object Recognition in Surveillance                                 </li> <li>114       Greedy Layer-wise Training for Weakly-supervised Object Localization and Segmentation                                   </li> <li>116       Design And Analysis of a Hardware CNN Accelerator                                   </li> <li>118       XNOR-Net on FPGA                                    </li> <li>119       Convolutional Neural Network Classification of Functional Shoe Types                                    </li> <li>121       CHILDNet: Curiosity-driven Human-In-the-Loop Deep Network                                   </li> <li>122       Automated Smart TV UI Performance Testing with Visual Recognition                                   </li> <li>125       Labeling Images with thematic and emotional content                                 </li> <li>126       Classifying U.S. Houses by Architectural Style                                  </li> <li>128       Finding Protests in Social Media Data                                   </li> <li>129       Deep Visual Learning of Reddit Images                                   </li> <li>130       Fast Softmax Sampling for Deep Neural Networks                                  </li> <li>131       Prototypical one-shot learning using high-dimensional embeddings                                    </li> <li>133       Malicious Dropout                                   </li> <li>134       Compression and Acceleration of CNN Training and Evaluation                                 </li> <li>135       YOLO for real time object detection on mobile                                   </li> <li>136       Identifying Architectural Styles by Convolutional Neural Network                                    </li> <li>200       UAV Depth Perception from Visual Images using a Deep Convolutional Neural Network                                   </li> <li>203       Depth Estimation from Single Image Using Convolutional Neural Networks                                  </li> <li>204       Improving 3D Scene Reconstruction through Object Recognition and Segmentation                                   </li> <li>205       Depth-Enhanced Classification Network                                   </li> <li>207       Depth Regression from a Single Monocular Image using a Multi-Scale Deep Network                                 </li> <li>209       Understanding Physical Geometry of a Scene from Single Monocular Images                                 </li> <li>210       StereoPhonic: Depth From Stereo on Phones                                   </li> <li>211       Deep Video Interpolation                                    </li> <li>212       To Post or Not To Post: Using CNNs to Classify Social Media Worthy Images                                   </li> <li>213       Item Removal Detection for Retail Environments with Neural Networks                                 </li> <li>214       Extracting Kinematic Information Using Pose Estimation                                  </li> <li>216       Assessing Driver Distraction from Real-Time Video                                   </li> <li>217       Hand Gesture Recognition using 3D models and Convolutional Neural Network                                   </li> <li>218       Hand Gesture Recognition using Image Processing and Convolutional Neural Network                                    </li> <li>219       Mobile, Marker-less, 3D Body Pose Estimation                                    </li> <li>220       Deep 3D Human Key Point Estimation                                  </li> <li>221       Touchy Feely: Real-time Emotion Recognition                                 </li> <li>222       Face detection and recognition with YOLO                                    </li> <li>223       Reconstructing Obfuscated Human Faces                                   </li> <li>224       Recognizing facial expressions using deep learning                                  </li> <li>225       Detection of Hand Grasping Tasks for \u201cGrab and Go\u201d Groceries                                    </li> <li>226       Ava Makeup                                  </li> <li>227       Lip Reading Word Classification                                 </li> <li>229       Gaze Capture with CNNs                                  </li> <li>230       Deep Networks for Robust Depth Estimation with Single-Photon Sensors                                    </li> <li>231       Facial Emotion Recognition for Wild Images                                  </li> <li>300       Effectiveness of Style Transfer as a Data Augmentation Technique                                    </li> <li>301       Historical and Modern Image-to-Image Translation with Generative Adversarial Networks                                   </li> <li>302       Colorization using ConvNets and GAN                                 </li> <li>304       Automatic Manga Colorization with Hint                                  </li> <li>305       Generative Adversarial Networks for Custom Image Generation                                 </li> <li>306       Evaluation of Image Completion Algorithm: Deep Convolutional Generative Adversarial Nets vs. Exemplar-Based Inpainting                                  </li> <li>307       Autoencoders for Inverse Rendering                                  </li> <li>308       Adversarial Generator-Encoder Networks Based Visual Recommender System                                  </li> <li>309       Semi-supervised learning via adversarial training                                   </li> <li>311       Chinese Painting Generation Using Deep Convolutional Generative Adversarial Networks                                    </li> <li>312       Super Resolution on Video using GANs                                    </li> <li>313       Using Generative Models for Semi-Supervised Learning                                    </li> <li>314       Class-Conditional Super-resolution with GANs                                    </li> <li>315       FlowGAN                                 </li> <li>316       Generative Text to Image Synthesis with Applications to Reinforcement Learning in Minecraft                                 </li> <li>317       Frame Interpolation Using Generative Adversarial Networks                                   </li> <li>318       Can We Train Dogs and Humans Together: Hidden Distribution GANs                                 </li> <li>319       Multi-task Learning on Multi-Spectral Images Using GANs for Predicting Poverty                                  </li> <li>320       From 2D Sketch to 3D Shading                                    </li> <li>322       Art Generation from text with GANs                                  </li> <li>323       Re(live) Photos: Predicting Neighboring Frames with GANs                                    </li> <li>324       Multi-instances text-to-image generation with StackGAN                                  </li> <li>325       Plant Disease Detection with Deep Learning                                  </li> <li>327       Label-Free Object Detection in Video                                    </li> <li>328       Filling the Blanks: GANs vs RNNs                                    </li> <li>329       Towards Facial Reconstruction from Sparse Data: Utilizing Encoder-Decoder Networks with GANs to Infer Facial Rotations                                  </li> <li>330       maaGMA: Modified-Adversarial-Autoencoding Generator with Multiple Adversaries                                   </li> <li>331       visual search by brushing                                   </li> <li>400       Using Convolutional Neural Networks to Predict Completion Year of Fine Art Paintings                                    </li> <li>401       Multi-style Transfer: Generalizing Style Transfer to an Artist                                  </li> <li>402       Style Transfer of Images Incorporating Depth Perception and Object Segmentation                                 </li> <li>403       Automatic Sketch Colourization                                  </li> <li>404       Semantic Segmented Style Transfer                                   </li> <li>405       Mixed Style Transfer                                    </li> <li>406       Artist Identification with Convolutional Neural Networks                                    </li> <li>407       Learning Instance Normalization Parameters for Real-Time Arbitrary Style Transfer                                   </li> <li>409       Automatic image colorization using deep neural networks                                 </li> <li>410       Classifying Rjiksmuseum Paintings by Artist                                 </li> <li>411       From Renaissance to Pop: A Study of Artistic Eras using Deep CNNs                                   </li> <li>412       Style transfer between two photographs                                  </li> <li>414       ArtTalk: Labeling Images with Thematic and Emotional Content                                    </li> <li>415       Labeling Paintings with Thematic and Emotional Content                                  </li> <li>416       Localized Style Transfer Using Semantic Segmentation                                    </li> <li>417       Smooth In-Image Style Transitions                                   </li> <li>418       AutoColorisation                                    </li> <li>419       Judging Thematic Similarity in Paintings                                    </li> <li>420       Freehand Sketch Recognition                                 </li> <li>421       EXIF Estimation With Convolutional Neural Networks                                  </li> <li>423       Full Resolutional Video Compression Using Recurrent Convolutional Neural Networks                                   </li> <li>424       HiDDeN: Hiding Data with Deep Networks                                  </li> <li>425       Line Drawing Colorization                                   </li> <li>426       DeepSynth: Synthesizing A Musical Instrument With Video                                 </li> <li>428       Exploring Style Transfer                                    </li> <li>500       Mice behaviour analysis in open field test                                  </li> <li>501       Unsupervised Deep Learning with variational autoencoder for Interpretable Mammogram CBIR and Risk scoring                                   </li> <li>502       Brain Tumor Segmentation                                    </li> <li>503       Vision-Based Approach to Senior Healthcare: Depth-Based Activity Recognition with Convolutional Neural Networks                                 </li> <li>505       Privacy-Preserving Knowledge Transfer for Hand Hygiene Detection                                    </li> <li>506       Depth-Based Activity Recognition in ICUs Using Convolutional Neural Networks                                    </li> <li>507       BURNED: Efficient and Accurate Burn Prognosis Using Deep Learning                                   </li> <li>511       Differentiating Tumor Cells from Healthy Cells in a Tumor Biopsy using CNNs                                 </li> <li>512       Multimodal Brain MRI Tumor Segmentation via Convolutional Neural Networks                                   </li> <li>513       Accelerating dynamic magnetic resonance image reconstruction using deep convolutional neural networks                                   </li> <li>514       Predict DNA methylation states from whole slide images of brain tumors                                  </li> <li>515       Predicting Lung Cancer Incidence from CT Imagery                                    </li> <li>516       Automatic Neuronal Cell Classification in Calcium Imaging with Convolutional Neural Networks                                    </li> <li>517       GlimpseNet: Multi-Instance Generative Attention for Full Mammogram Diagnosis                                    </li> <li>518       Deep Convolutional Neural Networks for Lung Cancer Detection                                    </li> <li>519       Predicting therapeutic efficacy in Non Small Cell Lung Cancer                                   </li> <li>520       Breast Cancer Image Segmentation: Cancer Cell vs Stroma                                 </li> <li>521       MRI to MGMT: Predicting Drug Efficacy for Glioblastoma Patients                                 </li> <li>523       Prediction of Head and Neck Cancer Submolecular Types from Pathology Images                                 </li> <li>524       Automated Detection of Diabetic Retinopathy using Deep Learning                                 </li> <li>525       Patch-based Head and Neck Cancer Subtype Classification                                 </li> <li>526       Deep Neural Nets for Brain Tumor Segmentation                                   </li> <li>527       Deep Learning for Chest X-ray abnormality detection                                 </li> <li>528       Multiparametric MR Image Analysis for Prostate Cancer Assessment with Convolutional Neural Networks                                 </li> <li>530       MR Contrast Prediction Using Deep Learning                                  </li> <li>531       Brendan - A Deep Convolutional Network for Representing Latent Features of Protein-Ligand Binding Poses                                 </li> <li>533       Using Deep Learning for Segmentation of Microscopy Images                                   </li> <li>534       Quantum Annealing Assisted Deep Learning for Lung Cancer Detection                                  </li> <li>536       Early Stage Integrated Circuit Design Efficacy Prediction using Congestion and Cell Density Images                                  </li> <li>537       Geological scenario identification using seismic impedance data                                 </li> <li>538       Variational Autoencoders for Classical Ising Models                                 </li> <li>539       Convolutional Neural Networks for Pile Up Identification in ATLAS                                   </li> <li>541       Deep Learning application to proton radiography analysis                                    </li> <li>542       Wave-dynamics simulation using deep neural networks                                 </li> <li>545       RouteAI: A Convolutional Neural Network based Router for Integrated Circuits                                    </li> <li>547       Convolutional Neural Networks For Automated Surface-Wettability Characterization                                    </li> <li>548       Data Classification with Residual Network for Single Particle Imaging Experiments                                   </li> <li>550       Extraction of Building Footprints from Satellite Imagery                                    </li> <li>551       Predicting Land Use and Atmospheric Conditions from Amazon Rainforest Satellite Imagery                                 </li> <li>552       Temporal Poverty Prediction                                 </li> <li>553       UAV Road Mapping Using Convolutional Neural Nets                                    </li> <li>554       Motion Prediction from Trajectory and Top-Down Visual Data                                  </li> <li>555       Understanding Localized Remote Sensing-Based Crop Yield Prediction using Convolutional Neural Networks                                  </li> <li>556       Neighborhood Watch: Using CNNs to Predict Income Brackets from Google StreetView Images                                 </li> <li>557       CNNs for wide-area precipitation estimation from geostationary satellite imagery                                    </li> <li>558       Mapping Tidal Salt Marshes                                  </li> <li>559       Predicting Health using Satellite Images                                    </li> <li>561       Convolutional Neural Nets for Segmentation of Satellite Imagery                                 </li> <li>563       Discovering Scenic Roads Using Neural Networks                                  </li> <li>564       Rail Network Detection from Aerial Imagery using Deep Learning                                  </li> <li>601       AI Attack: Learning to Play a Video Game Using Visual Inputs                                    </li> <li>602       Deep Predictive Action Conditional Neural Network for Frame Prediction in Atari Games                                   </li> <li>603       Playing Go without Game Tree Search Using Convolutional Neural Networks                                 </li> <li>604       Learning a Visual State Representation for Generative Adversarial Imitation Learning                                    </li> <li>605       Playing Geometry Dash with Deep Reinforcement Learning                                  </li> <li>606       End-to-end models for task-oriented visual dialogue with reinforcement                                  </li> <li>607       Classifying grocery items images using Convolutional Neural Networks                                    </li> <li>608       Imitation Learning with THOR                                    </li> <li>610       Differentiable Neural Computer for Maze Navigation                                  </li> <li>611       Playing Atari Games with Deep Learning                                  </li> <li>612       Trajectory-Aware Visual Navigation in Indoor Scenes with Target-driven Deep Reinforcement Learning                                  </li> <li>614       Indoor Navigation with Imitation Learning                                   </li> <li>616       Deep Q-Learning with OpenAI Gym                                 </li> <li>617       A3C Methods for General Game Playing                                    </li> <li>618       Deep Reinforcement Learning using Memory-based Approaches                                   </li> <li>619       FoxNet: A Deep-Learning Agent for Nintendo\u2019s Star Fox 64                                    </li> <li>620       Target-Driven Navigation with Imitation Learning                                    </li> <li>621       End-to-End Learning for Fighting Forest Fires (EELFFF)                                  </li> <li>622       Teach Me To Tango - Fidelity Estimation of Visual Odometry Systems for Robust Navigation                                    </li> <li>623       Indoor Target-driven Visual Navigation                                  </li> <li>624       NeuralKart: A Real-Time Mario Kart 64 AI                                    </li> <li>625       Understanding Driver\u2019s Intention Using Convolutional Neural Networks                                    </li> <li>626       Self-Driving Car Steering Angle Prediction Based on Image Recognition                                   </li> <li>627       Object Detection and Its Implementation on Android Devices                                  </li> <li>629       Convolutional Architectures for Self-Driving Cars                                   </li> <li>630       Real-Time Multiple Object Tracking for Autonomous Cars                                  </li> <li>631       Single Stage Detector for Object Detection                                  </li> <li>632       Convolutional Neural Network Information Fusion based on Dempster-Shafer Theory for Urban Scene Understanding                                   </li> <li>633       Street View Instance Segmentation using R-CNN models                                    </li> <li>700       Detecting videos containing (child) pornography                                 </li> <li>701       Superflow: Frame Prediction with Convolutional Neural Network                                   </li> <li>702       YouTube8-M: Video Classification                                    </li> <li>705       YouTube-8M Video Classificatoin                                 </li> <li>707       YOLO-based Adaptive Window Two-stream Convolutional Neural Network for Video Classification                                 </li> <li>708       Spotlight: A Smart Video Highlight Generator                                    </li> <li>709       Video Understanding: From Video Classification to Video Captioning                                  </li> <li>710       Selecting Youtube Video Thumbnails via Convolutional Neural Networks                                    </li> <li>711       Google Cloud and YouTube-8M Video Understanding Challenge                                   </li> <li>713       Prediction of Personality First Impression with Deep Bimodal LSTM                                   </li> <li>714       Video frame Interpolation and extrapolation                                 </li> <li>715       Using Convolutional Neural Networks to Classify Noisy Sports Videos                                 </li> <li>716       Detecting Guns in Video Content                                 </li> <li>717       Soccer Stats with Computer Vision                                   </li> <li>718       Real-time Surveillance Video Analytics System                                   </li> <li>800       Using Graphical Programming Languages Designed for Novices to Train Neural Networks to Understand and Write Simple Programs                                 </li> <li>801       Siamese Convolutional Neural Networks for Authorship Verification                                   </li> <li>804       Deep Convolutional Neural Networks for Automatic Speech Recognition                                 </li> <li>805       CLEVR Advancements for High-Level Visual Reasoning Programs                                 </li> <li>806       Pix2Sketch                                  </li> <li>807       Testing Image Understanding through Question Answering                                  </li> <li>808       Real-time Object detection                                  </li> <li>810       Handwritten Text Recognition using Deep Learning                                    </li> <li>811       Bilinear Pooling and Co-Attention Inspired Models for Visual Question Answering                                 </li> <li>812       Applying NLP Deep Learning Ideas to Image Classification                                    </li> <li>814       You CAN Judge a Book by its Cover                                   </li> <li>815       Image to Latex                                  </li> <li>816       Combining RNN and CNN to Understand the Usefulness of Yelp Reviews                                  </li> <li>817       Where is Waldo, a deep-learning approach to general purpose template matching                                   </li> <li>818       An Automated Art Historian                                  </li> <li>819       Deep Bayesian Pragmatics for Image Captioning                                   </li> <li>900       Predicting Deforestation in the Amazon                                  </li> <li>902       Amazon Rainforest Satellite Image Labeling                                  </li> <li>903       Understanding the Amazon from Space                                 </li> <li>904       Understand Amazon Deforestation using Neural Network                                    </li> <li>905       Multi-label Classification on Satellite Images of the Amazon Rainforest                                 </li> <li>906       Deep Multi-Label Classification for High Resolution Satellite Imagery of Rainforests                                    </li> <li>907       Classification of natural landmarks and human footprint of Amazon using satellite data                                  </li> <li>908       Image Classification with Deep Learning                                 </li> <li>909       Understanding the Amazon Basin from Space                                   </li> <li>911       Deeprootz: Mapping the Amazon                                   </li> <li>912       Understanding the Amazon from Space                                 </li> <li>913       Understanding the Amazon from Space                                 </li> <li>914       Understanding the Amazon from Space                                 </li> <li>915       DeepRootz: Classifying satellite images of the Amazon rainforest                                    </li> <li>916       Trainforest                                 </li> <li>917       Land Cover Classification in the Amazon                                 </li> <li>918       Tracking Human Impact on the Amazon rainforest                                  </li> <li>919       Understanding the Amazon from Space                                 </li> <li>920       Guiding the management of cervical cancer with convolutional neural networks                                    </li> <li>921       Cervix Screening Classification for Cancer Preventive Treatment                                 </li> <li>922       Deep Learning Approaches for Determining Optimal Cervical Cancer Treatment                                  </li> <li>923       Cervix Type Classification Using Deep Learning and Image Classification                                 </li> <li>924       Deep Learning for Cervical Cancer                                   </li> <li>925       Identifying Cervix Types using Deep Convolutional Networks                                  </li> <li>926       Exploring Methods on Tiny ImageNet Problem                                  </li> <li>927       Wide Residual Network for the Tiny Image Net Challenge                                  </li> <li>928       The Power of Inception: Tackling the Tiny ImageNet Challenge                                    </li> <li>929       Unconstrained Handwriting Recognition                                   </li> <li>930       Tiny ImageNet Challenge                                 </li> <li>931       Deep Convolutional Neural Networks for ImageNet Classification                                  </li> <li>933       Overwhelming Tiny ImageNet: Bayesian Optimization on Residual Networks                                  </li> <li>934       A Dense Take on Inception                                   </li> <li>935       Tiny ImageNet Challenge                                 </li> <li>937       Classification on Tiny ImageNet                                 </li> <li>938       Tiny ImageNet Challenge Investigating the Scaling of Inception Layers for Reduced Scale Classi\ufb01cation Problems                                  </li> <li>939       Attention Networks for ImageNet Classification                                  </li> <li>940       Image Classification for Tiny ImageNet Challenge                                    </li> <li>941       Analyzing Deforestation in the Amazon                                   </li> </ul>"},{"location":"python-colab/","title":"CS231n Python Tutorial With Google Colab","text":"<p>This tutorial was originally written by Justin Johnson for cs231n. It was adapted as a Jupyter notebook for cs228 by Volodymyr Kuleshov and Isaac Caswell.</p> <p>This version has been adapted for Colab by Kevin Zakka for the Spring 2020 edition of cs231n. It runs Python3 by default.</p> <p>Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.</p> <p>We expect that many of you will have some experience with Python and numpy; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.</p> <p>Some of you may have previous knowledge in Matlab, in which case we also recommend the numpy for Matlab users page (https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html).</p> <p>In this tutorial, we will cover:</p> <ul> <li>Basic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes</li> <li>Numpy: Arrays, Array indexing, Datatypes, Array math, Broadcasting</li> <li>Matplotlib: Plotting, Subplots, Images</li> <li>IPython: Creating notebooks, Typical workflows</li> </ul> In\u00a0[110]: Copied! <pre>!python --version\n</pre> !python --version <pre>Python 3.6.9\n</pre> <p>Python is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:</p> In\u00a0[2]: Copied! <pre>def quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nprint(quicksort([3,6,8,10,1,2,1]))\n</pre> def quicksort(arr):     if len(arr) &lt;= 1:         return arr     pivot = arr[len(arr) // 2]     left = [x for x in arr if x &lt; pivot]     middle = [x for x in arr if x == pivot]     right = [x for x in arr if x &gt; pivot]     return quicksort(left) + middle + quicksort(right)  print(quicksort([3,6,8,10,1,2,1])) <pre>[1, 1, 2, 3, 6, 8, 10]\n</pre> <p>Integers and floats work as you would expect from other languages:</p> In\u00a0[3]: Copied! <pre>x = 3\nprint(x, type(x))\n</pre> x = 3 print(x, type(x)) <pre>3 &lt;class 'int'&gt;\nERROR! Session/line number was not unique in database. History logging moved to new session 60\n</pre> In\u00a0[4]: Copied! <pre>print(x + 1)   # Addition\nprint(x - 1)   # Subtraction\nprint(x * 2)   # Multiplication\nprint(x ** 2)  # Exponentiation\n</pre> print(x + 1)   # Addition print(x - 1)   # Subtraction print(x * 2)   # Multiplication print(x ** 2)  # Exponentiation <pre>4\n2\n6\n9\n</pre> In\u00a0[6]: Copied! <pre>x += 1\nprint(x)\nx *= 2\nprint(x)\n</pre> x += 1 print(x) x *= 2 print(x) <pre>9\n18\n</pre> In\u00a0[8]: Copied! <pre>y = 2.5\nprint(type(y))\nprint(y, y + 1, y * 2, y ** 2)\n</pre> y = 2.5 print(type(y)) print(y, y + 1, y * 2, y ** 2) <pre>&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n</pre> <p>Note that unlike many languages, Python does not have unary increment (x++) or decrement (x--) operators.</p> <p>Python also has built-in types for long integers and complex numbers; you can find all of the details in the documentation.</p> <p>Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols (<code>&amp;&amp;</code>, <code>||</code>, etc.):</p> In\u00a0[9]: Copied! <pre>t, f = True, False\nprint(type(t))\n</pre> t, f = True, False print(type(t)) <pre>&lt;class 'bool'&gt;\n</pre> <p>Now we let's look at the operations:</p> In\u00a0[10]: Copied! <pre>print(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n</pre> print(t and f) # Logical AND; print(t or f)  # Logical OR; print(not t)   # Logical NOT; print(t != f)  # Logical XOR; <pre>False\nTrue\nFalse\nTrue\n</pre> In\u00a0[11]: Copied! <pre>hello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n</pre> hello = 'hello'   # String literals can use single quotes world = \"world\"   # or double quotes; it does not matter print(hello, len(hello)) <pre>hello 5\n</pre> In\u00a0[12]: Copied! <pre>hw = hello + ' ' + world  # String concatenation\nprint(hw)\n</pre> hw = hello + ' ' + world  # String concatenation print(hw) <pre>hello world\n</pre> In\u00a0[14]: Copied! <pre>hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n</pre> hw12 = '{} {} {}'.format(hello, world, 12)  # string formatting print(hw12) <pre>hello world 12\n</pre> <p>String objects have a bunch of useful methods; for example:</p> In\u00a0[15]: Copied! <pre>s = \"hello\"\nprint(s.capitalize())  # Capitalize a string\nprint(s.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(s.rjust(7))      # Right-justify a string, padding with spaces\nprint(s.center(7))     # Center a string, padding with spaces\nprint(s.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n</pre> s = \"hello\" print(s.capitalize())  # Capitalize a string print(s.upper())       # Convert a string to uppercase; prints \"HELLO\" print(s.rjust(7))      # Right-justify a string, padding with spaces print(s.center(7))     # Center a string, padding with spaces print(s.replace('l', '(ell)'))  # Replace all instances of one substring with another print('  world '.strip())  # Strip leading and trailing whitespace <pre>Hello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n</pre> <p>You can find a list of all string methods in the documentation.</p> <p>Python includes several built-in container types: lists, dictionaries, sets, and tuples.</p> <p>A list is the Python equivalent of an array, but is resizeable and can contain elements of different types:</p> In\u00a0[18]: Copied! <pre>xs = [3, 1, 2]   # Create a list\nprint(xs, xs[2])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n</pre> xs = [3, 1, 2]   # Create a list print(xs, xs[2]) print(xs[-1])     # Negative indices count from the end of the list; prints \"2\" <pre>[3, 1, 2] 2\n2\n</pre> In\u00a0[19]: Copied! <pre>xs[2] = 'foo'    # Lists can contain elements of different types\nprint(xs)\n</pre> xs[2] = 'foo'    # Lists can contain elements of different types print(xs) <pre>[3, 1, 'foo']\n</pre> In\u00a0[20]: Copied! <pre>xs.append('bar') # Add a new element to the end of the list\nprint(xs)\n</pre> xs.append('bar') # Add a new element to the end of the list print(xs)   <pre>[3, 1, 'foo', 'bar']\n</pre> In\u00a0[21]: Copied! <pre>x = xs.pop()     # Remove and return the last element of the list\nprint(x, xs)\n</pre> x = xs.pop()     # Remove and return the last element of the list print(x, xs) <pre>bar [3, 1, 'foo']\n</pre> <p>As usual, you can find all the gory details about lists in the documentation.</p> <p>In addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:</p> In\u00a0[23]: Copied! <pre>nums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n</pre> nums = list(range(5))    # range is a built-in function that creates a list of integers print(nums)         # Prints \"[0, 1, 2, 3, 4]\" print(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\" print(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\" print(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\" print(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\" print(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\" nums[2:4] = [8, 9] # Assign a new sublist to a slice print(nums)         # Prints \"[0, 1, 8, 9, 4]\" <pre>[0, 1, 2, 3, 4]\n[2, 3]\n[2, 3, 4]\n[0, 1]\n[0, 1, 2, 3, 4]\n[0, 1, 2, 3]\n[0, 1, 8, 9, 4]\n</pre> <p>You can loop over the elements of a list like this:</p> In\u00a0[24]: Copied! <pre>animals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n</pre> animals = ['cat', 'dog', 'monkey'] for animal in animals:     print(animal) <pre>cat\ndog\nmonkey\n</pre> <p>If you want access to the index of each element within the body of a loop, use the built-in <code>enumerate</code> function:</p> In\u00a0[25]: Copied! <pre>animals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n</pre> animals = ['cat', 'dog', 'monkey'] for idx, animal in enumerate(animals):     print('#{}: {}'.format(idx + 1, animal)) <pre>#1: cat\n#2: dog\n#3: monkey\n</pre> <p>When programming, frequently we want to transform one type of data into another. As a simple example, consider the following code that computes square numbers:</p> In\u00a0[26]: Copied! <pre>nums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n</pre> nums = [0, 1, 2, 3, 4] squares = [] for x in nums:     squares.append(x ** 2) print(squares) <pre>[0, 1, 4, 9, 16]\n</pre> <p>You can make this code simpler using a list comprehension:</p> In\u00a0[27]: Copied! <pre>nums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n</pre> nums = [0, 1, 2, 3, 4] squares = [x ** 2 for x in nums] print(squares) <pre>[0, 1, 4, 9, 16]\n</pre> <p>List comprehensions can also contain conditions:</p> In\u00a0[28]: Copied! <pre>nums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n</pre> nums = [0, 1, 2, 3, 4] even_squares = [x ** 2 for x in nums if x % 2 == 0] print(even_squares) <pre>[0, 4, 16]\n</pre> <p>A dictionary stores (key, value) pairs, similar to a <code>Map</code> in Java or an object in Javascript. You can use it like this:</p> In\u00a0[29]: Copied! <pre>d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n</pre> d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data print(d['cat'])       # Get an entry from a dictionary; prints \"cute\" print('cat' in d)     # Check if a dictionary has a given key; prints \"True\" <pre>cute\nTrue\n</pre> In\u00a0[30]: Copied! <pre>d['fish'] = 'wet'    # Set an entry in a dictionary\nprint(d['fish'])      # Prints \"wet\"\n</pre> d['fish'] = 'wet'    # Set an entry in a dictionary print(d['fish'])      # Prints \"wet\" <pre>wet\n</pre> In\u00a0[31]: Copied! <pre>print(d['monkey'])  # KeyError: 'monkey' not a key of d\n</pre> print(d['monkey'])  # KeyError: 'monkey' not a key of d <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-31-78fc9745d9cf&gt; in &lt;module&gt;()\n----&gt; 1 print(d['monkey'])  # KeyError: 'monkey' not a key of d\n\nKeyError: 'monkey'</pre> In\u00a0[32]: Copied! <pre>print(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\"\nprint(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\"\n</pre> print(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\" print(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\" <pre>N/A\nwet\n</pre> In\u00a0[33]: Copied! <pre>del d['fish']        # Remove an element from a dictionary\nprint(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\"\n</pre> del d['fish']        # Remove an element from a dictionary print(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\" <pre>N/A\n</pre> <p>You can find all you need to know about dictionaries in the documentation.</p> <p>It is easy to iterate over the keys in a dictionary:</p> In\u00a0[34]: Copied! <pre>d = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A {} has {} legs'.format(animal, legs))\n</pre> d = {'person': 2, 'cat': 4, 'spider': 8} for animal, legs in d.items():     print('A {} has {} legs'.format(animal, legs)) <pre>A person has 2 legs\nA cat has 4 legs\nA spider has 8 legs\n</pre> <p>Dictionary comprehensions: These are similar to list comprehensions, but allow you to easily construct dictionaries. For example:</p> In\u00a0[35]: Copied! <pre>nums = [0, 1, 2, 3, 4]\neven_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\nprint(even_num_to_square)\n</pre> nums = [0, 1, 2, 3, 4] even_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0} print(even_num_to_square) <pre>{0: 0, 2: 4, 4: 16}\n</pre> <p>A set is an unordered collection of distinct elements. As a simple example, consider the following:</p> In\u00a0[36]: Copied! <pre>animals = {'cat', 'dog'}\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' in animals)  # prints \"False\"\n</pre> animals = {'cat', 'dog'} print('cat' in animals)   # Check if an element is in a set; prints \"True\" print('fish' in animals)  # prints \"False\"  <pre>True\nFalse\n</pre> In\u00a0[37]: Copied! <pre>animals.add('fish')      # Add an element to a set\nprint('fish' in animals)\nprint(len(animals))       # Number of elements in a set;\n</pre> animals.add('fish')      # Add an element to a set print('fish' in animals) print(len(animals))       # Number of elements in a set; <pre>True\n3\n</pre> In\u00a0[38]: Copied! <pre>animals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals))       \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))\n</pre> animals.add('cat')       # Adding an element that is already in the set does nothing print(len(animals))        animals.remove('cat')    # Remove an element from a set print(len(animals))        <pre>3\n2\n</pre> <p>Loops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:</p> In\u00a0[40]: Copied! <pre>animals = {'cat', 'dog', 'fish'}\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal))\n</pre> animals = {'cat', 'dog', 'fish'} for idx, animal in enumerate(animals):     print('#{}: {}'.format(idx + 1, animal)) <pre>#1: dog\n#2: cat\n#3: fish\n</pre> <p>Set comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:</p> In\u00a0[41]: Copied! <pre>from math import sqrt\nprint({int(sqrt(x)) for x in range(30)})\n</pre> from math import sqrt print({int(sqrt(x)) for x in range(30)}) <pre>{0, 1, 2, 3, 4, 5}\n</pre> <p>A tuple is an (immutable) ordered list of values. A tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:</p> In\u00a0[42]: Copied! <pre>d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n</pre> d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys t = (5, 6)       # Create a tuple print(type(t)) print(d[t])        print(d[(1, 2)]) <pre>&lt;class 'tuple'&gt;\n5\n1\n</pre> In\u00a0[43]: Copied! <pre>t[0] = 1\n</pre> t[0] = 1 <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-43-c8aeb8cd20ae&gt; in &lt;module&gt;()\n----&gt; 1 t[0] = 1\n\nTypeError: 'tuple' object does not support item assignment</pre> <p>Python functions are defined using the <code>def</code> keyword. For example:</p> In\u00a0[44]: Copied! <pre>def sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\nfor x in [-1, 0, 1]:\n    print(sign(x))\n</pre> def sign(x):     if x &gt; 0:         return 'positive'     elif x &lt; 0:         return 'negative'     else:         return 'zero'  for x in [-1, 0, 1]:     print(sign(x)) <pre>negative\nzero\npositive\n</pre> <p>We will often define functions to take optional keyword arguments, like this:</p> In\u00a0[46]: Copied! <pre>def hello(name, loud=False):\n    if loud:\n        print('HELLO, {}'.format(name.upper()))\n    else:\n        print('Hello, {}!'.format(name))\n\nhello('Bob')\nhello('Fred', loud=True)\n</pre> def hello(name, loud=False):     if loud:         print('HELLO, {}'.format(name.upper()))     else:         print('Hello, {}!'.format(name))  hello('Bob') hello('Fred', loud=True) <pre>Hello, Bob!\nHELLO, FRED\n</pre> <p>The syntax for defining classes in Python is straightforward:</p> In\u00a0[48]: Copied! <pre>class Greeter:\n\n    # Constructor\n    def __init__(self, name):\n        self.name = name  # Create an instance variable\n\n    # Instance method\n    def greet(self, loud=False):\n        if loud:\n          print('HELLO, {}'.format(self.name.upper()))\n        else:\n          print('Hello, {}!'.format(self.name))\n\ng = Greeter('Fred')  # Construct an instance of the Greeter class\ng.greet()            # Call an instance method; prints \"Hello, Fred\"\ng.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\"\n</pre> class Greeter:      # Constructor     def __init__(self, name):         self.name = name  # Create an instance variable      # Instance method     def greet(self, loud=False):         if loud:           print('HELLO, {}'.format(self.name.upper()))         else:           print('Hello, {}!'.format(self.name))  g = Greeter('Fred')  # Construct an instance of the Greeter class g.greet()            # Call an instance method; prints \"Hello, Fred\" g.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\" <pre>Hello, Fred!\nHELLO, FRED\n</pre> <p>Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. If you are already familiar with MATLAB, you might find this tutorial useful to get started with Numpy.</p> <p>To use Numpy, we first need to import the <code>numpy</code> package:</p> In\u00a0[0]: Copied! <pre>import numpy as np\n</pre> import numpy as np <p>A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension.</p> <p>We can initialize numpy arrays from nested Python lists, and access elements using square brackets:</p> In\u00a0[50]: Copied! <pre>a = np.array([1, 2, 3])  # Create a rank 1 array\nprint(type(a), a.shape, a[0], a[1], a[2])\na[0] = 5                 # Change an element of the array\nprint(a)\n</pre> a = np.array([1, 2, 3])  # Create a rank 1 array print(type(a), a.shape, a[0], a[1], a[2]) a[0] = 5                 # Change an element of the array print(a)                   <pre>&lt;class 'numpy.ndarray'&gt; (3,) 1 2 3\n[5 2 3]\n</pre> In\u00a0[51]: Copied! <pre>b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array\nprint(b)\n</pre> b = np.array([[1,2,3],[4,5,6]])   # Create a rank 2 array print(b) <pre>[[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[52]: Copied! <pre>print(b.shape)\nprint(b[0, 0], b[0, 1], b[1, 0])\n</pre> print(b.shape) print(b[0, 0], b[0, 1], b[1, 0]) <pre>(2, 3)\n1 2 4\n</pre> <p>Numpy also provides many functions to create arrays:</p> In\u00a0[53]: Copied! <pre>a = np.zeros((2,2))  # Create an array of all zeros\nprint(a)\n</pre> a = np.zeros((2,2))  # Create an array of all zeros print(a) <pre>[[0. 0.]\n [0. 0.]]\n</pre> In\u00a0[54]: Copied! <pre>b = np.ones((1,2))   # Create an array of all ones\nprint(b)\n</pre> b = np.ones((1,2))   # Create an array of all ones print(b) <pre>[[1. 1.]]\n</pre> In\u00a0[56]: Copied! <pre>c = np.full((2,2), 7) # Create a constant array\nprint(c)\n</pre> c = np.full((2,2), 7) # Create a constant array print(c) <pre>[[7 7]\n [7 7]]\n</pre> In\u00a0[57]: Copied! <pre>d = np.eye(2)        # Create a 2x2 identity matrix\nprint(d)\n</pre> d = np.eye(2)        # Create a 2x2 identity matrix print(d) <pre>[[1. 0.]\n [0. 1.]]\n</pre> In\u00a0[58]: Copied! <pre>e = np.random.random((2,2)) # Create an array filled with random values\nprint(e)\n</pre> e = np.random.random((2,2)) # Create an array filled with random values print(e) <pre>[[0.8690054  0.57244319]\n [0.29647245 0.81464494]]\n</pre> <p>Numpy offers several ways to index into arrays.</p> <p>Slicing: Similar to Python lists, numpy arrays can be sliced. Since arrays may be multidimensional, you must specify a slice for each dimension of the array:</p> In\u00a0[59]: Copied! <pre>import numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\nprint(b)\n</pre> import numpy as np  # Create the following rank 2 array with shape (3, 4) # [[ 1  2  3  4] #  [ 5  6  7  8] #  [ 9 10 11 12]] a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])  # Use slicing to pull out the subarray consisting of the first 2 rows # and columns 1 and 2; b is the following array of shape (2, 2): # [[2 3] #  [6 7]] b = a[:2, 1:3] print(b) <pre>[[2 3]\n [6 7]]\n</pre> <p>A slice of an array is a view into the same data, so modifying it will modify the original array.</p> In\u00a0[60]: Copied! <pre>print(a[0, 1])\nb[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\nprint(a[0, 1])\n</pre> print(a[0, 1]) b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1] print(a[0, 1])  <pre>2\n77\n</pre> <p>You can also mix integer indexing with slice indexing. However, doing so will yield an array of lower rank than the original array. Note that this is quite different from the way that MATLAB handles array slicing:</p> In\u00a0[61]: Copied! <pre># Create the following rank 2 array with shape (3, 4)\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\nprint(a)\n</pre> # Create the following rank 2 array with shape (3, 4) a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) print(a) <pre>[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n</pre> <p>Two ways of accessing the data in the middle row of the array. Mixing integer indexing with slices yields an array of lower rank, while using only slices yields an array of the same rank as the original array:</p> In\u00a0[63]: Copied! <pre>row_r1 = a[1, :]    # Rank 1 view of the second row of a  \nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nrow_r3 = a[[1], :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)\nprint(row_r2, row_r2.shape)\nprint(row_r3, row_r3.shape)\n</pre> row_r1 = a[1, :]    # Rank 1 view of the second row of a   row_r2 = a[1:2, :]  # Rank 2 view of the second row of a row_r3 = a[[1], :]  # Rank 2 view of the second row of a print(row_r1, row_r1.shape) print(row_r2, row_r2.shape) print(row_r3, row_r3.shape) <pre>[5 6 7 8] (4,)\n[[5 6 7 8]] (1, 4)\n[[5 6 7 8]] (1, 4)\n</pre> In\u00a0[64]: Copied! <pre># We can make the same distinction when accessing columns of an array:\ncol_r1 = a[:, 1]\ncol_r2 = a[:, 1:2]\nprint(col_r1, col_r1.shape)\nprint()\nprint(col_r2, col_r2.shape)\n</pre> # We can make the same distinction when accessing columns of an array: col_r1 = a[:, 1] col_r2 = a[:, 1:2] print(col_r1, col_r1.shape) print() print(col_r2, col_r2.shape) <pre>[ 2  6 10] (3,)\n\n[[ 2]\n [ 6]\n [10]] (3, 1)\n</pre> <p>Integer array indexing: When you index into numpy arrays using slicing, the resulting array view will always be a subarray of the original array. In contrast, integer array indexing allows you to construct arbitrary arrays using the data from another array. Here is an example:</p> In\u00a0[66]: Copied! <pre>a = np.array([[1,2], [3, 4], [5, 6]])\n\n# An example of integer array indexing.\n# The returned array will have shape (3,) and \nprint(a[[0, 1, 2], [0, 1, 0]])\n\n# The above example of integer array indexing is equivalent to this:\nprint(np.array([a[0, 0], a[1, 1], a[2, 0]]))\n</pre> a = np.array([[1,2], [3, 4], [5, 6]])  # An example of integer array indexing. # The returned array will have shape (3,) and  print(a[[0, 1, 2], [0, 1, 0]])  # The above example of integer array indexing is equivalent to this: print(np.array([a[0, 0], a[1, 1], a[2, 0]])) <pre>[1 4 5]\n[1 4 5]\n</pre> In\u00a0[67]: Copied! <pre># When using integer array indexing, you can reuse the same\n# element from the source array:\nprint(a[[0, 0], [1, 1]])\n\n# Equivalent to the previous integer array indexing example\nprint(np.array([a[0, 1], a[0, 1]]))\n</pre> # When using integer array indexing, you can reuse the same # element from the source array: print(a[[0, 0], [1, 1]])  # Equivalent to the previous integer array indexing example print(np.array([a[0, 1], a[0, 1]])) <pre>[2 2]\n[2 2]\n</pre> <p>One useful trick with integer array indexing is selecting or mutating one element from each row of a matrix:</p> In\u00a0[68]: Copied! <pre># Create a new array from which we will select elements\na = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nprint(a)\n</pre> # Create a new array from which we will select elements a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) print(a) <pre>[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n</pre> In\u00a0[70]: Copied! <pre># Create an array of indices\nb = np.array([0, 2, 0, 1])\n\n# Select one element from each row of a using the indices in b\nprint(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n</pre> # Create an array of indices b = np.array([0, 2, 0, 1])  # Select one element from each row of a using the indices in b print(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\" <pre>[ 1  6  7 11]\n</pre> In\u00a0[71]: Copied! <pre># Mutate one element from each row of a using the indices in b\na[np.arange(4), b] += 10\nprint(a)\n</pre> # Mutate one element from each row of a using the indices in b a[np.arange(4), b] += 10 print(a) <pre>[[11  2  3]\n [ 4  5 16]\n [17  8  9]\n [10 21 12]]\n</pre> <p>Boolean array indexing: Boolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:</p> In\u00a0[72]: Copied! <pre>import numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n</pre> import numpy as np  a = np.array([[1,2], [3, 4], [5, 6]])  bool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;                     # this returns a numpy array of Booleans of the same                     # shape as a, where each slot of bool_idx tells                     # whether that element of a is &gt; 2.  print(bool_idx) <pre>[[False False]\n [ True  True]\n [ True  True]]\n</pre> In\u00a0[73]: Copied! <pre># We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n</pre> # We use boolean array indexing to construct a rank 1 array # consisting of the elements of a corresponding to the True values # of bool_idx print(a[bool_idx])  # We can do all of the above in a single concise statement: print(a[a &gt; 2]) <pre>[3 4 5 6]\n[3 4 5 6]\n</pre> <p>For brevity we have left out a lot of details about numpy array indexing; if you want to know more you should read the documentation.</p> <p>Every numpy array is a grid of elements of the same type. Numpy provides a large set of numeric datatypes that you can use to construct arrays. Numpy tries to guess a datatype when you create an array, but functions that construct arrays usually also include an optional argument to explicitly specify the datatype. Here is an example:</p> In\u00a0[74]: Copied! <pre>x = np.array([1, 2])  # Let numpy choose the datatype\ny = np.array([1.0, 2.0])  # Let numpy choose the datatype\nz = np.array([1, 2], dtype=np.int64)  # Force a particular datatype\n\nprint(x.dtype, y.dtype, z.dtype)\n</pre> x = np.array([1, 2])  # Let numpy choose the datatype y = np.array([1.0, 2.0])  # Let numpy choose the datatype z = np.array([1, 2], dtype=np.int64)  # Force a particular datatype  print(x.dtype, y.dtype, z.dtype) <pre>int64 float64 int64\n</pre> <p>You can read all about numpy datatypes in the documentation.</p> <p>Basic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module:</p> In\u00a0[75]: Copied! <pre>x = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\n\n# Elementwise sum; both produce the array\nprint(x + y)\nprint(np.add(x, y))\n</pre> x = np.array([[1,2],[3,4]], dtype=np.float64) y = np.array([[5,6],[7,8]], dtype=np.float64)  # Elementwise sum; both produce the array print(x + y) print(np.add(x, y)) <pre>[[ 6.  8.]\n [10. 12.]]\n[[ 6.  8.]\n [10. 12.]]\n</pre> In\u00a0[76]: Copied! <pre># Elementwise difference; both produce the array\nprint(x - y)\nprint(np.subtract(x, y))\n</pre> # Elementwise difference; both produce the array print(x - y) print(np.subtract(x, y)) <pre>[[-4. -4.]\n [-4. -4.]]\n[[-4. -4.]\n [-4. -4.]]\n</pre> In\u00a0[77]: Copied! <pre># Elementwise product; both produce the array\nprint(x * y)\nprint(np.multiply(x, y))\n</pre> # Elementwise product; both produce the array print(x * y) print(np.multiply(x, y)) <pre>[[ 5. 12.]\n [21. 32.]]\n[[ 5. 12.]\n [21. 32.]]\n</pre> In\u00a0[78]: Copied! <pre># Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\nprint(np.divide(x, y))\n</pre> # Elementwise division; both produce the array # [[ 0.2         0.33333333] #  [ 0.42857143  0.5       ]] print(x / y) print(np.divide(x, y)) <pre>[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n[[0.2        0.33333333]\n [0.42857143 0.5       ]]\n</pre> In\u00a0[79]: Copied! <pre># Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n</pre> # Elementwise square root; produces the array # [[ 1.          1.41421356] #  [ 1.73205081  2.        ]] print(np.sqrt(x)) <pre>[[1.         1.41421356]\n [1.73205081 2.        ]]\n</pre> <p>Note that unlike MATLAB, <code>*</code> is elementwise multiplication, not matrix multiplication. We instead use the dot function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices. dot is available both as a function in the numpy module and as an instance method of array objects:</p> In\u00a0[82]: Copied! <pre>x = np.array([[1,2],[3,4]])\ny = np.array([[5,6],[7,8]])\n\nv = np.array([9,10])\nw = np.array([11, 12])\n\n# Inner product of vectors; both produce 219\nprint(v.dot(w))\nprint(np.dot(v, w))\n</pre> x = np.array([[1,2],[3,4]]) y = np.array([[5,6],[7,8]])  v = np.array([9,10]) w = np.array([11, 12])  # Inner product of vectors; both produce 219 print(v.dot(w)) print(np.dot(v, w)) <pre>219\n219\n</pre> <p>You can also use the <code>@</code> operator which is equivalent to numpy's <code>dot</code> operator.</p> In\u00a0[83]: Copied! <pre>print(v @ w)\n</pre> print(v @ w) <pre>219\n</pre> In\u00a0[86]: Copied! <pre># Matrix / vector product; both produce the rank 1 array [29 67]\nprint(x.dot(v))\nprint(np.dot(x, v))\nprint(x @ v)\n</pre> # Matrix / vector product; both produce the rank 1 array [29 67] print(x.dot(v)) print(np.dot(x, v)) print(x @ v) <pre>[29 67]\n[29 67]\n[29 67]\n</pre> In\u00a0[87]: Copied! <pre># Matrix / matrix product; both produce the rank 2 array\n# [[19 22]\n#  [43 50]]\nprint(x.dot(y))\nprint(np.dot(x, y))\nprint(x @ y)\n</pre> # Matrix / matrix product; both produce the rank 2 array # [[19 22] #  [43 50]] print(x.dot(y)) print(np.dot(x, y)) print(x @ y) <pre>[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n[[19 22]\n [43 50]]\n</pre> <p>Numpy provides many useful functions for performing computations on arrays; one of the most useful is <code>sum</code>:</p> In\u00a0[88]: Copied! <pre>x = np.array([[1,2],[3,4]])\n\nprint(np.sum(x))  # Compute sum of all elements; prints \"10\"\nprint(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\"\n</pre> x = np.array([[1,2],[3,4]])  print(np.sum(x))  # Compute sum of all elements; prints \"10\" print(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\" print(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\" <pre>10\n[4 6]\n[3 7]\n</pre> <p>You can find the full list of mathematical functions provided by numpy in the documentation.</p> <p>Apart from computing mathematical functions using arrays, we frequently need to reshape or otherwise manipulate data in arrays. The simplest example of this type of operation is transposing a matrix; to transpose a matrix, simply use the T attribute of an array object:</p> In\u00a0[90]: Copied! <pre>print(x)\nprint(\"transpose\\n\", x.T)\n</pre> print(x) print(\"transpose\\n\", x.T) <pre>[[1 2]\n [3 4]]\ntranspose\n [[1 3]\n [2 4]]\n</pre> In\u00a0[91]: Copied! <pre>v = np.array([[1,2,3]])\nprint(v )\nprint(\"transpose\\n\", v.T)\n</pre> v = np.array([[1,2,3]]) print(v ) print(\"transpose\\n\", v.T) <pre>[[1 2 3]]\ntranspose\n [[1]\n [2]\n [3]]\n</pre> <p>Broadcasting is a powerful mechanism that allows numpy to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller array and a larger array, and we want to use the smaller array multiple times to perform some operation on the larger array.</p> <p>For example, suppose that we want to add a constant vector to each row of a matrix. We could do it like this:</p> In\u00a0[92]: Copied! <pre># We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = np.empty_like(x)   # Create an empty matrix with the same shape as x\n\n# Add the vector v to each row of the matrix x with an explicit loop\nfor i in range(4):\n    y[i, :] = x[i, :] + v\n\nprint(y)\n</pre> # We will add the vector v to each row of the matrix x, # storing the result in the matrix y x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = np.empty_like(x)   # Create an empty matrix with the same shape as x  # Add the vector v to each row of the matrix x with an explicit loop for i in range(4):     y[i, :] = x[i, :] + v  print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>This works; however when the matrix <code>x</code> is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the matrix <code>x</code> is equivalent to forming a matrix <code>vv</code> by stacking multiple copies of <code>v</code> vertically, then performing elementwise summation of <code>x</code> and <code>vv</code>. We could implement this approach like this:</p> In\u00a0[94]: Copied! <pre>vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\nprint(vv)                # Prints \"[[1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]\n                         #          [1 0 1]]\"\n</pre> vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other print(vv)                # Prints \"[[1 0 1]                          #          [1 0 1]                          #          [1 0 1]                          #          [1 0 1]]\" <pre>[[1 0 1]\n [1 0 1]\n [1 0 1]\n [1 0 1]]\n</pre> In\u00a0[95]: Copied! <pre>y = x + vv  # Add x and vv elementwise\nprint(y)\n</pre> y = x + vv  # Add x and vv elementwise print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>Numpy broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:</p> In\u00a0[96]: Copied! <pre>import numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  # Add v to each row of x using broadcasting\nprint(y)\n</pre> import numpy as np  # We will add the vector v to each row of the matrix x, # storing the result in the matrix y x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) v = np.array([1, 0, 1]) y = x + v  # Add v to each row of x using broadcasting print(y) <pre>[[ 2  2  4]\n [ 5  5  7]\n [ 8  8 10]\n [11 11 13]]\n</pre> <p>The line <code>y = x + v</code> works even though <code>x</code> has shape <code>(4, 3)</code> and <code>v</code> has shape <code>(3,)</code> due to broadcasting; this line works as if v actually had shape <code>(4, 3)</code>, where each row was a copy of <code>v</code>, and the sum was performed elementwise.</p> <p>Broadcasting two arrays together follows these rules:</p> <ol> <li>If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.</li> <li>The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.</li> <li>The arrays can be broadcast together if they are compatible in all dimensions.</li> <li>After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.</li> <li>In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension</li> </ol> <p>If this explanation does not make sense, try reading the explanation from the documentation or this explanation.</p> <p>Functions that support broadcasting are known as universal functions. You can find the list of all universal functions in the documentation.</p> <p>Here are some applications of broadcasting:</p> In\u00a0[97]: Copied! <pre># Compute outer product of vectors\nv = np.array([1,2,3])  # v has shape (3,)\nw = np.array([4,5])    # w has shape (2,)\n# To compute an outer product, we first reshape v to be a column\n# vector of shape (3, 1); we can then broadcast it against w to yield\n# an output of shape (3, 2), which is the outer product of v and w:\n\nprint(np.reshape(v, (3, 1)) * w)\n</pre> # Compute outer product of vectors v = np.array([1,2,3])  # v has shape (3,) w = np.array([4,5])    # w has shape (2,) # To compute an outer product, we first reshape v to be a column # vector of shape (3, 1); we can then broadcast it against w to yield # an output of shape (3, 2), which is the outer product of v and w:  print(np.reshape(v, (3, 1)) * w) <pre>[[ 4  5]\n [ 8 10]\n [12 15]]\n</pre> In\u00a0[98]: Copied! <pre># Add a vector to each row of a matrix\nx = np.array([[1,2,3], [4,5,6]])\n# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n# giving the following matrix:\n\nprint(x + v)\n</pre> # Add a vector to each row of a matrix x = np.array([[1,2,3], [4,5,6]]) # x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3), # giving the following matrix:  print(x + v) <pre>[[2 4 6]\n [5 7 9]]\n</pre> In\u00a0[100]: Copied! <pre># Add a vector to each column of a matrix\n# x has shape (2, 3) and w has shape (2,).\n# If we transpose x then it has shape (3, 2) and can be broadcast\n# against w to yield a result of shape (3, 2); transposing this result\n# yields the final result of shape (2, 3) which is the matrix x with\n# the vector w added to each column. Gives the following matrix:\n\nprint((x.T + w).T)\n</pre> # Add a vector to each column of a matrix # x has shape (2, 3) and w has shape (2,). # If we transpose x then it has shape (3, 2) and can be broadcast # against w to yield a result of shape (3, 2); transposing this result # yields the final result of shape (2, 3) which is the matrix x with # the vector w added to each column. Gives the following matrix:  print((x.T + w).T) <pre>[[ 5  6  7]\n [ 9 10 11]]\n</pre> In\u00a0[101]: Copied! <pre># Another solution is to reshape w to be a row vector of shape (2, 1);\n# we can then broadcast it directly against x to produce the same\n# output.\nprint(x + np.reshape(w, (2, 1)))\n</pre> # Another solution is to reshape w to be a row vector of shape (2, 1); # we can then broadcast it directly against x to produce the same # output. print(x + np.reshape(w, (2, 1))) <pre>[[ 5  6  7]\n [ 9 10 11]]\n</pre> In\u00a0[102]: Copied! <pre># Multiply a matrix by a constant:\n# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n# these can be broadcast together to shape (2, 3), producing the\n# following array:\nprint(x * 2)\n</pre> # Multiply a matrix by a constant: # x has shape (2, 3). Numpy treats scalars as arrays of shape (); # these can be broadcast together to shape (2, 3), producing the # following array: print(x * 2) <pre>[[ 2  4  6]\n [ 8 10 12]]\n</pre> <p>Broadcasting typically makes your code more concise and faster, so you should strive to use it where possible.</p> <p>This brief overview has touched on many of the important things that you need to know about numpy, but is far from complete. Check out the numpy reference to find out much more about numpy.</p> <p>Matplotlib is a plotting library. In this section give a brief introduction to the <code>matplotlib.pyplot</code> module, which provides a plotting system similar to that of MATLAB.</p> In\u00a0[0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt <p>By running this special iPython command, we will be displaying plots inline:</p> In\u00a0[0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>The most important function in <code>matplotlib</code> is plot, which allows you to plot 2D data. Here is a simple example:</p> In\u00a0[105]: Copied! <pre># Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\n</pre> # Compute the x and y coordinates for points on a sine curve x = np.arange(0, 3 * np.pi, 0.1) y = np.sin(x)  # Plot the points using matplotlib plt.plot(x, y) Out[105]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f0f3a0b4208&gt;]</pre> <p>With just a little bit of extra work we can easily plot multiple lines at once, and add a title, legend, and axis labels:</p> In\u00a0[106]: Copied! <pre>y_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\n</pre> y_sin = np.sin(x) y_cos = np.cos(x)  # Plot the points using matplotlib plt.plot(x, y_sin) plt.plot(x, y_cos) plt.xlabel('x axis label') plt.ylabel('y axis label') plt.title('Sine and Cosine') plt.legend(['Sine', 'Cosine']) Out[106]: <pre>&lt;matplotlib.legend.Legend at 0x7f0f39c04780&gt;</pre> <p>You can plot different things in the same figure using the subplot function. Here is an example:</p> In\u00a0[107]: Copied! <pre># Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n</pre> # Compute the x and y coordinates for points on sine and cosine curves x = np.arange(0, 3 * np.pi, 0.1) y_sin = np.sin(x) y_cos = np.cos(x)  # Set up a subplot grid that has height 2 and width 1, # and set the first such subplot as active. plt.subplot(2, 1, 1)  # Make the first plot plt.plot(x, y_sin) plt.title('Sine')  # Set the second subplot as active, and make the second plot. plt.subplot(2, 1, 2) plt.plot(x, y_cos) plt.title('Cosine')  # Show the figure. plt.show() <p>You can read much more about the <code>subplot</code> function in the documentation.</p> In\u00a0[0]: Copied! <pre>\n</pre>"},{"location":"python-colab/#cs231n-python-tutorial-with-google-colab","title":"CS231n Python Tutorial With Google Colab\u00b6","text":""},{"location":"python-colab/#introduction","title":"Introduction\u00b6","text":""},{"location":"python-colab/#a-brief-note-on-python-versions","title":"A Brief Note on Python Versions\u00b6","text":"<p>As of Janurary 1, 2020, Python has officially dropped support for <code>python2</code>. We'll be using Python 3.7 for this iteration of the course. You can check your Python version at the command line by running <code>python --version</code>. In Colab, we can enforce the Python version by clicking <code>Runtime -&gt; Change Runtime Type</code> and selecting <code>python3</code>. Note that as of April 2020, Colab uses Python 3.6.9 which should run everything without any errors.</p>"},{"location":"python-colab/#basics-of-python","title":"Basics of Python\u00b6","text":""},{"location":"python-colab/#basic-data-types","title":"Basic data types\u00b6","text":""},{"location":"python-colab/#numbers","title":"Numbers\u00b6","text":""},{"location":"python-colab/#booleans","title":"Booleans\u00b6","text":""},{"location":"python-colab/#strings","title":"Strings\u00b6","text":""},{"location":"python-colab/#containers","title":"Containers\u00b6","text":""},{"location":"python-colab/#lists","title":"Lists\u00b6","text":""},{"location":"python-colab/#slicing","title":"Slicing\u00b6","text":""},{"location":"python-colab/#loops","title":"Loops\u00b6","text":""},{"location":"python-colab/#list-comprehensions","title":"List comprehensions:\u00b6","text":""},{"location":"python-colab/#dictionaries","title":"Dictionaries\u00b6","text":""},{"location":"python-colab/#sets","title":"Sets\u00b6","text":""},{"location":"python-colab/#tuples","title":"Tuples\u00b6","text":""},{"location":"python-colab/#functions","title":"Functions\u00b6","text":""},{"location":"python-colab/#classes","title":"Classes\u00b6","text":""},{"location":"python-colab/#numpy","title":"Numpy\u00b6","text":""},{"location":"python-colab/#arrays","title":"Arrays\u00b6","text":""},{"location":"python-colab/#array-indexing","title":"Array indexing\u00b6","text":""},{"location":"python-colab/#datatypes","title":"Datatypes\u00b6","text":""},{"location":"python-colab/#array-math","title":"Array math\u00b6","text":""},{"location":"python-colab/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"python-colab/#matplotlib","title":"Matplotlib\u00b6","text":""},{"location":"python-colab/#plotting","title":"Plotting\u00b6","text":""},{"location":"python-colab/#subplots","title":"Subplots\u00b6","text":""},{"location":"rnn/","title":"Rnn","text":"<p>Table of Contents:</p> <ul> <li>Introduction to RNN</li> <li>RNN example as Character-level language model</li> <li>Multilayer RNNs</li> <li>Long-Short Term Memory (LSTM)</li> </ul> <p></p>"},{"location":"rnn/#introduction-to-rnn","title":"Introduction to RNN","text":"<p>In this lecture note, we're going to be talking about the Recurrent Neural Networks (RNNs). One great thing about the RNNs is that they offer a lot of flexibility on how we wire up the neural network architecture. Normally when we're working with neural networks (Figure 1), we are given a fixed sized input vector (red), then we process it with some hidden layers (green), and  we produce a fixed sized output vector (blue) as depicted in the leftmost model (\"Vanilla\" Neural Networks) in Figure 1. While \"Vanilla\" Neural Networks receive a single input and produce one label for that image, there are tasks where the model produce a sequence of outputs as shown in the one-to-many model in Figure 1. Recurrent Neural Networks allow  us to operate over sequences of input, output, or both at the same time.  * An example of one-to-many model is image captioning where we are given a fixed sized image and produce a sequence of words that describe the content of that image through RNN (second model in Figure 1). * An example of many-to-one task is action prediction where we look at a sequence of video frames instead of a single image and produce a label of what action was happening in the video as shown in the third model in Figure 1. Another example of many-to-one task is  sentiment classification in NLP where we are given a sequence of words of a sentence and then classify what sentiment (e.g. positive or negative) that sentence is. * An example of many-to-many task is video-captioning where the input is a sequence of video frames and the output is caption that describes what was in the video as shown in the fourth model in Figure 1. Another example of many-to-many task is machine translation in NLP, where we can have an RNN that takes a sequence of words of a sentence in English, and then this RNN is asked to produce a sequence of words of a sentence in French.  * There is a also a variation of many-to-many task as shown in the last model in Figure 1,  where the model generates an output at every timestep. An example of this many-to-many task is video classification on a frame level where the model classifies every single frame of video with some number of classes. We should note that we don't want  this prediction to only be a function of the current timestep (current frame of the video), but also all the timesteps (frames) that have come before this video. </p> <p>In general, RNNs allow us to wire up an architecture, where the prediction at every single timestep is a function of all the timesteps that have come before.</p>  Figure 1. Different (non-exhaustive) types of Recurrent Neural Network architectures. Red boxes are input vectors. Green boxes are hidden layers. Blue boxes are output vectors."},{"location":"rnn/#why-are-existing-convnets-insufficient","title":"Why are existing convnets insufficient?","text":"<p>The existing convnets are insufficient to deal with tasks that have inputs and outputs with variable sequence lengths.  In the example of video captioning, inputs have variable number of frames (e.g. 10-minute and 10-hour long video) and outputs are captions of variable length. Convnets can only take in inputs with a fixed size of width and height and cannot generalize over  inputs with different sizes. In order to tackle this problem, we introduce Recurrent Neural Networks (RNNs). </p>"},{"location":"rnn/#recurrent-neural-network","title":"Recurrent Neural Network","text":"<p>RNN is basically a blackbox (Left of Figure 2), where it has an \u201cinternal state\u201d that is updated as a sequence is processed. At every single timestep, we feed in an input vector into RNN where it modifies that state as a function of what it receives. When we tune RNN weights,  RNN will show different behaviors in terms of how its state evolves as it receives these inputs.  We are also interested in producing an output based on the RNN state, so we can produce these output vectors on top of the RNN (as depicted in Figure 2).</p> <p>If we unroll an RNN model (Right of Figure 2), then there are inputs (e.g. video frame) at different timesteps shown as \\(\\(x_1, x_2, x_3\\)\\) ... \\(\\(x_t\\)\\).  RNN at each timestep takes in two inputs -- an input frame (\\(\\(x_i\\)\\)) and previous representation of what it seems so far (i.e. history) -- to generate an output \\(\\(y_i\\)\\) and update its history, which will get forward propagated over time. All the RNN blocks in Figure 2 (Right) are the same block that share the same parameter, but have different inputs and history at each timestep.</p> Figure 2. Simplified RNN box (Left) and Unrolled RNN (Right). <p>More precisely, RNN can be represented as a recurrence formula of some function \\(\\(f_W\\)\\) with parameters \\(\\(W\\)\\):</p> \\[ h_t = f_W(h_{t-1}, x_t) \\] <p>where at every timestep it receives some previous state as a vector \\(\\(h_{t-1}\\)\\) of previous iteration timestep \\(\\(t-1\\)\\) and current input vector \\(\\(x_t\\)\\) to produce the current state as a vector \\(\\(h_t\\)\\). A fixed function \\(\\(f_W\\)\\) with weights \\(\\(W\\)\\) is applied at every single timestep and that allows us to use the Recurrent Neural Network on sequences without having to commit to the size of the sequence because we apply the exact same function at every single timestep, no matter how long the input or output sequences are.</p> <p>In the most simplest form of RNN, which we call a Vanilla RNN, the network is just a single hidden state \\(\\(h\\)\\) where we use a recurrence formula that basically tells us how we should update our hidden state \\(\\(h\\)\\) as a function of previous hidden state \\(\\(h_{t-1}\\)\\) and the current input \\(\\(x_t\\)\\). In particular, we're going to have weight matrices \\(\\(W_{hh}\\)\\) and \\(\\(W_{xh}\\)\\), where they will project both the hidden state \\(\\(h_{t-1}\\)\\) from the previous timestep and the current input \\(\\(x_t\\)\\), and then those are going to be summed and squished with \\(\\(tanh\\)\\) function to update the hidden state \\(\\(h_t\\)\\) at timestep \\(\\(t\\)\\). This recurrence is telling us how \\(\\(h\\)\\) will change as a function of its history and also the current input at this timestep:</p> \\[ h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t) \\] <p>We can base predictions on top of \\(\\(h_t\\)\\) by using just another matrix projection on top of the hidden state. This is the simplest complete case in which you can wire up a neural network:</p> \\[ y_t = W_{hy}h_t \\] <p>So far we have showed RNN in terms of abstract vectors \\(\\(x, h, y\\)\\), however we can endow these vectors with semantics in the following section.</p> <p></p>"},{"location":"rnn/#rnn-example-as-character-level-language-model","title":"RNN example as Character-level language model","text":"<p>One of the simplest ways in which we can use an RNN is in the case of a character-level language model since it's intuitive to understand. The way this RNN will work is we will feed a sequence of characters into the RNN and at every single timestep, we will ask the RNN to predict the next character in the sequence. The prediction of RNN will be in the form of score distribution of the characters in the vocabulary for what RNN thinks should come next in the sequence that it has seen so far.</p> <p>So suppose, in a very simple example (Figure 3), we have the training sequence of just one string \\(\\(\\text{\"hello\"}\\)\\), and we have a vocabulary \\(\\(V \\in \\{\\text{\"h\"}, \\text{\"e\"}, \\text{\"l\"}, \\text{\"o\"}\\}\\)\\) of 4 characters in the entire dataset. We are going to try to get an RNN to learn to predict the next character in the sequence on this training data.</p> Figure 3. Simplified Character-level Language Model RNN. <p>As shown in Figure 3, we'll feed in one character at a time into an RNN, first \\(\\(\\text{\"h\"}\\)\\), then \\(\\(\\text{\"e\"}\\)\\), then \\(\\(\\text{\"l\"}\\)\\), and finally \\(\\(\\text{\"l\"}\\)\\). All characters are encoded in the representation of what's called a one-hot vector, where only one unique bit of the vector is turned on for each unique character in the vocabulary. For example:</p> \\[ \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\text{\"h\"}\\ \\  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\text{\"e\"}\\ \\  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\text{\"l\"}\\ \\  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} = \\text{\"o\"} \\] <p>Then we're going to use the recurrence formula from the previous section at every single timestep. Suppose we start off with \\(\\(h\\)\\) as a vector of size 3 with all zeros. By using this fixed recurrence formula, we're going to end up with a 3-dimensional representation of the next hidden state \\(\\(h\\)\\) that basically at any point in time summarizes all the characters that have come until then:</p> \\[ \\begin{aligned} \\begin{bmatrix}0.3 \\\\ -0.1 \\\\ 0.9 \\end{bmatrix} &amp;= f_W(W_{hh}\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\end{bmatrix} + W_{xh}\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}) \\ \\ \\ \\ &amp;(1) \\\\ \\begin{bmatrix}1.0 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} &amp;= f_W(W_{hh}\\begin{bmatrix}0.3 \\\\ -0.1 \\\\ 0.9 \\end{bmatrix} + W_{xh}\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}) \\ \\ \\ \\ &amp;(2) \\\\ \\begin{bmatrix}0.1 \\\\ -0.5 \\\\ -0.3 \\end{bmatrix} &amp;= f_W(W_{hh}\\begin{bmatrix}1.0 \\\\ 0.3 \\\\ 0.1 \\end{bmatrix} + W_{xh}\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}) \\ \\ \\ \\ &amp;(3) \\\\ \\begin{bmatrix}-0.3 \\\\ 0.9 \\\\ 0.7 \\end{bmatrix} &amp;= f_W(W_{hh}\\begin{bmatrix}0.1 \\\\ -0.5 \\\\ -0.3 \\end{bmatrix} + W_{xh}\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}) \\ \\ \\ \\ &amp;(4) \\end{aligned} \\] <p>As we apply this recurrence at every timestep, we're going to predict what should be the next character in the sequence at every timestep. Since we have four characters in vocabulary \\(\\(V\\)\\), we're going to predict 4-dimensional vector of logits at every single timestep.</p> <p>As shown in Figure 3, in the very first timestep we fed in \\(\\(\\text{\"h\"}\\)\\), and the RNN with its current setting of weights computed a vector of logits:</p> \\[ \\begin{bmatrix}1.0 \\\\ 2.2 \\\\ -3.0 \\\\ 4.1 \\end{bmatrix} \\rightarrow \\begin{bmatrix}\\text{\"h\"} \\\\ \\text{\"e\"} \\\\ \\text{\"l\"}\\\\ \\text{\"o\"} \\end{bmatrix} \\] <p>where RNN thinks that the next character \\(\\(\\text{\"h\"}\\)\\) is \\(\\(1.0\\)\\) likely to come next, \\(\\(\\text{\"e\"}\\)\\) is \\(\\(2.2\\)\\) likely, \\(\\(\\text{\"e\"}\\)\\) is \\(\\(-3.0\\)\\) likely, and \\(\\(\\text{\"o\"}\\)\\) is \\(\\(4.1\\)\\) likely to come next. In this case, RNN incorrectly suggests that \\(\\(\\text{\"o\"}\\)\\) should come next, as the score of \\(\\(4.1\\)\\) is the highest. However, of course, we know that in this training sequence \\(\\(\\text{\"e\"}\\)\\) should follow \\(\\(\\text{\"h\"}\\)\\), so in fact the score of \\(\\(2.2\\)\\) is the correct answer as it's highlighted in green in Figure 3, and we want that to be high and all other scores to be low. At every single timestep we have a target for what next character should come in the sequence, therefore the error signal is backpropagated as a gradient of the loss function through the connections. As a loss function we could choose to have a softmax classifier, for example, so we just get all those losses flowing down from the top backwards to calculate the gradients on all the weight matrices to figure out how to shift the matrices so that the correct probabilities are coming out of the RNN. Similarly we can imagine how to scale up the training of the model over larger training dataset.</p> <p></p>"},{"location":"rnn/#multilayer-rnns","title":"Multilayer RNNs","text":"<p>So far we have only shown RNNs with just one layer. However, we're not limited to only a single layer architectures. One of the ways, RNNs are used today is in more complex manner. RNNs can be stacked together in multiple layers, which gives more depth, and empirically deeper architectures tend to work better (Figure 4).</p> Figure 4. Multilayer RNN example. <p>For example, in Figure 4, there are three separate RNNs each with their own set of weights. Three RNNs are stacked on top of each other, so the input of the second RNN (second RNN layer in Figure 4) is the vector of the hidden state vector of the first RNN (first RNN layer in Figure 4). All stacked RNNs are trained jointly, and the diagram in Figure 4 represents one computational graph.</p> <p></p>"},{"location":"rnn/#long-short-term-memory-lstm","title":"Long-Short Term Memory (LSTM)","text":"<p>So far we have seen only a simple recurrence formula for the Vanilla RNN. In practice, we actually will rarely ever use Vanilla RNN formula. Instead, we will use what we call a Long-Short Term Memory (LSTM) RNN.</p>"},{"location":"rnn/#vanilla-rnn-gradient-flow-vanishing-gradient-problem","title":"Vanilla RNN Gradient Flow &amp; Vanishing Gradient Problem","text":"<p>An RNN block takes in input \\(\\(x_t\\)\\) and previous hidden representation \\(\\(h_{t-1}\\)\\) and learn a transformation, which is then passed through tanh to produce the hidden representation \\(\\(h_{t}\\)\\) for the next time step and output \\(\\(y_{t}\\)\\) as shown in the equation below.</p> \\[ h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t) \\] <p>For the back propagation, Let's examine how the output at the very last timestep affects the weights at the very first time step. The partial derivative of \\(\\(h_t\\)\\) with respect to \\(\\(h_{t-1}\\)\\) is written as:  $$ \\frac{\\partial h_t}{\\partial h_{t-1}} =  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh} $$</p> <p>We update the weights \\(\\(W_{hh}\\)\\) by getting the derivative of the loss at the very last time step \\(\\(L_{t}\\)\\) with respect to \\(\\(W_{hh}\\)\\). </p> \\[ \\begin{aligned} \\frac{\\partial L_{t}}{\\partial W_{hh}} = \\frac{\\partial L_{t}}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial h_{t-1} } \\dots \\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\ = \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial  h_{t-1}})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\ = \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T}  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\ \\end{aligned} \\] <ul> <li> <p>Vanishing gradient: We see that \\(\\(tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)\\)\\) will almost always be less than 1 because tanh is always between negative one and one. Thus, as \\(\\(t\\)\\) gets larger (i.e. longer timesteps),  the gradient ($$\\frac{\\partial L_{t}}{\\partial W} $$) will descrease in value and get close to zero.  This will lead to vanishing gradient problem, where gradients at future time steps rarely impact gradients at the very first time step. This is problematic when we model long sequence of inputs because the updates will be extremely slow. </p> </li> <li> <p>Removing non-linearity (tanh): If we remove non-linearity (tanh) to solve the vanishing gradient problem, then we will be left with $$ \\begin{aligned} \\frac{\\partial L_{t}}{\\partial W} = \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T} W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W}  \\end{aligned} $$</p> <ul> <li>Exploding gradients: If the largest singular value of W_{hh} is greater than 1, then the gradients will blow up and the model will get very large gradients coming back from future time steps. Exploding gradient often leads to getting gradients that are NaNs. </li> <li>Vanishing gradients: If the laregest singular value of W_{hh} is smaller than 1, then we will have vanishing gradient problem as mentioned above which will significantly slow down learning.   </li> </ul> </li> </ul> <p>In practice, we can treat the exploding gradient problem through gradient clipping, which is clipping large gradient values to a maximum threshold. However, since vanishing gradient problem still exists in cases where largest singular value of W_{hh} matrix is less than one, LSTM was designed to avoid this problem. </p>"},{"location":"rnn/#lstm-formulation","title":"LSTM Formulation","text":"<p>The following is the precise formulation for LSTM. On step \\(\\(t\\)\\), there is a hidden state \\(\\(h_t\\)\\) and a cell state \\(\\(c_t\\)\\). Both \\(\\(h_t\\)\\) and \\(\\(c_t\\)\\) are vectors of size \\(\\(n\\)\\). One distinction of LSTM from Vanilla RNN is that LSTM has this additional \\(\\(c_t\\)\\) cell state, and intuitively it can be thought of as \\(\\(c_t\\)\\) stores long-term information. LSTM can read, erase, and write information to and from this \\(\\(c_t\\)\\) cell. The way LSTM alters \\(\\(c_t\\)\\) cell is through three special gates: \\(\\(i, f, o\\)\\) which correspond to \u201cinput\u201d, \u201cforget\u201d, and \u201coutput\u201d gates. The values of these gates vary from closed (0) to open (1). All \\(\\(i, f, o\\)\\) gates are vectors of size \\(\\(n\\)\\).</p> <p>At every timestep we have an input vector \\(\\(x_t\\)\\), previous hidden state \\(\\(h_{t-1}\\)\\), previous cell state \\(\\(c_{t-1}\\)\\), and LSTM computes the next hidden state \\(\\(h_t\\)\\), and next cell state \\(\\(c_t\\)\\) at timestep \\(\\(t\\)\\) as follows:</p> \\[ \\begin{aligned} f_t &amp;= \\sigma(W_{hf}h_{t_1} + W_{xf}x_t) \\\\ i_t &amp;= \\sigma(W_{hi}h_{t_1} + W_{xi}x_t) \\\\ o_t &amp;= \\sigma(W_{ho}h_{t_1} + W_{xo}x_t) \\\\ g_t &amp;= \\text{tanh}(W_{hg}h_{t_1} + W_{xg}x_t) \\\\ \\end{aligned} \\] \\[ \\begin{aligned} c_t &amp;= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\ h_t &amp;= o_t \\odot \\text{tanh}(c_t) \\\\ \\end{aligned} \\] <p>where \\(\\(\\odot\\)\\) is an element-wise Hadamard product. \\(\\(g_t\\)\\) in the above formulas is an intermediary calculation cache that's later used with \\(\\(o\\)\\) gate in the above formulas.</p> <p>Since all \\(\\(f, i, o\\)\\) gate vector values range from 0 to 1, because they were squashed by sigmoid function \\(\\(\\sigma\\)\\), when multiplied element-wise, we can see that:</p> <ul> <li>Forget Gate: Forget gate \\(\\(f_t\\)\\) at time step \\(\\(t\\)\\) controls how much information needs to be \"removed\" from the previous cell state \\(\\(c_{t-1}\\)\\).  This forget gate learns to erase hidden representations from the previous time steps, which is why LSTM will have two  hidden represtnations \\(\\(h_t\\)\\) and cell state \\(\\(c_t\\)\\). This \\(\\(c_t\\)\\) will get propagated over time and learn whether to forget the previous cell state or not.</li> <li>Input Gate: Input gate \\(\\(i_t\\)\\) at time step \\(\\(t\\)\\) controls how much information needs to be \"added\" to the next cell state \\(\\(c_t\\)\\) from previous hidden state \\(\\(h_{t-1}\\)\\) and input \\(\\(x_t\\)\\). Instead of tanh, the \"input\" gate \\(\\(i\\)\\) has a sigmoid function, which converts inputs to values between zero and one. This serves as a switch, where values are either almost always zero or almost always one. This \"input\" gate decides whether to take the RNN output that is produced by the \"gate\" gate \\(\\(g\\)\\) and multiplies the output with input gate \\(\\(i\\)\\).</li> <li>Output Gate: Output gate \\(\\(o_t\\)\\) at time step \\(\\(t\\)\\) controls how much information needs to be \"shown\" as output in the current hidden state \\(\\(h_t\\)\\). </li> </ul> <p>The key idea of LSTM is the cell state, the horizontal line running through between recurrent timesteps. You can imagine the cell state to be some kind of highway of information passing through straight down the entire chain, with only some minor linear interactions. With the formulation above, it's easy for information to just flow along this highway (Figure 5). Thus, even when there is a bunch of LSTMs stacked together, we can get an uninterrupted gradient flow where the gradients flow back through cell states instead of hidden states \\(\\(h\\)\\) without vanishing in every time step.</p> <p>This greatly fixes the gradient vanishing/exploding problem we have outlined above. Figure 5 also shows that gradient contains a vector of activations of the \"forget\" gate. This allows better control of gradients values by using suitable parameter updates of the \"forget\" gate.</p> Figure 5. LSTM cell state highway."},{"location":"rnn/#does-lstm-solve-the-vanishing-gradient-problem","title":"Does LSTM solve the vanishing gradient problem?","text":"<p>LSTM architecture makes it easier for the RNN to preserve information over many recurrent time steps. For example, if the forget gate is set to 1, and the input gate is set to 0, then the infomation of the cell state will always be preserved over many recurrent time steps. For a Vanilla RNN, in contrast, it's much harder to preserve information in hidden states in recurrent time steps by just making use of a single weight matrix.</p> <p>LSTMs do not guarantee that there is no vanishing/exploding gradient problems, but it does provide an easier way for the model to learn long-distance dependencies.</p>"},{"location":"terminal-tutorial/","title":"Terminal.com Tutorial","text":"<p>For the assignments, we offer an option to use Terminal for developing and testing your implementations. Notice that we're not using the main Terminal.com site but a subdomain which has been assigned specifically for this class. Terminal is an online computing platform that allows us to access pre-configured command line environments. Note that, it's not required to use Terminal for your assignments; however, it might make life easier with all the required dependencies and development toolkits configured for you.</p> <p>This tutorial lists the necessary steps of working on the assignments using Terminal. First of all, sign up your own account. Log in Terminal with the account that you have just created.</p> <p>For each assignment, we will provide you a link to a shared terminal snapshot. These snapshots are pre-configured command line environments with the starter code, where you can write your implementations and execute the code.</p> <p>Here's an example of what a snapshot page looked like for an assignment in 2015:</p> <p>Yours will look similar. Click the \"Start\" button on the lower right corner. This will clone the shared snapshot to your own account. Now you should be able to find the terminal under the My Terminals tab.</p> <p>Yours will look similar. You are all set! To work on the assignments, click the link to your terminal (shown in the red box in the above image). This link will open up the user interface layer over an AWS machine. It will look something similar to this:</p> <p>We have set up the Jupyter Notebook and other dependencies in the terminal. Launch a new console window with the small + sign (if you don't already have one), navigate around and look for the assignment folder and code. Launch a Jupyer notebook and work on the assignment. If your're a student enrolled in the class you will submit your assignment through Coursework:</p> <p>For more information about Terminal, check out the FAQ page.</p> <p>Important Note: the usage of Terminal is charged on an hourly rate based on the instance type. A medium type instance costs $0.124 per hour. If you are enrolled in the class email Serena Yeung (syyeung@cs.stanford.edu) to request Terminal credits. We will send you $3 the first time around, and you can request more funds on a rolling basis when you run out. Please be responsible with the funds we allocate you.</p>"},{"location":"transformers/","title":"Transformers","text":"<p>Table of Contents:</p> <ul> <li>Transformers Overview</li> <li>Why Transformers?</li> <li>Multi-Headed Attention</li> <li>Multi-Headed Attention Tips</li> <li>Transformer Steps: Encoder-Decoder</li> </ul> <p></p>"},{"location":"transformers/#transformer-overview","title":"Transformer Overview","text":"<p>In \"Attention Is All You Need\", Vaswani et al. introduced the Transformer, which introduces parallelism and enables models to learn long-range dependencies--thereby helping solve two key issues with RNNs: their slow speed of training and their difficulty in encoding long-range dependencies. Transformers are highly scalable and highly parallelizable, allowing for faster training, larger models, and better performance across vision and language tasks. Transformers are beginning to replace RNNs and LSTMs and may soon replace convolutions as well.</p> <p></p>"},{"location":"transformers/#why-transformers","title":"Why Transformers?","text":"<ul> <li>Transformers are great for working with long input sequences since the attention calculation looks at all inputs. In   contrast, RNNs struggle to encode long-range dependencies. LSTMs are much better at capturing long-range dependencies   by using the input, output, and forget gates.</li> <li>Transformers can operate over unordered sets or ordered sequences with positional encodings (using positional encoding   to add ordering the sets). In contrast, RNN/LSTM expect an ordered sequence of inputs.</li> <li>Transformers use parallel computation where all alignment and attention scores for all inputs can be done in parallel.   In contrast, RNN/LSTM uses sequential computation since the hidden state at a current timestep can only be computed   after the previous states are calculated which makes them often slow to train.</li> </ul>"},{"location":"transformers/#multi-headed-attention","title":"Multi-Headed Attention","text":"<p>Let\u2019s refresh our concepts from the attention unit to help us with transformers. </p> <ul> <li>Dot-Product Attention:</li> </ul> Dot-Product Attention <p> With query q (D,), value vectors {v_1,...,v_n} where v_i (D,), key vectors {k_1,...,k_n} where k_i (D,), attention weights a_i, and output c (D,). The output is a weighted average over the value vectors.</p> <ul> <li>Self-Attention: we derive values, keys, and queries from the input</li> </ul> Value, Key, and Query <p> Combining the above two, we can now implement multi-headed scaled dot product attention for transformers. </p> <ul> <li>Multi-Headed Scaled Dot Product Attention: We learn a parameter matrix V_i, K_i, Q_i (DxD) for each head i, which   increases the model\u2019s expressivity to attend to different parts of the input. We apply a scaling term (1/sqrt(d/h)) to   the dot-product attention described previously in order to reduce the effect of large magnitude vectors.</li> </ul> Multi-Headed Scaled Dot Product Attention <p> We can then apply dropout, generate the output of the attention layer, and finally add a linear transformation to the output of the attention operation, which allows the model to learn the relationship between heads, thereby improving the model\u2019s expressivity.</p> <p></p>"},{"location":"transformers/#step-by-step-multi-headed-attention-with-intermediate-dimensions","title":"Step-by-Step Multi-Headed Attention with Intermediate Dimensions","text":"<p>There's a lot happening throughout the Multi-Headed Attention so hopefully this chart will help further clarify the intermediate steps and how the dimensions change after each step!</p> Step-by-Step Multi-Headed Attention with Intermediate Dimensions"},{"location":"transformers/#a-couple-tips-on-permute-and-reshape","title":"A couple tips on Permute and Reshape:","text":"<p>To create the multiple heads, we divide the embedding dimension by the number of heads and use Reshape (Ex: Reshape allows us to go from shape (N x S x D) to (N x S x H x D//H) ). It is important to note that Reshape doesn\u2019t change the ordering of your data. It simply takes the original data and \u2018reshapes\u2019 it into the dimensions you provide. We use Permute (or can use Transpose) to rearrange the ordering of dimensions of the data (Ex: Permute allows us to rearrange the dimensions from (N x S x H x D//H) to (N x H x S x D//H) ). Notice why we needed to use Permute before Reshaping after the final MatMul operation. Our current tensor had a shape of (N x H x S x D//H) but in order to reshape it to be (N x S x D) we needed to first ensure that the H and D//H dimensions are right next to each other because reshape doesn\u2019t change the ordering of the data. Therefore we use Permute first to rearrange the dimensions from (N x H x S x D//H) to (N x S x H x D//H) and then can use reshape to get the shape of  (N x S x D).</p> <p></p>"},{"location":"transformers/#transformer-steps-encoder-decoder","title":"Transformer Steps: Encoder-Decoder","text":""},{"location":"transformers/#encoder-block","title":"Encoder Block","text":"<p>The role of the Encoder block is to encode all the image features (where the spatial features are extracted using pretrained CNN) into a set of context vectors. The context vectors outputted are a representation of the input sequence in a higher dimensional space. We define the Encoder as c = T_W(z) where z is the spatial CNN features and T_w(.) is the transformer encoder. In the \"Attention Is All You Need\" paper a transformer encoder block made up of N encoder blocks (N = 6, D = 512) is used.</p> Encoder Block <p></p> <p>Let\u2019s walk through the steps of the Encoder block!</p> <ul> <li>We first take in a set of input vectors X (where each input vector represents a word for instance)</li> <li>We then add positional encoding to the input vectors.</li> <li>We pass the positional encoded vectors through the Multi-head self-attention layer (where each vector attends on   all the other vectors). The output of this layer gives us a set of context vectors.</li> <li>We have a Residual Connection after the Multi-head self-attention layer which allows us to bypass the attention layer   if it\u2019s not needed.</li> <li>We then apply Layer Normalization on the output which normalizes each individual vector.</li> <li>We then apply MLP over each vector individually.</li> <li>We then have another Residual Connection.</li> <li>A final Layer Normalization on the output.</li> <li>And finally the set of context vectors C is outputted!</li> </ul>"},{"location":"transformers/#decoder-block","title":"Decoder Block","text":"<p>The Decoder block takes in the set of context vectors C outputted from the encoder block and set of input vectors X and outputs a set of vectors Y which defines the output sequence. We define the Decoder as y_t = T_S(y_{0:t-1},c) where T_D( .) is the transformer decoder. In the\"Attention Is All You Need\" paper a transformer decoder block made up of N decoder blocks (N = 6, D = 512) is used.</p> Decoder Block <p></p> <p>Let\u2019s walk through the steps of the Decoder block!</p> <ul> <li>We take in the set of input vectors X and context vectors C (outputted from Encoder block)</li> <li>We then add positional encoding to the input vectors X.</li> <li>We pass the positional encoded vectors through the Masked Multi-head self-attention layer. The mask ensures that   we only attend over previous inputs.</li> <li>We have a Residual Connection after this layer which allows us to bypass the attention layer if it\u2019s not needed.</li> <li>We then apply Layer Normalization on the output which normalizes each individual vector.</li> <li>Then we pass the output through another Multi-head attention layer which takes in the context vectors outputted by   the Encoder block as well as the output of the Layer Normalization. In this step the Key comes from the set of context   vectors C, the Value comes from the set of context vectors C, and the Query comes from the output of the Layer   Normalization step.</li> <li>We then have another Residual Connection.</li> <li>Apply another Layer Normalization.</li> <li>Apply MLP over each vector individually.</li> <li>Another Residual Connection</li> <li>A final Layer Normalization</li> <li>And finally we pass the output through a Fully-connected layer which produces the final set of output vectors Y which   is the output sequence.</li> </ul>"},{"location":"transformers/#additional-notes-on-layer-normalization-and-mlp","title":"Additional Notes on Layer Normalization and MLP","text":"<p>Layer Normalization: As seen in the Encoder and Decoder block implementation, we use Layer Normalization after the Residual Connections in both the Encoder and Decoder Blocks. Recall that in Layer Normalization we are normalizing across the feature dimension (so we are applying LayerNorm over the image features). Using Layer Normalization at these points helps us prevent issues with vanishing or exploding gradients, helps stabilize the network, and can reduce training time.</p> <p>MLP: Both the encoder and decoder blocks contain position-wise fully-connected feed-forward networks, which are \u201capplied to each position separately and identically\u201d (Vaswani et al.). The linear transformations use different parameters across layers. FFN(x) = max(0, xW_1 + b_1)W_2 + b_2. Additionally, the combination of a self-attention layer and a point-wise feed-forward layer reduces the complexity required by convolutional layers.</p>"},{"location":"transformers/#additional-resources","title":"Additional Resources","text":"<p>Additional resources related to implementation:</p> <ul> <li>\"Attention Is All You Need\"</li> </ul>"},{"location":"assignments/2023/assignment1/","title":"Assignment 1","text":"<p>This assignment is due on Friday, April 21 2023 at 11:59pm PST.</p> <p>Starter code containing Colab notebooks can be downloaded here.</p> <ul> <li>Setup</li> <li>Goals</li> <li>Q1: k-Nearest Neighbor classifier</li> <li>Q2: Training a Support Vector Machine</li> <li>Q3: Implement a Softmax classifier</li> <li>Q4: Two-Layer Neural Network</li> <li>Q5: Higher Level Representations: Image Features</li> <li>Submitting your work</li> </ul>"},{"location":"assignments/2023/assignment1/#setup","title":"Setup","text":"<p>Please familiarize yourself with the recommended workflow before starting the assignment. You should also watch the Colab walkthrough tutorial below.</p> <p>Note. Ensure you are periodically saving your notebook (<code>File -&gt; Save</code>) so that you don't lose your progress if you step away from the assignment and the Colab VM disconnects.</p> <p>Once you have completed all Colab notebooks except <code>collect_submission.ipynb</code>, proceed to the submission instructions.</p>"},{"location":"assignments/2023/assignment1/#goals","title":"Goals","text":"<p>In this assignment you will practice putting together a simple image classification pipeline based on the k-Nearest Neighbor or the SVM/Softmax classifier. The goals of this assignment are as follows:</p> <ul> <li>Understand the basic Image Classification pipeline and the data-driven approach (train/predict stages).</li> <li>Understand the train/val/test splits and the use of validation data for hyperparameter tuning.</li> <li>Develop proficiency in writing efficient vectorized code with numpy.</li> <li>Implement and apply a k-Nearest Neighbor (kNN) classifier.</li> <li>Implement and apply a Multiclass Support Vector Machine (SVM) classifier.</li> <li>Implement and apply a Softmax classifier.</li> <li>Implement and apply a Two layer neural network classifier.</li> <li>Understand the differences and tradeoffs between these classifiers.</li> <li>Get a basic understanding of performance improvements from using higher-level representations as opposed to raw pixels, e.g. color histograms, Histogram of Oriented Gradient (HOG) features, etc.</li> </ul>"},{"location":"assignments/2023/assignment1/#q1-k-nearest-neighbor-classifier","title":"Q1: k-Nearest Neighbor classifier","text":"<p>The notebook knn.ipynb will walk you through implementing the kNN classifier.</p>"},{"location":"assignments/2023/assignment1/#q2-training-a-support-vector-machine","title":"Q2: Training a Support Vector Machine","text":"<p>The notebook svm.ipynb will walk you through implementing the SVM classifier.</p>"},{"location":"assignments/2023/assignment1/#q3-implement-a-softmax-classifier","title":"Q3: Implement a Softmax classifier","text":"<p>The notebook softmax.ipynb will walk you through implementing the Softmax classifier.</p>"},{"location":"assignments/2023/assignment1/#q4-two-layer-neural-network","title":"Q4: Two-Layer Neural Network","text":"<p>The notebook two_layer_net.ipynb will walk you through the implementation of a two-layer neural network classifier.</p>"},{"location":"assignments/2023/assignment1/#q5-higher-level-representations-image-features","title":"Q5: Higher Level Representations: Image Features","text":"<p>The notebook features.ipynb will examine the improvements gained by using higher-level representations as opposed to using raw pixel values.</p>"},{"location":"assignments/2023/assignment1/#submitting-your-work","title":"Submitting your work","text":"<p>Important. Please make sure that the submitted notebooks have been run and the cell outputs are visible.</p> <p>Once you have completed all notebooks and filled out the necessary code, you need to follow the below instructions to submit your work:</p> <p>1. Open <code>collect_submission.ipynb</code> in Colab and execute the notebook cells.</p> <p>This notebook/script will:</p> <ul> <li>Generate a zip file of your code (<code>.py</code> and <code>.ipynb</code>) called <code>a1_code_submission.zip</code>.</li> <li>Convert all notebooks into a single PDF file.</li> </ul> <p>If your submission for this step was successful, you should see the following display message:</p> <p><code>### Done! Please submit a1_code_submission.zip and a1_inline_submission.pdf to Gradescope. ###</code></p> <p>2. Submit the PDF and the zip file to Gradescope.</p> <p>Remember to download <code>a1_code_submission.zip</code> and <code>a1_inline_submission.pdf</code> locally before submitting to Gradescope.</p>"},{"location":"assignments/2023/assignment2/","title":"Assignment 2","text":"<p>This assignment is due on Monday, May 08 2023 at 11:59pm PST.</p> <p>Starter code containing Colab notebooks can be downloaded here.</p> <ul> <li>Setup</li> <li>Goals</li> <li>Q1: Multi-Layer Fully Connected Neural Networks</li> <li>Q2: Batch Normalization</li> <li>Q3: Dropout</li> <li>Q4: Convolutional Neural Networks</li> <li>Q5: PyTorch on CIFAR-10</li> <li>Submitting your work</li> </ul>"},{"location":"assignments/2023/assignment2/#setup","title":"Setup","text":"<p>Please familiarize yourself with the recommended workflow before starting the assignment. You should also watch the Colab walkthrough tutorial below.</p> <p>Note. Ensure you are periodically saving your notebook (<code>File -&gt; Save</code>) so that you don't lose your progress if you step away from the assignment and the Colab VM disconnects.</p> <p>While we don't officially support local development, we've added a requirements.txt file that you can use to setup a virtual env.</p> <p>Once you have completed all Colab notebooks except <code>collect_submission.ipynb</code>, proceed to the submission instructions.</p>"},{"location":"assignments/2023/assignment2/#goals","title":"Goals","text":"<p>In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows:</p> <ul> <li>Understand Neural Networks and how they are arranged in layered architectures.</li> <li>Understand and be able to implement (vectorized) backpropagation.</li> <li>Implement various update rules used to optimize Neural Networks.</li> <li>Implement Batch Normalization and Layer Normalization for training deep networks.</li> <li>Implement Dropout to regularize networks.</li> <li>Understand the architecture of Convolutional Neural Networks and get practice with training them.</li> <li>Gain experience with a major deep learning framework, PyTorch.</li> </ul>"},{"location":"assignments/2023/assignment2/#q1-multi-layer-fully-connected-neural-networks","title":"Q1: Multi-Layer Fully Connected Neural Networks","text":"<p>The notebook <code>FullyConnectedNets.ipynb</code> will have you implement fully connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.</p>"},{"location":"assignments/2023/assignment2/#q2-batch-normalization","title":"Q2: Batch Normalization","text":"<p>In notebook <code>BatchNormalization.ipynb</code> you will implement batch normalization, and use it to train deep fully connected networks.</p>"},{"location":"assignments/2023/assignment2/#q3-dropout","title":"Q3: Dropout","text":"<p>The notebook <code>Dropout.ipynb</code> will help you implement dropout and explore its effects on model generalization.</p>"},{"location":"assignments/2023/assignment2/#q4-convolutional-neural-networks","title":"Q4: Convolutional Neural Networks","text":"<p>In the notebook <code>ConvolutionalNetworks.ipynb</code> you will implement several new layers that are commonly used in convolutional networks.</p>"},{"location":"assignments/2023/assignment2/#q5-pytorch-on-cifar-10","title":"Q5: PyTorch on CIFAR-10","text":"<p>For this part, you will be working with PyTorch, a popular and powerful deep learning framework.</p> <p>Open up <code>PyTorch.ipynb</code>. There, you will learn how the framework works, culminating in training a convolutional network of your own design on CIFAR-10 to get the best performance you can.</p>"},{"location":"assignments/2023/assignment2/#submitting-your-work","title":"Submitting your work","text":"<p>Important. Please make sure that the submitted notebooks have been run and the cell outputs are visible.</p> <p>Once you have completed all notebooks and filled out the necessary code, you need to follow the below instructions to submit your work:</p> <p>1. Open <code>collect_submission.ipynb</code> in Colab and execute the notebook cells.</p> <p>This notebook/script will:</p> <ul> <li>Generate a zip file of your code (<code>.py</code> and <code>.ipynb</code>) called <code>a2_code_submission.zip</code>.</li> <li>Convert all notebooks into a single PDF file.</li> </ul> <p>If your submission for this step was successful, you should see the following display message:</p> <p><code>### Done! Please submit a2_code_submission.zip and a2_inline_submission.pdf to Gradescope. ###</code></p> <p>2. Submit the PDF and the zip file to Gradescope.</p> <p>Remember to download <code>a2_code_submission.zip</code> and <code>a2_inline_submission.pdf</code> locally before submitting to Gradescope.</p>"},{"location":"assignments/2023/assignment3/","title":"Assignment 3","text":"<p>This assignment is due on Tuesday, May 30 2023 at 11:59pm PST.</p> <p>Starter code containing Colab notebooks can be downloaded here.</p> <ul> <li>Setup</li> <li>Goals</li> <li>Q1: Network Visualization: Saliency Maps, Class Visualization, and Fooling Images</li> <li>Q2: Image Captioning with Vanilla RNNs</li> <li>Q3: Image Captioning with Transformers</li> <li>Q4: Generative Adversarial Networks</li> <li>Q5: Self-Supervised Learning for Image Classification</li> <li>Extra Credit: Image Captioning with LSTMs</li> <li>Submitting your work</li> </ul>"},{"location":"assignments/2023/assignment3/#setup","title":"Setup","text":"<p>Please familiarize yourself with the recommended workflow before starting the assignment. You should also watch the Colab walkthrough tutorial below.</p> <p>Note. Ensure you are periodically saving your notebook (<code>File -&gt; Save</code>) so that you don't lose your progress if you step away from the assignment and the Colab VM disconnects.</p> <p>While we don't officially support local development, we've added a requirements.txt file that you can use to setup a virtual env.</p> <p>Once you have completed all Colab notebooks except <code>collect_submission.ipynb</code>, proceed to the submission instructions.</p>"},{"location":"assignments/2023/assignment3/#goals","title":"Goals","text":"<p>In this assignment, you will implement language networks and apply them to image captioning on the COCO dataset. Then you will train a Generative Adversarial Network to generate images that look like a training dataset. Finally, you will be introduced to self-supervised learning to automatically learn the visual representations of an unlabeled dataset.</p> <p>The goals of this assignment are as follows:</p> <ul> <li>Understand and implement RNN and Transformer networks. Combine them with CNN networks for image captioning.</li> <li>Understand how to train and implement a Generative Adversarial Network (GAN) to produce images that resemble samples from a dataset.</li> <li>Understand how to leverage self-supervised learning techniques to help with image classification tasks.</li> </ul> <p>You will use PyTorch for the majority of this homework.</p>"},{"location":"assignments/2023/assignment3/#q1-network-visualization-saliency-maps-class-visualization-and-fooling-images","title":"Q1: Network Visualization: Saliency Maps, Class Visualization, and Fooling Images","text":"<p>The notebook <code>Network_Visualization.ipynb</code> will introduce the pretrained SqueezeNet model, compute gradients with respect to images, and use them to produce saliency maps and fooling images.</p>"},{"location":"assignments/2023/assignment3/#q2-image-captioning-with-vanilla-rnns","title":"Q2: Image Captioning with Vanilla RNNs","text":"<p>The notebook <code>RNN_Captioning.ipynb</code> will walk you through the implementation of vanilla recurrent neural networks and apply them to image captioning on COCO.</p>"},{"location":"assignments/2023/assignment3/#q3-image-captioning-with-transformers","title":"Q3: Image Captioning with Transformers","text":"<p>The notebook <code>Transformer_Captioning.ipynb</code> will walk you through the implementation of a Transformer model and apply it to image captioning on COCO.</p>"},{"location":"assignments/2023/assignment3/#q4-generative-adversarial-networks","title":"Q4: Generative Adversarial Networks","text":"<p>In the notebook <code>Generative_Adversarial_Networks.ipynb</code> you will learn how to generate images that match a training dataset and use these models to improve classifier performance when training on a large amount of unlabeled data and a small amount of labeled data. When first opening the notebook, go to <code>Runtime &gt; Change runtime type</code> and set <code>Hardware accelerator</code> to <code>GPU</code>.</p>"},{"location":"assignments/2023/assignment3/#q5-self-supervised-learning-for-image-classification","title":"Q5: Self-Supervised Learning for Image Classification","text":"<p>In the notebook <code>Self_Supervised_Learning.ipynb</code>, you will learn how to leverage self-supervised pretraining to obtain better performance on image classification tasks. When first opening the notebook, go to <code>Runtime &gt; Change runtime type</code> and set <code>Hardware accelerator</code> to <code>GPU</code>.</p>"},{"location":"assignments/2023/assignment3/#extra-credit-image-captioning-with-lstms","title":"Extra Credit: Image Captioning with LSTMs","text":"<p>The notebook <code>LSTM_Captioning.ipynb</code> will walk you through the implementation of Long-Short Term Memory (LSTM) RNNs and apply them to image captioning on COCO.</p>"},{"location":"assignments/2023/assignment3/#submitting-your-work","title":"Submitting your work","text":"<p>Important. Please make sure that the submitted notebooks have been run and the cell outputs are visible.</p> <p>Once you have completed all notebooks and filled out the necessary code, you need to follow the below instructions to submit your work:</p> <p>1. Open <code>collect_submission.ipynb</code> in Colab and execute the notebook cells.</p> <p>This notebook/script will:</p> <ul> <li>Generate a zip file of your code (<code>.py</code> and <code>.ipynb</code>) called <code>a3_code_submission.zip</code>.</li> <li>Convert all notebooks into a single PDF file called <code>a3_inline_submission.pdf</code>.</li> </ul> <p>If your submission for this step was successful, you should see the following display message:</p> <p><code>### Done! Please submit a3_code_submission.zip and a3_inline_submission.pdf to Gradescope. ###</code></p> <p>2. Submit the PDF and the zip file to Gradescope.</p> <p>Remember to download <code>a3_code_submission.zip</code> and <code>a3_inline_submission.pdf</code> locally before submitting to Gradescope.</p>"},{"location":"courses/CNN/convolutional-networks/","title":"\u7ed3\u6784\u200b\u3001\u200b\u5377\u79ef\u200b/\u200b\u6c60\u5316\u5c42","text":"<p>Table of Contents:</p> <ul> <li>Architecture Overview</li> <li>ConvNet Layers</li> <li>Convolutional Layer</li> <li>Pooling Layer</li> <li>Normalization Layer</li> <li>Fully-Connected Layer</li> <li>Converting Fully-Connected Layers to Convolutional Layers</li> <li>ConvNet Architectures</li> <li>Layer Patterns</li> <li>Layer Sizing Patterns</li> <li>Case Studies (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)</li> <li>Computational Considerations</li> <li>Additional References</li> </ul>"},{"location":"courses/CNN/convolutional-networks/#convolutional-neural-networks-cnns-convnets","title":"Convolutional Neural Networks (CNNs / ConvNets)","text":"<p>Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.</p> <p>So what changes? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#architecture-overview","title":"Architecture Overview","text":"<p>Recall: Regular Neural Nets. As we saw in the previous chapter, Neural Networks receive an input (a single vector), and transform it through a series of hidden layers. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the \"output layer\" and in classification settings it represents the class scores.</p> <p>Regular Neural Nets don't scale well to full images. In CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.</p> <p>3D volumes of neurons. Convolutional Neural Networks take advantage of the fact that the input consists of images and they constrain the architecture in a more sensible way. In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions 32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by the end of the ConvNet architecture we will reduce the full image into a single vector of class scores, arranged along the depth dimension. Here is a visualization:</p> Left: A regular 3-layer Neural Network. Right: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels). <p>A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that may or may not have parameters.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#layers-used-to-build-convnets","title":"Layers used to build ConvNets","text":"<p>As we described above, a simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet architecture. </p> <p>Example Architecture: Overview. We will go into more details below, but a simple ConvNet for CIFAR-10 classification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:</p> <ul> <li>INPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li> <li>CONV layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.</li> <li>RELU layer will apply an elementwise activation function, such as the \\(max(0,x)\\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).</li> <li>POOL layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].</li> <li>FC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layer will be connected to all the numbers in the previous volume.</li> </ul> <p>In this way, ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note that some layers contain parameters and other don't. In particular, the CONV/FC layers perform transformations that are a function of not only the activations in the input volume, but also of the parameters (the weights and biases of the neurons). On the other hand, the RELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image. </p> <p>In summary:</p> <ul> <li>A ConvNet architecture is in the simplest case a list of Layers that transform the image volume into an output volume (e.g. holding the class scores)</li> <li>There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)</li> <li>Each Layer accepts an input 3D volume and transforms it to an output 3D volume through a differentiable function</li> <li>Each Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don't)</li> <li>Each Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesn't)</li> </ul>      The activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and the last volume stores the class scores (right). Each volume of activations along the processing path is shown as a column. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volume holds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one. The full web-based demo is shown in the header of our website. The architecture shown here is a tiny VGG Net, which we will discuss later.    <p>We now describe the individual layers and the details of their hyperparameters and their connectivities.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#convolutional-layer","title":"Convolutional Layer","text":"<p>The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting. </p> <p>Overview and intuition without brain stuff. Let's first discuss what the CONV layer computes without brain/neuron analogies. The CONV layer's parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of the network. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.</p> <p>The brain view. If you're a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter). </p> <p>We now discuss the details of the neuron connectivities, their arrangement in space, and their parameter sharing scheme.</p> <p>Local Connectivity. When dealing with high-dimensional inputs such as images, as we saw above it is impractical to connect neurons to all neurons in the previous volume. Instead, we will connect each neuron to only a local region of the input volume. The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size). The extent of the connectivity along the depth axis is always equal to the depth of the input volume. It is important to emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and the depth dimension: The connections are local in 2D space (along width and height), but always full along the entire depth of the input volume.</p> <p>Example 1. For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.</p> <p>Example 2. Suppose an input volume had size [16x16x20]. Then using an example receptive field size of 3x3, every neuron in the Conv Layer would now have a total of 3*3*20 = 180 connections to the input volume. Notice that, again, the connectivity is local in 2D space (e.g. 3x3), but full along the input depth (20).</p> Left: An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input: the lines that connect this column of 5 neurons do not represent the weights (i.e. these 5 neurons do not share the same weights, but they are associated with 5 different filters), they just indicate that these neurons are connected to or looking at the same receptive field or region of the input volume, i.e. they share the same receptive field but not the same weights. Right: The neurons from the Neural Network chapter remain unchanged: They still compute a dot product of their weights with the input followed by a non-linearity, but their connectivity is now restricted to be local spatially.    <p>Spatial arrangement. We have explained the connectivity of each neuron in the Conv Layer to the input volume, but we haven't yet discussed how many neurons there are in the output volume or how they are arranged. Three hyperparameters control the size of the output volume: the depth, stride and zero-padding. We discuss these next:</p> <ol> <li>First, the depth of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a depth column (some people also prefer the term fibre).</li> <li>Second, we must specify the stride with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.</li> <li>As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes (most commonly as we'll see soon we will use it to exactly preserve the spatial size of the input volume so the input and output width and height are the same).</li> </ol> <p>We can compute the spatial size of the output volume as a function of the input volume size (\\(W\\)), the receptive field size of the Conv Layer neurons (\\(F\\)), the stride with which they are applied (\\(S\\)), and the amount of zero padding used (\\(P\\)) on the border. You can convince yourself that the correct formula for calculating how many neurons \"fit\" is given by \\((W - F + 2P)/S + 1\\). For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output. Lets also see one more graphical example:</p>      Illustration of spatial arrangement. In this example there is only one spatial dimension (x-axis), one neuron with a receptive field size of F = 3, the input size is W = 5, and there is zero padding of P = 1. Left: The neuron strided across the input in stride of S = 1, giving output of size (5 - 3 + 2)/1+1 = 5. Right: The neuron uses stride of S = 2, giving output of size (5 - 3 + 2)/2+1 = 3. Notice that stride S = 3 could not be used since it wouldn't fit neatly across the volume. In terms of the equation, this can be determined since (5 - 3 + 2) = 4 is not divisible by 3.      The neuron weights are in this example [1,0,-1] (shown on very right), and its bias is zero. These weights are shared across all yellow neurons (see parameter sharing below).    <p>Use of zero-padding. In the example above on left, note that the input dimension was 5 and the output dimension was equal: also 5. This worked out so because our receptive fields were 3 and we used zero padding of 1. If there was no zero-padding used, then the output volume would have had spatial dimension of only 3, because that is how many neurons would have \"fit\" across the original input. In general, setting zero padding to be \\(P = (F - 1)/2\\) when the stride is \\(S = 1\\) ensures that the input volume and output volume will have the same size spatially. It is very common to use zero-padding in this way and we will discuss the full reasons when we talk more about ConvNet architectures.</p> <p>Constraints on strides. Note again that the spatial arrangement hyperparameters have mutual constraints. For example, when the input has size \\(W = 10\\), no zero-padding is used \\(P = 0\\), and the filter size is \\(F = 3\\), then it would be impossible to use stride \\(S = 2\\), since \\((W - F + 2P)/S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5\\), i.e. not an integer, indicating that the neurons don't \"fit\" neatly and symmetrically across the input. Therefore, this setting of the hyperparameters is considered to be invalid, and a ConvNet library could throw an exception or zero pad the rest to make it fit, or crop the input to make it fit, or something. As we will see in the ConvNet architectures section, sizing the ConvNets appropriately so that all the dimensions \"work out\" can be a real headache, which the use of zero-padding and some design guidelines will significantly alleviate.</p> <p>Real-world example. The Krizhevsky et al. architecture that won the ImageNet challenge in 2012 accepted images of size [227x227x3]. On the first Convolutional Layer, it used neurons with receptive field size \\(F = 11\\), stride \\(S = 4\\) and no zero padding \\(P = 0\\). Since (227 - 11)/4 + 1 = 55, and since the Conv layer had a depth of \\(K = 96\\), the Conv layer output volume had size [55x55x96]. Each of the 55*55*96 neurons in this volume was connected to a region of size [11x11x3] in the input volume. Moreover, all 96 neurons in each depth column are connected to the same [11x11x3] region of the input, but of course with different weights. As a fun aside, if you read the actual paper it claims that the input images were 224x224, which is surely incorrect because (224 - 11)/4 + 1 is quite clearly not an integer. This has confused many people in the history of ConvNets and little is known about what happened. My own best guess is that Alex used zero-padding of 3 extra pixels that he does not mention in the paper.</p> <p>Parameter Sharing. Parameter sharing scheme is used in Convolutional Layers to control the number of parameters. Using the real-world example above, we see that there are 55*55*96 = 290,400 neurons in the first Conv Layer, and each has 11*11*3 = 363 weights and 1 bias. Together, this adds up to 290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number is very high.</p> <p>It turns out that we can dramatically reduce the number of parameters by making one reasonable assumption: That if one feature is useful to compute at some spatial position (x,y), then it should also be useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensional slice of depth as a depth slice (e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias. With this parameter sharing scheme, the first Conv Layer in our example would now have only 96 unique set of weights (one for each depth slice), for a total of 96*11*11*3 = 34,848 unique weights, or 34,944 parameters (+96 biases). Alternatively, all 55*55 neurons in each depth slice will now be using the same parameters. In practice during backpropagation, every neuron in the volume will compute the gradient for its weights, but these gradients will be added up across each depth slice and only update a single set of weights per slice.</p> <p>Notice that if all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a convolution of the neuron's weights with the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to the sets of weights as a filter (or a kernel), that is convolved with the input.</p>      Example filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is shared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relatively reasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful at some other location as well due to the translationally-invariant structure of images. There is therefore no need to relearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.    <p>Note that sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a ConvNet have some specific centered structure, where we should expect, for example, that completely different features should be learned on one side of the image than another. One practical example is when the input are faces that have been centered in the image. You might expect that different eye-specific or hair-specific features could (and should) be learned in different spatial locations. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a Locally-Connected Layer.</p> <p>Numpy examples. To make the discussion above more concrete, lets express the same ideas but in code and with a specific example. Suppose that the input volume is a numpy array <code>X</code>. Then:</p> <ul> <li>A depth column (or a fibre) at position <code>(x,y)</code> would be the activations <code>X[x,y,:]</code>.</li> <li>A depth slice, or equivalently an activation map at depth <code>d</code> would be the activations <code>X[:,:,d]</code>. </li> </ul> <p>Conv Layer Example. Suppose that the input volume <code>X</code> has shape <code>X.shape: (11,11,4)</code>. Suppose further that we use no zero padding (\\(P = 0\\)), that the filter size is \\(F = 5\\), and that the stride is \\(S = 2\\). The output volume would therefore have spatial size (11-5)/2+1 = 4, giving a volume with width and height of 4. The activation map in the output volume (call it <code>V</code>), would then look as follows (only some of the elements are computed in this example):</p> <ul> <li><code>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</code></li> <li><code>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</code></li> <li><code>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</code></li> <li><code>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</code></li> </ul> <p>Remember that in numpy, the operation <code>*</code> above denotes elementwise multiplication between the arrays. Notice also that the weight vector <code>W0</code> is the weight vector of that neuron and <code>b0</code> is the bias. Here, <code>W0</code> is assumed to be of shape <code>W0.shape: (5,5,4)</code>, since the filter size is 5 and the depth of the input volume is 4. Notice that at each point, we are computing the dot product as seen before in ordinary neural networks. Also, we see that we are using the same weight and bias (due to parameter sharing), and where the dimensions along the width are increasing in steps of 2 (i.e. the stride). To construct a second activation map in the output volume, we would have:</p> <ul> <li><code>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</code></li> <li><code>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</code></li> <li><code>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</code></li> <li><code>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</code></li> <li><code>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</code> (example of going along y)</li> <li><code>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</code> (or along both)</li> </ul> <p>where we see that we are indexing into the second depth dimension in <code>V</code> (at index 1) because we are computing the second activation map, and that a different set of parameters (<code>W1</code>) is now used. In the example above, we are for brevity leaving out some of the other operations the Conv Layer would perform to fill the other parts of the output array <code>V</code>. Additionally, recall that these activation maps are often followed elementwise through an activation function such as ReLU, but this is not shown here.</p> <p>Summary. To summarize, the Conv Layer:</p> <ul> <li>Accepts a volume of size \\(W_1 \\times H_1 \\times D_1\\)</li> <li>Requires four hyperparameters: </li> <li>Number of filters \\(K\\), </li> <li>their spatial extent \\(F\\), </li> <li>the stride \\(S\\), </li> <li>the amount of zero padding \\(P\\).</li> <li>Produces a volume of size \\(W_2 \\times H_2 \\times D_2\\) where:</li> <li>\\(W_2 = (W_1 - F + 2P)/S + 1\\)</li> <li>\\(H_2 = (H_1 - F + 2P)/S + 1\\) (i.e. width and height are computed equally by symmetry)</li> <li>\\(D_2 = K\\)</li> <li>With parameter sharing, it introduces \\(F \\cdot F \\cdot D_1\\) weights per filter, for a total of \\((F \\cdot F \\cdot D_1) \\cdot K\\) weights and \\(K\\) biases.</li> <li>In the output volume, the \\(d\\)-th depth slice (of size \\(W_2 \\times H_2\\)) is the result of performing a valid convolution of the \\(d\\)-th filter over the input volume with a stride of \\(S\\), and then offset by \\(d\\)-th bias.</li> </ul> <p>A common setting of the hyperparameters is \\(F = 3, S = 1, P = 1\\). However, there are common conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures section below.</p> <p>Convolution Demo. Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize, all the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green)) are visualized with each depth slice stacked in rows. The input volume is of size \\(W_1 = 5, H_1 = 5, D_1 = 3\\), and the CONV layer parameters are \\(K = 2, F = 3, S = 2, P = 1\\). That is, we have two filters of size \\(3 \\times 3\\), and they are applied with a stride of 2. Therefore, the output volume size has spatial size (5 - 3 + 2)/2 + 1 = 3. Moreover, notice that a padding of \\(P = 1\\) is applied to the input volume, making the outer border of the input volume zero. The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.</p> <p>Implementation as Matrix Multiplication. Note that the convolution operation essentially performs dot products between the filters and local regions of the input. A common implementation pattern of the CONV layer is to take advantage of this fact and formulate the forward pass of a convolutional layer as one big matrix multiply as follows:</p> <ol> <li>The local regions in the input image are stretched out into columns in an operation commonly called im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix <code>X_col</code> of im2col of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.</li> <li>The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix <code>W_row</code> of size [96 x 363].</li> <li>The result of a convolution is now equivalent to performing one large matrix multiply <code>np.dot(W_row, X_col)</code>, which evaluates the dot product between every filter and every receptive field location. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location. </li> <li>The result must finally be reshaped back to its proper output dimension [55x55x96].</li> </ol> <p>This approach has the downside that it can use a lot of memory, since some values in the input volume are replicated multiple times in <code>X_col</code>. However, the benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (for example, in the commonly used BLAS API). Moreover, the same im2col idea can be reused to perform the pooling operation, which we discuss next.</p> <p>Backpropagation. The backward pass for a convolution operation (for both the data and the weights) is also a convolution (but with spatially-flipped filters). This is easy to derive in the 1-dimensional case with a toy example (not expanded on for now).</p> <p>1x1 convolution. As an aside, several papers use 1x1 convolutions, as first investigated by Network in Network. Some people are at first confused to see 1x1 convolutions especially when they come from signal processing background. Normally signals are 2-dimensional so 1x1 convolutions do not make sense (it's just pointwise scaling). However, in ConvNets this is not the case because one must remember that we operate over 3-dimensional volumes, and that the filters always extend through the full depth of the input volume. For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels).</p> <p>Dilated convolutions. A recent development (e.g. see paper by Fisher Yu and Vladlen Koltun) is to introduce one more hyperparameter to the CONV layer called the dilation. So far we've only discussed CONV filters that are contiguous. However, it's possible to have filters that have spaces between each cell, called dilation. As an example, in one dimension a filter <code>w</code> of size 3 would compute over input <code>x</code> the following: <code>w[0]*x[0] + w[1]*x[1] + w[2]*x[2]</code>. This is dilation of 0. For dilation 1 the filter would instead compute <code>w[0]*x[0] + w[1]*x[2] + w[2]*x[4]</code>; In other words there is a gap of 1 between the applications. This can be very useful in some settings to use in conjunction with 0-dilated filters because it allows you to merge spatial information across the inputs much more agressively with fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then you can convince yourself that the neurons on the 2nd layer are a function of a 5x5 patch of the input (we would say that the effective receptive field of these neurons is 5x5). If we use dilated convolutions then this effective receptive field would grow much quicker.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#pooling-layer","title":"Pooling Layer","text":"<p>It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:</p> <ul> <li>Accepts a volume of size \\(W_1 \\times H_1 \\times D_1\\)</li> <li>Requires two hyperparameters: </li> <li>their spatial extent \\(F\\), </li> <li>the stride \\(S\\), </li> <li>Produces a volume of size \\(W_2 \\times H_2 \\times D_2\\) where:</li> <li>\\(W_2 = (W_1 - F)/S + 1\\)</li> <li>\\(H_2 = (H_1 - F)/S + 1\\)</li> <li>\\(D_2 = D_1\\)</li> <li>Introduces zero parameters since it computes a fixed function of the input</li> <li>For Pooling layers, it is not common to pad the input using zero-padding.</li> </ul> <p>It is worth noting that there are only two commonly seen variations of the max pooling layer found in practice: A pooling layer with \\(F = 3, S = 2\\) (also called overlapping pooling), and more commonly \\(F = 2, S = 2\\). Pooling sizes with larger receptive fields are too destructive.</p> <p>General pooling. In addition to max pooling, the pooling units can also perform other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.</p>      Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. Right: The most common downsampling operation is max, giving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).    <p>Backpropagation. Recall from the backpropagation chapter that the backward pass for a max(x, y) operation has a simple interpretation as only routing the gradient to the input that had the highest value in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called the switches) so that gradient routing is efficient during backpropagation.</p> <p>Getting rid of pooling. Many people dislike the pooling operation and think that we can get away without it. For example, Striving for Simplicity: The All Convolutional Net proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#normalization-layer","title":"Normalization Layer","text":"<p>Many types of normalization layers have been proposed for use in ConvNet architectures, sometimes with the intentions of implementing inhibition schemes observed in the biological brain. However, these layers have since fallen out of favor because in practice their contribution has been shown to be minimal, if any. For various types of normalizations, see the discussion in Alex Krizhevsky's cuda-convnet library API.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#fully-connected-layer","title":"Fully-connected layer","text":"<p>Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset. See the Neural Network section of the notes for more information.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#converting-fc-layers-to-conv-layers","title":"Converting FC layers to CONV layers","text":"<p>It is worth noting that the only difference between FC and CONV layers is that the neurons in the CONV layer are connected only to a local region in the input, and that many of the neurons in a CONV volume share parameters. However, the neurons in both layers still compute dot products, so their functional form is identical. Therefore, it turns out that it's possible to convert between FC and CONV layers:</p> <ul> <li>For any CONV layer there is an FC layer that implements the same forward function. The weight matrix would be a large matrix that is mostly zero except for at certain blocks (due to local connectivity) where the weights in many of the blocks are equal (due to parameter sharing).</li> <li>Conversely, any FC layer can be converted to a CONV layer. For example, an FC layer with \\(K = 4096\\) that is looking at some input volume of size \\(7 \\times 7 \\times 512\\) can be equivalently expressed as a CONV layer with \\(F = 7, P = 0, S = 1, K = 4096\\). In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be \\(1 \\times 1 \\times 4096\\) since only a single depth column \"fits\" across the input volume, giving identical result as the initial FC layer.</li> </ul> <p>FC-&gt;CONV conversion. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that we'll see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above:</p> <ul> <li>Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \\(F = 7\\), giving output volume [1x1x4096].</li> <li>Replace the second FC layer with a CONV layer that uses filter size \\(F = 1\\), giving output volume [1x1x4096]</li> <li>Replace the last FC layer similarly, with \\(F=1\\), giving final output [1x1x1000]</li> </ul> <p>Each of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix \\(W\\) in each FC layer into CONV layer filters. It turns out that this conversion allows us to \"slide\" the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass. </p> <p>For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], we're now getting an entire 6x6 array of class scores across the 384x384 image.</p> <p>Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.</p> <p>Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores.</p> <p>Lastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride smaller than 32 pixels? We could achieve this with multiple forward passes. For example, note that if we wanted to use a stride of 16 pixels we could do so by combining the volumes received by forwarding the converted ConvNet twice: First over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and height.</p> <ul> <li>An IPython Notebook on Net Surgery shows how to perform the conversion in practice, in code (using Caffe)</li> </ul> <p></p>"},{"location":"courses/CNN/convolutional-networks/#convnet-architectures","title":"ConvNet Architectures","text":"<p>We have seen that Convolutional Networks are commonly made up of only three layer types: CONV, POOL (we assume Max pool unless stated otherwise) and FC (short for fully-connected). We will also explicitly write the RELU activation function as a layer, which applies elementwise non-linearity. In this section we discuss how these are commonly stacked together to form entire ConvNets. </p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#layer-patterns","title":"Layer Patterns","text":"<p>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:</p> <p><code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></p> <p>where the <code>*</code> indicates repetition, and the <code>POOL?</code> indicates an optional pooling layer. Moreover, <code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>), <code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually <code>K &lt; 3</code>). For example, here are some common ConvNet architectures you may see that follow this pattern:</p> <ul> <li><code>INPUT -&gt; FC</code>, implements a linear classifier. Here <code>N = M = K = 0</code>.</li> <li><code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code></li> <li><code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</code>. Here we see that there is a single CONV layer between every POOL layer.</li> <li><code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</code> Here we see two CONV layers stacked before every POOL layer. This is generally a good idea for larger and deeper networks, because multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation.</li> </ul> <p>Prefer a stack of small filter CONV to one large receptive field CONV layer. Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages. First, the neurons would be computing a linear function over the input, while the three stacks of CONV layers contain non-linearities that make their features more expressive. Second, if we suppose that all the volumes have \\(C\\) channels, then it can be seen that the single 7x7 CONV layer would contain \\(C \\times (7 \\times 7 \\times C) = 49 C^2\\) parameters, while the three 3x3 CONV layers would only contain \\(3 \\times (C \\times (3 \\times 3 \\times C)) = 27 C^2\\) parameters. Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.</p> <p>Recent departures. It should be noted that the conventional paradigm of a linear list of layers has recently been challenged, in Google's Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures.</p> <p>In practice: use whatever works best on ImageNet. If you're feeling a bit of a fatigue in thinking about the architectural decisions, you'll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as \"don't be a hero\": Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning school.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#layer-sizing-patterns","title":"Layer Sizing Patterns","text":"<p>Until now we've omitted mentions of common hyperparameters used in each of the layers in a ConvNet. We will first state the common rules of thumb for sizing the architectures and then follow the rules with a discussion of the notation:</p> <p>The input layer (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.</p> <p>The conv layers should be using small filters (e.g. 3x3 or at most 5x5), using a stride of \\(S = 1\\), and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when \\(F = 3\\), then using \\(P = 1\\) will retain the original size of the input. When \\(F = 5\\), \\(P = 2\\). For a general \\(F\\), it can be seen that \\(P = (F - 1) / 2\\) preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.</p> <p>The pool layers are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. \\(F = 2\\)), and with a stride of 2 (i.e. \\(S = 2\\)). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes \"fitting\" more complicated (e.g., a 32x32x3 layer would require zero padding to be used with a max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.</p> <p>Reducing sizing headaches. The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or don't zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters \"work out\", and that the ConvNet architecture is nicely and symmetrically wired.</p> <p>Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.</p> <p>Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be \"washed away\" too quickly.</p> <p>Compromising based on memory constraints. In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#case-studies","title":"Case studies","text":"<p>There are several architectures in the field of Convolutional Networks that have a name. The most common are:</p> <ul> <li>LeNet. The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990's. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.</li> <li>AlexNet. The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).</li> <li>ZF Net. The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler &amp; Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.</li> <li>GoogLeNet. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4.</li> <li>VGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.</li> <li>ResNet. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming's presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016).</li> </ul> <p>VGGNet in detail. Lets break down the VGGNet in more detail as a case study. The whole VGGNet is composed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layers that perform 2x2 max pooling with stride 2 (and no padding). We can write out the size of the representation at each step of the processing and keep track of both the representation size and the total number of weights:</p> <pre><code>INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n\nTOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)\nTOTAL params: 138M parameters\n</code></pre> <p>As is common with Convolutional Networks, notice that most of the memory (and also compute time) is used in the early CONV layers, and that most of the parameters are in the last FC layers. In this particular case, the first FC layer contains 100M weights, out of a total of 140M.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#computational-considerations","title":"Computational Considerations","text":"<p>The largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about 12GB of memory. There are three major sources of memory to keep track of:</p> <ul> <li>From the intermediate volume sizes: These are the raw number of activations at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.</li> <li>From the parameter sizes: These are the numbers that hold the network parameters, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.</li> <li>Every ConvNet implementation has to maintain miscellaneous memory, such as the image data batches, perhaps their augmented versions, etc.</li> </ul> <p>Once you have a rough estimate of the total number of values (for activations, gradients, and misc), the number should be converted to size in GB. Take the number of values, multiply by 4 to get the raw number of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and then divide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your network doesn't fit, a common heuristic to \"make it fit\" is to decrease the batch size, since most of the memory is usually consumed by the activations.</p> <p></p>"},{"location":"courses/CNN/convolutional-networks/#additional-resources","title":"Additional Resources","text":"<p>Additional resources related to implementation:</p> <ul> <li>Soumith benchmarks for CONV performance</li> <li>ConvNetJS CIFAR-10 demo allows you to play with ConvNet architectures and see the results and computations in real time, in the browser.</li> <li>Caffe, one of the popular ConvNet libraries.</li> <li>State of the art ResNets in Torch7</li> </ul>"},{"location":"courses/CNN/transfer-learning/","title":"\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4e0e\u200b\u5fae\u8c03","text":"<p>(These notes are currently in draft form and under development)</p> <p>Table of Contents:</p> <ul> <li>Transfer Learning</li> <li>Additional References</li> </ul> <p></p>"},{"location":"courses/CNN/transfer-learning/#transfer-learning","title":"Transfer Learning","text":"<p>In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest. The three major Transfer Learning scenarios look as follows:</p> <ul> <li>ConvNet as fixed feature extractor. Take a ConvNet pretrained on ImageNet, remove the last fully-connected layer (this layer's outputs are the 1000 class scores for a different task like ImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. In an AlexNet, this would compute a 4096-D vector for every image that contains the activations of the hidden layer immediately before the classifier. We call these features CNN codes. It is important for performance that these codes are ReLUd (i.e. thresholded at zero) if they were also thresholded during the training of the ConvNet on ImageNet (as is usually the case). Once you extract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmax classifier) for the new dataset.</li> <li>Fine-tuning the ConvNet. The second strategy is to not only replace and retrain the classifier on top of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrained network by continuing the backpropagation. It is possible to fine-tune all the layers of the ConvNet, or it's possible to keep some of the earlier layers fixed (due to overfitting concerns) and only fine-tune some higher-level portion of the network. This is motivated by the observation that the earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks, but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset. In case of ImageNet for example, which contains many dog breeds, a significant portion of the representational power of the ConvNet may be devoted to features that are specific to differentiating between dog breeds.</li> <li>Pretrained models. Since modern ConvNets take 2-3 weeks to train across multiple GPUs on ImageNet, it is common to see people release their final ConvNet checkpoints for the benefit of others who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo where people share their network weights.</li> </ul> <p>When and how to fine-tune? How do you decide what type of transfer learning you should perform on a new dataset? This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different, such as microscope images). Keeping in mind that ConvNet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios:</p> <ol> <li>New dataset is small and similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.</li> <li>New dataset is large and similar to the original dataset. Since we have more data, we can have more confidence that we won't overfit if we were to try to fine-tune through the full network.</li> <li>New dataset is small but very different from the original dataset. Since the data is small, it is likely best to only train a linear classifier. Since the dataset is very different, it might not be best to train the classifier form the top of the network, which contains more dataset-specific features. Instead, it might work better to train the SVM classifier from activations somewhere earlier in the network.</li> <li>New dataset is large and very different from the original dataset. Since the dataset is very large, we may expect that we can afford to train a ConvNet from scratch. However, in practice it is very often still beneficial to initialize with weights from a pretrained model. In this case, we would have enough data and confidence to fine-tune through the entire network.</li> </ol> <p>Practical advice. There are a few additional things to keep in mind when performing Transfer Learning:</p> <ul> <li>Constraints from pretrained models. Note that if you wish to use a pretrained network, you may be slightly constrained in terms of the architecture you can use for your new dataset. For example, you can't arbitrarily take out Conv layers from the pretrained network. However, some changes are straight-forward: Due to parameter sharing, you can easily run a pretrained network on images of different spatial size. This is clearly evident in the case of Conv/Pool layers because their forward function is independent of the input volume spatial size (as long as the strides \"fit\"). In case of FC layers, this still holds true because FC layers can be converted to a Convolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer is of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a Convolutional Layer that has receptive field size 6x6, and is applied with padding of 0.</li> <li>Learning rates. It's common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset. This is because we expect that the ConvNet weights are relatively good, so we don't wish to distort them too quickly and too much (especially while the new Linear Classifier above them is being trained from random initialization).</li> </ul> <p></p>"},{"location":"courses/CNN/transfer-learning/#additional-references","title":"Additional References","text":"<ul> <li>CNN Features off-the-shelf: an Astounding Baseline for Recognition trains SVMs on features from ImageNet-pretrained ConvNet and reports several state of the art results.</li> <li>DeCAF reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-based precursor to the C++ Caffe library.</li> <li>How transferable are features in deep neural networks? studies the transfer learning performance in detail, including some unintuitive findings about layer co-adaptations.</li> </ul>"},{"location":"courses/CNN/understanding-cnn/","title":"\u7406\u89e3\u200b\u4e0e\u200b\u53ef\u89c6\u5316","text":"<p>(this page is currently in draft form)</p>"},{"location":"courses/CNN/understanding-cnn/#visualizing-what-convnets-learn","title":"Visualizing what ConvNets learn","text":"<p>Several approaches for understanding and visualizing Convolutional Networks have been developed in the literature, partly as a response the common criticism that the learned features in a Neural Network are not interpretable. In this section we briefly survey some of these approaches and related work.</p>"},{"location":"courses/CNN/understanding-cnn/#visualizing-the-activations-and-first-layer-weights","title":"Visualizing the activations and first-layer weights","text":"<p>Layer Activations. The most straight-forward visualization technique is to show the activations of the network during the forward pass. For ReLU networks, the activations usually start out looking relatively blobby and dense, but as the training progresses the activations usually become more sparse and localized. One dangerous pitfall that can be easily noticed with this visualization is that some activation maps may be all zero for many different inputs, which can indicate dead filters, and can be a symptom of high learning rates.</p>      Typical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking at a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations are sparse (most values are zero, in this visualization shown in black) and mostly local.    <p>Conv/FC Filters. The second common strategy is to visualize the weights. These are usually most interpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible to also show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns can be an indicator of a network that hasn't been trained for long enough, or possibly a very low regularization strength that may have led to overfitting.</p>      Typical-looking filters on the first CONV layer (left), and the 2nd CONV layer (right) of a trained AlexNet. Notice that the first-layer weights are very nice and smooth, indicating nicely converged network. The color/grayscale features are clustered because the AlexNet contains two separate streams of processing, and an apparent consequence of this architecture is that one stream develops high-frequency grayscale features and the other low-frequency color features. The 2nd CONV layer weights are not as interpretable, but it is apparent that they are still smooth, well-formed, and absent of noisy patterns."},{"location":"courses/CNN/understanding-cnn/#retrieving-images-that-maximally-activate-a-neuron","title":"Retrieving images that maximally activate a neuron","text":"<p>Another visualization technique is to take a large dataset of images, feed them through the network and keep track of which images maximally activate some neuron. We can then visualize the images to get an understanding of what the neuron is looking for in its receptive field. One such visualization (among others) is shown in Rich feature hierarchies for accurate object detection and semantic segmentation by Ross Girshick et al.:</p>      Maximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the receptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a function of a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies, text, or specular highlights.    <p>One problem with this approach is that ReLU neurons do not necessarily have any semantic meaning by themselves. Rather, it is more appropriate to think of multiple ReLU neurons as the basis vectors of some space that represents in image patches. In other words, the visualization is showing the patches at the edge of the cloud of representations, along the (arbitrary) axes that correspond to the filter weights. This can also be seen by the fact that neurons in a ConvNet operate linearly over the input space, so any arbitrary rotation of that space is a no-op. This point was further argued in Intriguing properties of neural networks by Szegedy et al., where they perform a similar visualization along arbitrary directions in the representation space.</p>"},{"location":"courses/CNN/understanding-cnn/#embedding-the-codes-with-t-sne","title":"Embedding the codes with t-SNE","text":"<p>ConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier. We can get a rough idea about the topology of this space by embedding images into two dimensions so that their low-dimensional representation has approximately equal distances than their high-dimensional representation. There are many embedding methods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of the best-known methods that consistently produces visually-pleasing results.</p> <p>To produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including the ReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image. The corresponding images can them be visualized in a grid:</p>      t-SNE embedding of a set of images based on their CNN codes. Images that are nearby each other are also close in the CNN representation space, which implies that the CNN \"sees\" them as being very similar. Notice that the similarities are more often class-based and semantic rather than pixel and color-based. For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to t-SNE visualization of CNN codes."},{"location":"courses/CNN/understanding-cnn/#occluding-parts-of-the-image","title":"Occluding parts of the image","text":"<p>Suppose that a ConvNet classifies an image as a dog. How can we be certain that it's actually picking up on the dog in the image as opposed to some contextual cues from the background or some other miscellaneous object? One way of investigating which part of the image some classification prediction is coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of the position of an occluder object. That is, we iterate over regions of the image, set a patch of the image to be all zero, and look at the probability of the class. We can visualize the probability as a 2-dimensional heat map. This approach has been used in Matthew Zeiler's Visualizing and Understanding Convolutional Networks:</p>      Three input images (top). Notice that the occluder region is shown in grey. As we slide the occluder over the image we record the probability of the correct class and then visualize it as a heatmap (shown below each image). For instance, in the left-most image we see that the probability of Pomeranian plummets when the occluder covers the face of the dog, giving us some level of confidence that the dog's face is primarily responsible for the high classification score. Conversely, zeroing out other parts of the image is seen to have relatively negligible impact."},{"location":"courses/CNN/understanding-cnn/#visualizing-the-data-gradient-and-friends","title":"Visualizing the data gradient and friends","text":"<p>Data Gradient.</p> <p>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</p> <p>DeconvNet.</p> <p>Visualizing and Understanding Convolutional Networks</p> <p>Guided Backpropagation.</p> <p>Striving for Simplicity: The All Convolutional Net</p>"},{"location":"courses/CNN/understanding-cnn/#reconstructing-original-images-based-on-cnn-codes","title":"Reconstructing original images based on CNN Codes","text":"<p>Understanding Deep Image Representations by Inverting Them</p>"},{"location":"courses/CNN/understanding-cnn/#how-much-spatial-information-is-preserved","title":"How much spatial information is preserved?","text":"<p>Do ConvNets Learn Correspondence? (tldr: yes)</p>"},{"location":"courses/CNN/understanding-cnn/#plotting-performance-as-a-function-of-image-attributes","title":"Plotting performance as a function of image attributes","text":"<p>ImageNet Large Scale Visual Recognition Challenge</p>"},{"location":"courses/CNN/understanding-cnn/#fooling-convnets","title":"Fooling ConvNets","text":"<p>Explaining and Harnessing Adversarial Examples</p>"},{"location":"courses/CNN/understanding-cnn/#comparing-convnets-to-human-labelers","title":"Comparing ConvNets to Human labelers","text":"<p>What I learned from competing against a ConvNet on ImageNet</p>"},{"location":"courses/neural_network/classification/","title":"\u56fe\u50cf\u200b\u5206\u7c7b","text":"<p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u65e8\u5728\u200b\u5411\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u5916\u200b\u7684\u200b\u4eba\u200b\u4ecb\u7ecd\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u548c\u200b\u6570\u636e\u200b\u9a71\u52a8\u200b\u65b9\u6cd5\u200b\u7684\u200b\u5165\u95e8\u200b\u4ecb\u7ecd\u200b\u3002\u200b\u76ee\u5f55\u200b\u5982\u200b\u53f3\u4fa7\u200b\u6240\u793a\u200b\uff1a</p>"},{"location":"courses/neural_network/classification/#_1","title":"\u56fe\u50cf\u200b\u5206\u7c7b","text":"<p>\u200b\u52a8\u673a\u200b\u3002\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u5206\u914d\u200b\u7ed9\u200b\u56fa\u5b9a\u200b\u7c7b\u522b\u200b\u96c6\u5408\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u7684\u200b\u6838\u5fc3\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u5177\u6709\u200b\u8bb8\u591a\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u770b\u4f3c\u200b\u4e0d\u540c\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4efb\u52a1\u200b\uff08\u200b\u5982\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u3001\u200b\u5206\u5272\u200b\uff09\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5f52\u7ed3\u4e3a\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u3002</p> <p>\u200b\u793a\u4f8b\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e2d\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\u5e76\u200b\u4e3a\u200b4\u200b\u4e2a\u200b\u6807\u7b7e\u200b\uff08{\u200b\u732b\u200b\u3001\u200b\u72d7\u200b\u3001\u200b\u5e3d\u5b50\u200b\u3001\u200b\u676f\u5b50\u200b}\uff09\u200b\u5206\u914d\u200b\u6982\u7387\u200b\u3002\u200b\u5982\u56fe\u6240\u793a\u200b\uff0c\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u800c\u8a00\u200b\uff0c\u200b\u56fe\u50cf\u200b\u88ab\u200b\u8868\u793a\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5927\u200b\u7684\u200b\u4e09\u7ef4\u200b\u6570\u5b57\u200b\u6570\u7ec4\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u732b\u200b\u7684\u200b\u56fe\u50cf\u200b\u5bbd\u5ea6\u200b\u4e3a\u200b248\u200b\u50cf\u7d20\u200b\uff0c\u200b\u9ad8\u5ea6\u200b\u4e3a\u200b400\u200b\u50cf\u7d20\u200b\uff0c\u200b\u5e76\u200b\u5177\u6709\u200b\u4e09\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff1a\u200b\u7ea2\u8272\u200b\u3001\u200b\u7eff\u8272\u200b\u3001\u200b\u84dd\u8272\u200b\uff08\u200b\u7b80\u79f0\u200bRGB\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u56fe\u50cf\u200b\u7531\u200b248 x 400 x 3\u200b\u4e2a\u200b\u6570\u5b57\u200b\u7ec4\u6210\u200b\uff0c\u200b\u603b\u5171\u200b297,600\u200b\u4e2a\u200b\u6570\u5b57\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u6570\u5b57\u200b\u90fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4ece\u200b0\uff08\u200b\u9ed1\u8272\u200b\uff09\u200b\u5230\u200b255\uff08\u200b\u767d\u8272\u200b\uff09\u200b\u7684\u200b\u6574\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u5c06\u200b\u8fd9\u200b25\u200b\u4e07\u4e2a\u200b\u6570\u5b57\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u6807\u7b7e\u200b\uff0c\u200b\u6bd4\u5982\u200b\"\u200b\u732b\u200b\"\u3002</p> <p> </p> <p>\u200b\u5728\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u4e3a\u200b\u7ed9\u5b9a\u200b\u7684\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u6807\u7b7e\u200b\uff08\u200b\u6216\u200b\u5982\u6b64\u200b\u5904\u6240\u200b\u793a\u200b\uff0c\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u7684\u200b\u5206\u5e03\u200b\uff0c\u200b\u4ee5\u200b\u8868\u793a\u200b\u6211\u4eec\u200b\u7684\u200b\u7f6e\u4fe1\u5ea6\u200b\uff09\u3002\u200b\u56fe\u50cf\u200b\u662f\u200b\u7531\u200b\u6574\u6570\u200b\u6784\u6210\u200b\u7684\u200b\u4e09\u7ef4\u200b\u6570\u7ec4\u200b\uff0c\u200b\u53d6\u503c\u200b\u8303\u56f4\u200b\u4ece\u200b0\u200b\u5230\u200b255\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b\u5bbd\u5ea6\u200b x \u200b\u9ad8\u5ea6\u200b x 3\u3002\u200b\u8fd9\u91cc\u200b\u7684\u200b3\u200b\u4ee3\u8868\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff1a\u200b\u7ea2\u8272\u200b\u3001\u200b\u7eff\u8272\u200b\u3001\u200b\u84dd\u8272\u200b\u3002</p> <p>\u200b\u6311\u6218\u200b\u3002\u200b\u7531\u4e8e\u200b\u5bf9\u4e8e\u200b\u4eba\u7c7b\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8bc6\u522b\u200b\u89c6\u89c9\u200b\u6982\u5ff5\u200b\uff08\u200b\u4f8b\u5982\u200b\u732b\u200b\uff09\u200b\u76f8\u5bf9\u200b\u5bb9\u6613\u200b\uff0c\u200b\u56e0\u6b64\u200b\u503c\u5f97\u200b\u4ece\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7b97\u6cd5\u200b\u7684\u200b\u89d2\u5ea6\u200b\u8003\u8651\u200b\u6240\u200b\u6d89\u53ca\u200b\u7684\u200b\u6311\u6218\u200b\u3002\u200b\u5728\u200b\u4e0b\u9762\u200b\u6211\u4eec\u200b\u5217\u4e3e\u200b\u4e86\u200b\uff08\u200b\u4e0d\u200b\u5168\u9762\u200b\u7684\u200b\uff09\u200b\u4e00\u4e9b\u200b\u6311\u6218\u200b\u65f6\u200b\uff0c\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\u56fe\u50cf\u200b\u7684\u200b\u539f\u59cb\u200b\u8868\u793a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7531\u200b\u4eae\u5ea6\u200b\u503c\u200b\u7ec4\u6210\u200b\u7684\u200b\u4e09\u7ef4\u200b\u6570\u7ec4\u200b\uff1a</p> <ul> <li>\u200b\u89c6\u89d2\u200b\u53d8\u5316\u200b\u3002\u200b\u540c\u4e00\u200b\u5bf9\u8c61\u200b\u7684\u200b\u5355\u4e2a\u200b\u5b9e\u4f8b\u200b\u53ef\u4ee5\u200b\u4ee5\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u76f8\u673a\u200b\u5b9a\u4f4d\u200b\u3002</li> <li>\u200b\u5c3a\u5ea6\u200b\u53d8\u5316\u200b\u3002\u200b\u89c6\u89c9\u200b\u7c7b\u522b\u200b\u901a\u5e38\u200b\u5728\u200b\u5927\u5c0f\u200b\u4e0a\u200b\u8868\u73b0\u200b\u51fa\u200b\u53d8\u5316\u200b\uff08\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u5b9e\u9645\u200b\u4e16\u754c\u200b\u4e2d\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u800c\u200b\u4e0d\u4ec5\u4ec5\u200b\u662f\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u5927\u5c0f\u200b\uff09\u3002</li> <li>\u200b\u5f62\u53d8\u200b\u3002\u200b\u8bb8\u591a\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u5bf9\u8c61\u200b\u4e0d\u662f\u200b\u521a\u4f53\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4ee5\u200b\u6781\u7aef\u200b\u65b9\u5f0f\u200b\u53d8\u5f62\u200b\u3002</li> <li>\u200b\u906e\u6321\u200b\u3002\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u5bf9\u8c61\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u88ab\u200b\u906e\u6321\u200b\u3002\u200b\u6709\u65f6\u5019\u200b\uff0c\u200b\u53ea\u6709\u200b\u5bf9\u8c61\u200b\u7684\u200b\u4e00\u5c0f\u90e8\u5206\u200b\uff08\u200b\u53ef\u80fd\u200b\u53ea\u6709\u200b\u51e0\u4e2a\u200b\u50cf\u7d20\u200b\uff09\u200b\u53ef\u89c1\u200b\u3002</li> <li>\u200b\u5149\u7167\u200b\u6761\u4ef6\u200b\u3002\u200b\u5149\u7167\u200b\u5bf9\u200b\u50cf\u7d20\u200b\u7ea7\u522b\u200b\u4ea7\u751f\u200b\u663e\u8457\u200b\u5f71\u54cd\u200b\u3002</li> <li>\u200b\u80cc\u666f\u200b\u6742\u4e71\u200b\u3002\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u5bf9\u8c61\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0e\u5176\u200b\u73af\u5883\u200b\u878d\u4e3a\u4e00\u4f53\u200b\uff0c\u200b\u96be\u4ee5\u200b\u8bc6\u522b\u200b\u3002</li> <li>\u200b\u7c7b\u5185\u200b\u53d8\u5f02\u200b\u3002\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u7c7b\u522b\u200b\u901a\u5e38\u200b\u76f8\u5bf9\u200b\u5e7f\u6cdb\u200b\uff0c\u200b\u4f8b\u5982\u200b\u6905\u5b50\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5bf9\u8c61\u200b\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u578b\u200b\uff0c\u200b\u6bcf\u79cd\u200b\u7c7b\u578b\u200b\u90fd\u200b\u6709\u200b\u81ea\u5df1\u200b\u7684\u200b\u5916\u89c2\u200b\u3002</li> </ul> <p>\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u5fc5\u987b\u200b\u5bf9\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u53d8\u5316\u200b\u7684\u200b\u4ea4\u53c9\u200b\u4e58\u79ef\u200bcross product\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u540c\u65f6\u200b\u4fdd\u7559\u200b\u5bf9\u200b\u7c7b\u95f4\u200b\u53d8\u5316\u200b\u7684\u200b\u654f\u611f\u6027\u200b\u3002</p> <p> </p> <p>\u200b\u6570\u636e\u200b\u9a71\u52a8\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u5c06\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u5230\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u7b97\u6cd5\u200b\u5462\u200b\uff1f\u200b\u4e0e\u200b\u7f16\u5199\u200b\u7528\u4e8e\u200b\u5bf9\u200b\u4e00\u5217\u200b\u6570\u5b57\u200b\u8fdb\u884c\u200b\u6392\u5e8f\u200b\u7684\u200b\u7b97\u6cd5\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5f88\u96be\u200b\u60f3\u8c61\u200b\u5982\u4f55\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u8bc6\u522b\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u732b\u200b\u7684\u200b\u7b97\u6cd5\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e0e\u5176\u200b\u8bd5\u56fe\u200b\u76f4\u63a5\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u6307\u5b9a\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u5916\u89c2\u200b\uff0c\u200b\u6211\u4eec\u200b\u91c7\u53d6\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4e0e\u200b\u60a8\u200b\u5bf9\u5f85\u200b\u5b69\u5b50\u200b\u7684\u200b\u65b9\u6cd5\u200b\u7c7b\u4f3c\u200b\uff1a\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e3a\u200b\u8ba1\u7b97\u673a\u200b\u63d0\u4f9b\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u8bb8\u591a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u7136\u540e\u200b\u5f00\u53d1\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7b97\u6cd5\u200b\u4f1a\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u5e76\u200b\u5b66\u4e60\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u89c6\u89c9\u200b\u5916\u89c2\u200b\u3002\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u6570\u636e\u200b\u9a71\u52a8\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u9996\u5148\u200b\u4f9d\u8d56\u4e8e\u200b\u79ef\u7d2f\u200b\u5e26\u6709\u200b\u6807\u7b7e\u200b\u7684\u200b\u56fe\u50cf\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u793a\u4f8b\u200b\uff1a</p> <p></p> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u56db\u4e2a\u200b\u89c6\u89c9\u200b\u7c7b\u522b\u200b\u7684\u200b\u793a\u4f8b\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u6709\u200b\u6570\u5343\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u90fd\u200b\u6709\u200b\u6570\u5341\u4e07\u200b\u5f20\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6d41\u7a0b\u200b\u3002\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\uff0c\u200b\u5728\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4e2d\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u5c06\u200b\u8868\u793a\u200b\u5355\u4e2a\u200b\u56fe\u50cf\u200b\u7684\u200b\u50cf\u7d20\u200b\u6570\u7ec4\u200b\u5206\u914d\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u5b8c\u6574\u200b\u6d41\u7a0b\u200b\u53ef\u4ee5\u200b\u6b63\u5f0f\u200b\u8868\u8ff0\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u8f93\u5165\u200b\uff1a\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u5305\u62ec\u200b\u4e00\u7ec4\u200bN\u200b\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6bcf\u5f20\u200b\u56fe\u50cf\u200b\u90fd\u200b\u5e26\u6709\u200bK\u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u79f0\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002</li> <li>\u200b\u5b66\u4e60\u200b\uff1a\u200b\u6211\u4eec\u200b\u7684\u200b\u4efb\u52a1\u200b\u662f\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u96c6\u6765\u200b\u5b66\u4e60\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u5916\u89c2\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u79f0\u4e3a\u200b\u8bad\u7ec3\u200b\u5206\u7c7b\u5668\u200b\u6216\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u8bc4\u4f30\u200b\uff1a\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u8981\u6c42\u200b\u5206\u7c7b\u5668\u200b\u4e3a\u200b\u5b83\u200b\u4ee5\u524d\u200b\u4ece\u672a\u89c1\u8fc7\u200b\u7684\u200b\u4e00\u7ec4\u200b\u65b0\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u6765\u200b\u8bc4\u4f30\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u8d28\u91cf\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u7684\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u4e0e\u200b\u5206\u7c7b\u5668\u200b\u9884\u6d4b\u200b\u7684\u200b\u6807\u7b7e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5f88\u591a\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u7b54\u6848\u200b\uff08\u200b\u6211\u4eec\u200b\u79f0\u4e4b\u4e3a\u200b\u771f\u503c\u200bground truth\uff09\u200b\u76f8\u5339\u914d\u200b\u3002</li> </ul> <p></p>"},{"location":"courses/neural_network/classification/#nearest-neighbor-classifier","title":"\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b Nearest Neighbor Classifier","text":"<p>\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f00\u53d1\u200b\u4e00\u79cd\u200b\u79f0\u4e3a\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u4e0e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65e0\u5173\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u5f88\u5c11\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u5c06\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e86\u89e3\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u7684\u200b\u57fa\u672c\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff1aCIFAR-10\u3002\u200b\u4e00\u4e2a\u200b\u6d41\u884c\u200b\u7684\u200b\u73a9\u5177\u200btoy\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u662f\u200bCIFAR-10\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b60,000\u200b\u5f20\u200b\u5c3a\u5bf8\u200b\u4e3a\u200b32x32\u200b\u50cf\u7d20\u200b\u7684\u200b\u5c0f\u200b\u56fe\u50cf\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u90fd\u200b\u5e26\u6709\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\uff08\u200b\u4f8b\u5982\u200b\"\u200b\u98de\u673a\u200b\u3001\u200b\u6c7d\u8f66\u200b\u3001\u200b\u9e1f\u7c7b\u200b\u7b49\u200b\"\uff09\u3002\u200b\u8fd9\u200b60,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\u5206\u4e3a\u200b50,000\u200b\u5f20\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b10,000\u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e2d\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6765\u81ea\u200b\u6bcf\u4e2a\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u7684\u200b10\u200b\u4e2a\u200b\u968f\u673a\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\uff1a</p> <p> </p> <p>\u200b\u5de6\u56fe\u200b\uff1aCIFAR-10\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u3002\u200b\u53f3\u56fe\u200b\uff1a\u200b\u7b2c\u4e00\u5217\u200b\u663e\u793a\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff0c\u200b\u65c1\u8fb9\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u90fd\u200b\u663e\u793a\u200b\u4e86\u200b\u6839\u636e\u200b\u50cf\u7d20\u200b\u5dee\u5f02\u200b\u800c\u200b\u786e\u5b9a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\u524d\u200b10\u200b\u4e2a\u200b\u6700\u8fd1\u200b\u90bb\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6709\u200bCIFAR-10\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u542b\u200b50,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u6807\u7b7e\u200b\u6709\u200b5,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u5269\u4f59\u200b\u7684\u200b10,000\u200b\u5f20\u200b\u8fdb\u884c\u200b\u6807\u8bb0\u200b\u3002\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u5c06\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u5e76\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u6700\u200b\u63a5\u8fd1\u200b\u7684\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u76f8\u540c\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u56fe\u50cf\u200b\u53f3\u4fa7\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fd9\u79cd\u200b\u8fc7\u7a0b\u200b\u7684\u200b10\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u8fd9\u200b10\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u53ea\u6709\u200b\u7ea6\u200b3\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u200b\u68c0\u7d22\u200b\u5230\u200b\u4e86\u200b\u76f8\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u5176\u4ed6\u200b7\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u4e2d\u5219\u200b\u4e0d\u662f\u200b\u8fd9\u6837\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u7b2c\u200b8\u200b\u884c\u200b\uff0c\u200b\u9a6c\u5934\u200b\u7684\u200b\u6700\u8fd1\u200b\u90bb\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u662f\u200b\u4e00\u8f86\u200b\u7ea2\u8272\u200b\u6c7d\u8f66\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u7531\u4e8e\u200b\u5f3a\u70c8\u200b\u7684\u200b\u9ed1\u8272\u200b\u80cc\u666f\u200b\u9020\u6210\u200b\u7684\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u5f20\u200b\u9a6c\u200b\u7684\u200b\u56fe\u50cf\u200b\u4f1a\u200b\u88ab\u200b\u9519\u8bef\u200b\u5730\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u6c7d\u8f66\u200b\u3002</p> <p>\u200b\u60a8\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e76\u672a\u200b\u660e\u786e\u200b\u8bf4\u660e\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u7cbe\u786e\u200b\u6bd4\u8f83\u200b\u4e24\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u7ec6\u8282\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u4e24\u4e2a\u200b32 x 32 x 3\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u3002\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u53ef\u80fd\u6027\u200b\u662f\u200b\u9010\u200b\u50cf\u7d20\u200b\u6bd4\u8f83\u200b\u56fe\u50cf\u200b\u5e76\u200b\u7d2f\u52a0\u200b\u6240\u6709\u200b\u5dee\u5f02\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u7ed9\u5b9a\u200b\u4e24\u5f20\u200b\u56fe\u50cf\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8868\u793a\u200b\u4e3a\u200b\u5411\u91cf\u200b \\(I_1\\), \\(I_2\\) \uff0c\u200b\u6bd4\u8f83\u200b\u5b83\u4eec\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5408\u7406\u200b\u9009\u62e9\u200b\u53ef\u80fd\u200b\u662f\u200bL1\u200b\u8ddd\u79bb\u200b\uff1a</p> \\[ d_1 (I_1, I_2) = \\sum_{p} \\left| I^p_1 - I^p_2 \\right| \\] <p>\u200b\u5176\u4e2d\u200b\u603b\u548c\u200b\u53d6\u81ea\u200b\u6240\u6709\u200b\u50cf\u7d20\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u8be5\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u8fc7\u7a0b\u200b\uff1a</p> <p> </p> <p>\u200b\u4f7f\u7528\u200b\u50cf\u7d20\u200b\u5dee\u5f02\u200b\u6765\u200b\u6bd4\u8f83\u200b\u4e24\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u4f7f\u7528\u200bL1\u200b\u8ddd\u79bb\u200b\uff08\u200b\u5728\u200b\u6b64\u200b\u793a\u4f8b\u200b\u4e2d\u4e3a\u200b\u4e00\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff09\u3002\u200b\u4e24\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9010\u200b\u5143\u7d20\u200b\u76f8\u51cf\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u6240\u6709\u200b\u5dee\u5f02\u200b\u76f8\u52a0\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u6570\u5b57\u200b\u3002\u200b\u5982\u679c\u200b\u4e24\u5f20\u200b\u56fe\u50cf\u200b\u76f8\u540c\u200b\uff0c\u200b\u7ed3\u679c\u200b\u5c06\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u56fe\u50cf\u200b\u975e\u5e38\u200b\u4e0d\u540c\u200b\uff0c\u200b\u7ed3\u679c\u200b\u5c06\u200b\u5f88\u5927\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e5f\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200bCIFAR-10\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5230\u200b\u5185\u5b58\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u4e3a\u200b4\u200b\u4e2a\u200b\u6570\u7ec4\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b/\u200b\u6807\u7b7e\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b/\u200b\u6807\u7b7e\u200b\u3002\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c<code>Xtr</code>\uff08\u200b\u5927\u5c0f\u200b\u4e3a\u200b50,000 x 32 x 32 x 3\uff09\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\uff0c\u200b\u76f8\u5e94\u200b\u7684\u200b\u4e00\u7ef4\u200b\u6570\u7ec4\u200b<code>Ytr</code>\uff08\u200b\u957f\u5ea6\u200b\u4e3a\u200b50,000\uff09\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u6807\u7b7e\u200b\uff08\u200b\u4ece\u200b0\u200b\u5230\u200b9\uff09\uff1a</p> <pre><code>Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide\n# flatten out all images to be one-dimensional\nXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\nXte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072\n</code></pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u5c55\u5f00\u200b\u4e3a\u884c\u200b\uff0c\u200b\u4e0b\u9762\u200b\u662f\u200b\u5982\u4f55\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>nn = NearestNeighbor() # create a Nearest Neighbor classifier class\nnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels\nYte_predict = nn.predict(Xte_rows) # predict labels on the test images\n# and now print the classification accuracy, which is the average number\n# of examples that are correctly predicted (i.e. label matches)\nprint 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )\n</code></pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u8bc4\u4f30\u200b\u6807\u51c6\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u51c6\u786e\u5ea6\u200baccuracy\uff0c\u200b\u5b83\u200b\u6d4b\u91cf\u200b\u4e86\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u7684\u200b\u6240\u6709\u200b\u5206\u7c7b\u5668\u200b\u90fd\u200b\u6ee1\u8db3\u200b\u4e00\u4e2a\u200b\u5171\u540c\u200b\u7684\u200bAPI\uff1a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b<code>train(X,y)</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u6570\u636e\u200b\u548c\u200b\u6807\u7b7e\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\u3002\u200b\u5728\u200b\u5185\u90e8\u200b\uff0c\u200b\u8be5\u7c7b\u200b\u5e94\u8be5\u200b\u6784\u5efa\u200b\u4e00\u4e9b\u200b\u5173\u4e8e\u200b\u6807\u7b7e\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u4ece\u200b\u6570\u636e\u200b\u4e2d\u200b\u9884\u6d4b\u200b\u5b83\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u7136\u540e\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b<code>predict(X)</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u65b0\u200b\u6570\u636e\u200b\u5e76\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u6211\u4eec\u200b\u7701\u7565\u200b\u4e86\u200b\u4e8b\u60c5\u200b\u7684\u200b\u5b9e\u8d28\u200b\u90e8\u5206\u200b - \u200b\u5b9e\u9645\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u672c\u8eab\u200b\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200bL1\u200b\u8ddd\u79bb\u200b\u7684\u200b\u7b80\u5355\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5b83\u200b\u6ee1\u8db3\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u6a21\u677f\u200b\uff1a</p> <pre><code>import numpy as np\n\nclass NearestNeighbor(object):\n  def __init__(self):\n    pass\n\n  def train(self, X, y):\n\"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n    # the nearest neighbor classifier simply remembers all the training data\n    self.Xtr = X\n    self.ytr = y\n\n  def predict(self, X):\n\"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type matches the input type\n    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n    # loop over all test rows\n    for i in range(num_test):\n      # find the nearest training image to the i'th test image\n      # using the L1 distance (sum of absolute value differences)\n      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n      min_index = np.argmin(distances) # get the index with smallest distance\n      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n\n    return Ypred\n</code></pre> <p>\u200b\u5982\u679c\u200b\u8fd0\u884c\u200b\u6b64\u200b\u4ee3\u7801\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u770b\u5230\u200b\u8be5\u200b\u5206\u7c7b\u5668\u200b\u5728\u200bCIFAR-10\u200b\u4e0a\u200b\u53ea\u80fd\u200b\u8fbe\u5230\u200b38.6%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002\u200b\u8fd9\u6bd4\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u8981\u200b\u597d\u5f97\u591a\u200b\uff08\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u5c06\u200b\u63d0\u4f9b\u200b10%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6709\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\uff0c\u200b\u4f46\u200b\u8fdc\u8fdc\u200b\u4e0d\u53ca\u200b\u4eba\u7c7b\u200b\u8868\u73b0\u200b\uff08\u200b\u4f30\u8ba1\u200b\u7ea6\u200b\u4e3a\u200b94%\uff09\u200b\u53c2\u8003\u200b\u94fe\u63a5\u200b\uff0c\u200b\u4e5f\u200b\u8fdc\u8fdc\u200b\u4e0d\u53ca\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5b83\u4eec\u200b\u8fbe\u5230\u200b\u4e86\u200b\u7ea6\u200b95%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u4e0e\u200b\u4eba\u7c7b\u200b\u51c6\u786e\u7387\u200b\u76f8\u5339\u914d\u200b\uff08\u200b\u8bf7\u53c2\u9605\u200bCIFAR-10\u200b\u7684\u200bKaggle\u200b\u7ade\u8d5b\u200b\u6392\u884c\u699c\u200b\uff09\u3002</p> <p>\u200b\u8ddd\u79bb\u200b\u7684\u200b\u9009\u62e9\u200b\u3002\u200b\u8ba1\u7b97\u200b\u5411\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ddd\u79bb\u200b\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u65b9\u6cd5\u200b\u3002\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u9009\u62e9\u200b\u53ef\u80fd\u200b\u662f\u200b\u4f7f\u7528\u200bL2\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u5b83\u200b\u5177\u6709\u200b\u8ba1\u7b97\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6b27\u51e0\u91cc\u5fb7\u200b\u8ddd\u79bb\u200b\u7684\u200b\u51e0\u4f55\u200b\u89e3\u91ca\u200b\u3002\u200b\u8be5\u200b\u8ddd\u79bb\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> \\[ d_2 (I_1, I_2) = \\sqrt{\\sum_{p} \\left( I^p_1 - I^p_2 \\right)^2} \\] <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u50cf\u200b\u4ee5\u524d\u200b\u4e00\u6837\u200b\u8ba1\u7b97\u200b\u50cf\u7d20\u200b\u5dee\u5f02\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5168\u90e8\u200b\u5e73\u65b9\u200b\uff0c\u200b\u76f8\u52a0\u200b\uff0c\u200b\u7136\u540e\u200b\u6700\u540e\u200b\u53d6\u200b\u5e73\u65b9\u6839\u200b\u3002\u200b\u5728\u200bnumpy\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u66ff\u6362\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u3002\u200b\u5373\u200b\u8ba1\u7b97\u200b\u8ddd\u79bb\u200b\u7684\u200b\u90a3\u200b\u4e00\u884c\u200b\uff1a</p> <pre><code>distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = 1))\n</code></pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u200b\u5728\u200b\u4e0a\u9762\u200b\u5305\u62ec\u200b\u4e86\u200b<code>np.sqrt</code>\u200b\u8c03\u7528\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u5b9e\u9645\u200b\u7684\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7701\u7565\u200b\u5e73\u65b9\u6839\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5e73\u65b9\u6839\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u8c03\u200b\u51fd\u6570\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u7f29\u653e\u200b\u8ddd\u79bb\u200b\u7684\u200b\u7edd\u5bf9\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4f46\u4f1a\u200b\u4fdd\u7559\u200b\u6392\u5e8f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5e26\u200b\u6216\u200b\u4e0d\u200b\u5e26\u200b\u5b83\u200b\u7684\u200b\u6700\u8fd1\u200b\u90bb\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002\u200b\u5982\u679c\u200b\u5728\u200bCIFAR-10\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u8ddd\u79bb\u200b\u8fd0\u884c\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u83b7\u5f97\u200b35.4%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff08\u200b\u7565\u4f4e\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200bL1\u200b\u8ddd\u79bb\u200b\u7ed3\u679c\u200b\uff09\u3002</p> <p>L1 vs. L2\u3002\u200b\u8003\u8651\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u5ea6\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u662f\u200b\u5f88\u200b\u6709\u8da3\u200b\u7684\u200b\u3002\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u5f53\u200b\u6d89\u53ca\u200b\u5230\u200b\u4e24\u4e2a\u200b\u5411\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u65f6\u200b\uff0cL2\u200b\u8ddd\u79bb\u200b\u6bd4\u200bL1\u200b\u8ddd\u79bb\u200b\u66f4\u52a0\u200b\u4e25\u683c\u200bunforgiving \u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0cL2\u200b\u8ddd\u79bb\u200b\u66f4\u200b\u559c\u6b22\u200b\u8bb8\u591a\u200b\u4e2d\u7b49\u200b\u7a0b\u5ea6\u200b\u7684\u200b\u5dee\u5f02\u200bmedium disagreements\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u5927\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002L1\u200b\u548c\u200bL2\u200b\u8ddd\u79bb\u200b\uff08\u200b\u6216\u8005\u200b\u7b49\u6548\u200b\u5730\u200b\uff0c\u200b\u4e00\u5bf9\u200b\u56fe\u50cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u7684\u200bL1/L2\u200b\u8303\u6570\u200b\uff09\u200b\u662f\u200bp-\u200b\u8303\u6570\u200b\u7684\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u3002</p> <p></p>"},{"location":"courses/neural_network/classification/#k-k-nearest-neighbor-classifier","title":"k-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b k - Nearest Neighbor Classifier","text":"<p>\u200b\u60a8\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u4ec5\u200b\u4f7f\u7528\u200b\u6700\u8fd1\u200b\u56fe\u50cf\u200b\u7684\u200b\u6807\u7b7e\u200b\u662f\u200b\u5947\u602a\u200b\u7684\u200b\u3002\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u6240\u8c13\u200b\u7684\u200bk-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u6765\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u60f3\u6cd5\u200b\u975e\u5e38\u7b80\u5355\u200b\uff1a\u200b\u4e0d\u518d\u200b\u53ea\u200b\u627e\u5230\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\u5355\u4e00\u200b\u6700\u8fd1\u200b\u56fe\u50cf\u200b\uff0c\u200b\u800c\u662f\u200b\u627e\u5230\u200bk\u200b\u4e2a\u200b\u6700\u8fd1\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u8ba9\u200b\u5b83\u4eec\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u7684\u200b\u6807\u7b7e\u200b\u8fdb\u884c\u200b\u6295\u7968\u200b\u3002\u200b\u7279\u522b\u200b\u5730\u200b\uff0c\u200b\u5f53\u200bk=1\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u6062\u590d\u200b\u4e86\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200bk\u200b\u503c\u200b\u5177\u6709\u200b\u5e73\u6ed1\u200b\u6548\u679c\u200b\uff0c\u200b\u4f7f\u200b\u5206\u7c7b\u5668\u200b\u66f4\u80fd\u200b\u62b5\u6297\u200b\u5f02\u5e38\u200b\u503c\u200boutliers\uff1a</p> <p></p> <p>\u200b\u8fd9\u662f\u200b\u6700\u8fd1\u200b\u90bb\u200b\u548c\u200b5-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u793a\u4f8b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e8c\u7ef4\u200b\u70b9\u200b\u548c\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\uff08\u200b\u7ea2\u8272\u200b\u3001\u200b\u84dd\u8272\u200b\u3001\u200b\u7eff\u8272\u200b\uff09\u3002\u200b\u5f69\u8272\u200b\u533a\u57df\u200b\u663e\u793a\u200b\u4e86\u200b\u7531\u200bL2\u200b\u8ddd\u79bb\u200b\u8bf1\u5bfc\u200binduced\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u4ea7\u751f\u200b\u7684\u200b\u51b3\u7b56\u200b\u8fb9\u754c\u200b\u3002\u200b\u767d\u8272\u200b\u533a\u57df\u200b\u663e\u793a\u200b\u4e86\u200b\u6a21\u7cca\u200b\u5206\u7c7b\u200b\u7684\u200b\u70b9\u200b\uff08\u200b\u5373\u200b\u81f3\u5c11\u200b\u6709\u200b\u4e24\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u6295\u7968\u200b\u76f8\u7b49\u200b\uff09\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0coutlier\u200b\u5f02\u5e38\u200b\u6570\u636e\u200b\u70b9\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u84dd\u70b9\u200b\u4e91\u200b\u4e2d\u95f4\u200b\u7684\u200b\u7eff\u70b9\u200b\uff09\u200b\u4f1a\u200b\u521b\u5efa\u200b\u53ef\u80fd\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u5c0f\u5c9b\u5c7f\u200b\u9884\u6d4b\u200b\uff0c\u200b\u800c\u200b5-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u5219\u200b\u5e73\u6ed1\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u4e0d\u89c4\u5219\u200b\u6027\u200birregularities\uff0c\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff08\u200b\u672a\u200b\u663e\u793a\u200b\uff09\u200b\u66f4\u597d\u200b\u7684\u200b\u6cdb\u5316\u200b\u3002\u200b\u8fd8\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b5-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u7070\u8272\u200b\u533a\u57df\u200b\u662f\u200b\u7531\u4e8e\u200b\u5728\u200b\u6700\u8fd1\u200b\u90bb\u4e2d\u200b\u7684\u200b\u6295\u7968\u200b\u4e2d\u200b\u5b58\u5728\u200b\u5e73\u5c40\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c2\u200b\u4e2a\u200b\u6700\u8fd1\u200b\u90bb\u662f\u200b\u7ea2\u8272\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6700\u8fd1\u200b\u90bb\u662f\u200b\u84dd\u8272\u200b\uff0c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u6700\u8fd1\u200b\u90bb\u662f\u200b\u7eff\u8272\u200b\uff09\u200b\u6240\u200b\u5bfc\u81f4\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u60a8\u200b\u51e0\u4e4e\u200b\u603b\u662f\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200bk-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u3002\u200b\u4f46\u662f\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u4ec0\u4e48\u200b\u503c\u200b\u7684\u200bk\uff1f\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u5c06\u200b\u8ba8\u8bba\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> <p></p>"},{"location":"courses/neural_network/classification/#validation-sets-for-hyperparameter-tuning","title":"\u7528\u4e8e\u200b\u8d85\u200b\u53c2\u6570\u200b\u8c03\u6574\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u96c6\u200b Validation sets for Hyperparameter tuning","text":"<p>k-\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200bk\u3002\u200b\u4f46\u662f\u200b\u4ec0\u4e48\u200b\u6570\u503c\u200b\u6700\u597d\u200b\uff1f\u200b\u6b64\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u8ddd\u79bb\u200b\u51fd\u6570\u200b\u53ef\u200b\u4f9b\u9009\u62e9\u200b\uff1aL1\u200b\u8303\u6570\u200b\u3001L2\u200b\u8303\u6570\u200b\uff0c\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u9009\u62e9\u200b\uff0c\u200b\u751a\u81f3\u200b\u6211\u4eec\u200b\u90fd\u200b\u6ca1\u6709\u200b\u8003\u8651\u200b\uff08\u200b\u4f8b\u5982\u200b\u70b9\u79ef\u200b\uff09\u3002\u200b\u8fd9\u4e9b\u200b\u9009\u62e9\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5728\u200b\u8bb8\u591a\u200b\u4ece\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u4e2d\u200b\u7ecf\u5e38\u51fa\u73b0\u200b\u3002\u200b\u901a\u5e38\u200b\u4e0d\u200b\u660e\u663e\u200b\u5e94\u8be5\u200b\u9009\u62e9\u200b\u4ec0\u4e48\u200b\u503c\u200b/\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5efa\u8bae\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u770b\u770b\u200b\u54ea\u4e2a\u200b\u6548\u679c\u200b\u6700\u597d\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u786e\u5b9e\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u5c06\u8981\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u4f46\u200b\u5fc5\u987b\u200b\u975e\u5e38\u200b\u5c0f\u5fc3\u200b\u3002\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u80fd\u200b\u4f7f\u7528\u200b\u6d4b\u8bd5\u200b\u96c6\u6765\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u65e0\u8bba\u200b\u4f55\u65f6\u200b\u8bbe\u8ba1\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u60a8\u200b\u90fd\u200b\u5e94\u8be5\u200b\u5c06\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u89c6\u4e3a\u200b\u4e00\u79cd\u200b\u975e\u5e38\u200b\u5b9d\u8d35\u200b\u7684\u200b\u8d44\u6e90\u200b\uff0c\u200b\u6700\u597d\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u6b21\u200b\u4e4b\u524d\u200b\u6c38\u8fdc\u200b\u4e0d\u8981\u200b\u89e6\u6478\u200b\u5b83\u200b\u3002\u200b\u5426\u5219\u200b\uff0c\u200b\u975e\u5e38\u200b\u73b0\u5b9e\u200b\u7684\u200b\u5371\u9669\u200b\u662f\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u4ee5\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\uff0c\u200b\u6027\u80fd\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u663e\u8457\u200b\u964d\u4f4e\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8bf4\u200b\u60a8\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8fc7\u200b\u62df\u5408\u200boverfit\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u770b\u5f85\u200b\u95ee\u9898\u200b\u7684\u200b\u65b9\u5f0f\u200b\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u5c06\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7528\u4f5c\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\u65f6\u200b\u89c2\u5bdf\u200b\u5230\u200b\u7684\u200b\u6027\u80fd\u200b\u5c06\u200b\u4e0e\u200b\u60a8\u200b\u5b9e\u9645\u200b\u89c2\u5bdf\u200b\u5230\u200b\u7684\u200b\u6027\u80fd\u200b\u76f8\u6bd4\u200b\u592a\u8fc7\u200b\u4e50\u89c2\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u53ea\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u6b21\u200b\u4f7f\u7528\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5b83\u200b\u4ecd\u7136\u200b\u662f\u200b\u8861\u91cf\u200b\u5206\u7c7b\u5668\u200b\u6cdb\u5316\u200bgeneralization\u200b\u7684\u200b\u826f\u597d\u200b\u4ee3\u7406\u200bproxy \uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u6709\u5173\u200b\u6cdb\u5316\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u8ba8\u8bba\u200b\uff09\u3002</p> <p>\u200b\u4ec5\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u6b21\u200b\u8bc4\u4f30\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> <p>\u200b\u5e78\u8fd0\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u6709\u200b\u4e00\u79cd\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u63a5\u89e6\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002\u200b\u601d\u8def\u200b\u662f\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u5206\u4e3a\u200b\u4e24\u200b\u90e8\u5206\u200b\uff1a\u200b\u7a0d\u200b\u5c0f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6211\u4eec\u200b\u79f0\u4e4b\u4e3a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u90e8\u5206\u200b\u3002\u200b\u4ee5\u200bCIFAR-10\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f8b\u5982\u200b\u4f7f\u7528\u200b49,000\u200b\u5f20\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5c06\u200b1,000\u200b\u5f20\u7559\u4f5c\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5047\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u5728\u200bCIFAR-10\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before\n# recall Xtr_rows is 50,000 x 3072 matrix\nXval_rows = Xtr_rows[:1000, :] # take first 1000 for validation\nYval = Ytr[:1000]\nXtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for train\nYtr = Ytr[1000:]\n\n# find hyperparameters that work best on the validation set\nvalidation_accuracies = []\nfor k in [1, 3, 5, 10, 20, 50, 100]:\n\n  # use a particular value of k and evaluation on validation data\n  nn = NearestNeighbor()\n  nn.train(Xtr_rows, Ytr)\n  # here we assume a modified NearestNeighbor class that can take a k as input\n  Yval_predict = nn.predict(Xval_rows, k = k)\n  acc = np.mean(Yval_predict == Yval)\n  print 'accuracy: %f' % (acc,)\n\n  # keep track of what works on the validation set\n  validation_accuracies.append((k, acc))\n</code></pre> <p>\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u7ed3\u675f\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ed8\u5236\u200b\u4e00\u4e2a\u200b\u56fe\u8868\u200b\uff0c\u200b\u663e\u793a\u200b\u54ea\u4e9b\u200bk\u200b\u503c\u200b\u6548\u679c\u200b\u6700\u597d\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u8fdb\u884c\u200b\u6700\u7ec8\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u4ec5\u200b\u5728\u200b\u5b9e\u9645\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8fd0\u884c\u200b\u4e00\u6b21\u200b\u5e76\u200b\u62a5\u544a\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u3002\u200b\u4f7f\u7528\u200b\u9a8c\u8bc1\u200b\u96c6\u6765\u200b\u8c03\u6574\u200b\u6240\u6709\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8fd0\u884c\u200b\u4e00\u6b21\u200b\u5e76\u200b\u62a5\u544a\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200bCross-validation \u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u56e0\u6b64\u200b\u4e5f\u200b\u662f\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\uff09\u200b\u53ef\u80fd\u200b\u5f88\u5c0f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4eba\u4eec\u200b\u6709\u65f6\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u4e00\u79cd\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u8c03\u6574\u200b\u6280\u672f\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u4ee5\u200b\u524d\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u7684\u200b\u601d\u60f3\u200b\u662f\u200b\uff0c\u200b\u4e0d\u662f\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u524d\u200b1000\u200b\u4e2a\u200b\u6570\u636e\u200b\u70b9\u200b\u4f5c\u4e3a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\uff0c\u200b\u5269\u4e0b\u200b\u7684\u200b\u4f5c\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u800c\u662f\u200b\u901a\u8fc7\u200b\u8fed\u4ee3\u200b\u4e0d\u540c\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u96c6\u6765\u200b\u66f4\u597d\u200b\u5730\u200b\u4f30\u8ba1\u200b\u67d0\u4e2a\u200bk\u200b\u503c\u200b\u7684\u200b\u6027\u80fd\u200b\uff0c\u200b\u51cf\u5c11\u200b\u566a\u58f0\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b5\u200b\u6298\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u4e2d\u200b(5-fold cross-validation)\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5206\u4e3a\u200b5\u200b\u4e2a\u200b\u76f8\u7b49\u200b\u7684\u200b\u6298\u53e0\u200bfold\uff0c\u200b\u4f7f\u7528\u200b\u5176\u4e2d\u200b4\u200b\u4e2a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c1\u200b\u4e2a\u200b\u8fdb\u884c\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8fed\u4ee3\u200b\u54ea\u4e2a\u200b\u6298\u53e0\u200b\u662f\u200b\u9a8c\u8bc1\u200b\u6298\u53e0\u200bfold\uff0c\u200b\u8bc4\u4f30\u200b\u6027\u80fd\u200b\uff0c\u200b\u6700\u540e\u200b\u5728\u200b\u4e0d\u540c\u200b\u6298\u53e0\u200bfold\u200b\u4e4b\u95f4\u200b\u5e73\u5747\u200b\u6027\u80fd\u200b\u3002</p> <p> </p> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u53c2\u6570\u200bk\u200b\u7684\u200b5\u200b\u6298\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u8fd0\u884c\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200bk\u200b\u503c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b4\u200b\u4e2a\u200b\u6298\u53e0\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u7b2c\u200b5\u200b\u4e2a\u200b\u6298\u53e0\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200bk\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5f97\u5230\u200b5\u200b\u4e2a\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u6298\u53e0\u200b\u4e0a\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\uff08\u200b\u51c6\u786e\u6027\u200b\u662f\u200by\u200b\u8f74\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7ed3\u679c\u200b\u90fd\u200b\u662f\u200b\u4e00\u4e2a\u70b9\u200b\uff09\u3002\u200b\u8d8b\u52bf\u200b\u7ebf\u200b\u901a\u8fc7\u200b\u6bcf\u4e2a\u200bk\u200b\u7684\u200b\u7ed3\u679c\u200b\u5e73\u5747\u503c\u200b\u7ed8\u5236\u200b\uff0c\u200b\u8bef\u5dee\u200b\u6761\u200b\u8868\u793a\u200b\u6807\u51c6\u5dee\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u5efa\u8bae\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u7279\u5b9a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4f7f\u7528\u200b\u7ea6\u200bk = 7\u200b\u7684\u200b\u503c\u200b\u6548\u679c\u200b\u6700\u597d\u200b\uff08\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u56fe\u200b\u4e2d\u200b\u7684\u200b\u5cf0\u503c\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u4e86\u200b\u8d85\u8fc7\u200b5\u200b\u4e2a\u200b\u6298\u53e0\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5e73\u6ed1\u200b\uff08\u200b\u5373\u200b\u66f4\u5c11\u200b\u566a\u58f0\u200b\uff09\u200b\u7684\u200b\u66f2\u7ebf\u200b\u3002</p> <p>\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4eba\u4eec\u200b\u66f4\u200b\u559c\u6b22\u200b\u907f\u514d\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\uff0c\u200b\u800c\u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u5355\u4e00\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u62c6\u5206\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u53ef\u80fd\u200b\u8ba1\u7b97\u6210\u672c\u200b\u8f83\u200b\u9ad8\u200b\u3002\u200b\u4eba\u4eec\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u7684\u200b\u62c6\u5206\u200b\u65b9\u5f0f\u200b\u662f\u200b\u5c06\u200b50%-90%\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5269\u4f59\u200b\u90e8\u5206\u200b\u7528\u4e8e\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u200b\u53d6\u51b3\u4e8e\u200b\u591a\u4e2a\u200b\u56e0\u7d20\u200b\uff1a\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u6570\u91cf\u200b\u5f88\u5927\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u66f4\u200b\u559c\u6b22\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u62c6\u5206\u200b\u3002\u200b\u5982\u679c\u200b\u9a8c\u8bc1\u200b\u96c6\u4e2d\u200b\u7684\u200b\u793a\u4f8b\u200b\u6570\u91cf\u200b\u5f88\u5c0f\u200b\uff08\u200b\u53ef\u80fd\u200b\u4ec5\u200b\u6709\u200b\u51e0\u767e\u4e2a\u200b\uff09\uff0c\u200b\u5219\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u66f4\u200b\u5b89\u5168\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u6298\u53e0\u200b\u6570\u91cf\u200b\u53ef\u80fd\u200b\u662f\u200b3\u200b\u6298\u200b\u30015\u200b\u6298\u200b\u6216\u200b10\u200b\u6298\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u3002</p> <p> </p> <p>\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u636e\u200b\u62c6\u5206\u200b\u65b9\u5f0f\u200b\u3002\u200b\u7ed9\u5b9a\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u4e00\u4e2a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u88ab\u200b\u5206\u6210\u200b\u82e5\u5e72\u200b\u6298\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6709\u200b5\u200b\u6298\u200b\uff09\u3002\u200b\u6298\u200b1-4\u200b\u6210\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002\u200b\u4e00\u6298\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7684\u200b\u9ec4\u8272\u200b\u6298\u200b5\uff09\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u9a8c\u8bc1\u200b\u6298\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8c03\u6574\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u66f4\u8fdb\u4e00\u6b65\u200b\uff0c\u200b\u8fed\u4ee3\u200b\u9009\u62e9\u200b\u54ea\u4e2a\u200b\u6298\u662f\u200b\u9a8c\u8bc1\u200b\u6298\u200b\uff0c\u200b\u5206\u522b\u200b\u4ece\u200b1\u200b\u5230\u200b5\u3002\u200b\u8fd9\u200b\u5c06\u200b\u88ab\u200b\u79f0\u4e3a\u200b5\u200b\u6298\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u5728\u200b\u6700\u540e\u200b\uff0c\u200b\u4e00\u65e6\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u5e76\u200b\u786e\u5b9a\u200b\u4e86\u200b\u6240\u6709\u200b\u6700\u4f73\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u6a21\u578b\u200b\u4f1a\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u6027\u200b\u7684\u200b\u8bc4\u4f30\u200b\uff08\u200b\u7ea2\u8272\u200b\uff09\u3002</p> <p></p> <p>\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u4f18\u7f3a\u70b9\u200b</p> <p>\u200b\u503c\u5f97\u200b\u8003\u8651\u4e00\u4e0b\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u4e00\u4e9b\u200b\u4f18\u70b9\u200b\u548c\u200b\u7f3a\u70b9\u200b\u3002\u200b\u663e\u7136\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u4f18\u70b9\u200b\u662f\u200b\u5b83\u200b\u975e\u5e38\u7b80\u5355\u200b\u6613\u61c2\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8be5\u200b\u5206\u7c7b\u5668\u200b\u65e0\u9700\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u53ea\u662f\u200b\u5b58\u50a8\u200b\u548c\u200b\u53ef\u80fd\u200b\u7d22\u5f15\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u6211\u4eec\u200b\u8981\u200b\u4ed8\u51fa\u200b\u8ba1\u7b97\u6210\u672c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u9700\u8981\u200b\u4e0e\u200b\u6bcf\u4e2a\u200b\u8bad\u7ec3\u6837\u672c\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002\u200b\u8fd9\u662f\u200b\u76f8\u53cd\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u66f4\u200b\u5173\u5fc3\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u7684\u200b\u6548\u7387\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u6548\u7387\u200b\u3002\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u540e\u9762\u200b\u5f00\u53d1\u200b\u7684\u200b\u6df1\u5ea6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c06\u200b\u8fd9\u79cd\u200b\u6743\u8861\u200b\u63a8\u5411\u200b\u4e86\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6781\u7aef\u200b\uff1a\u200b\u5b83\u4eec\u200b\u8bad\u7ec3\u200b\u6210\u672c\u200b\u5f88\u200b\u9ad8\u200b\uff0c\u200b\u4f46\u200b\u4e00\u65e6\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\uff0c\u200b\u5bf9\u200b\u65b0\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u975e\u5e38\u200b\u4fbf\u5b9c\u200b\u3002\u200b\u8fd9\u79cd\u200b\u64cd\u4f5c\u200b\u6a21\u5f0f\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u66f4\u52a0\u200b\u53ef\u53d6\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\uff0c\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u8ba1\u7b97\u200b\u590d\u6742\u6027\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6d3b\u8dc3\u200b\u7684\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\uff0c\u200b\u5b58\u5728\u200b\u591a\u79cd\u200b\u8fd1\u4f3c\u200b\u6700\u8fd1\u200b\u90bb\u200bApproximate Nearest Neighbor\uff08ANN\uff09\u200b\u7b97\u6cd5\u200b\u548c\u200b\u5e93\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u52a0\u901f\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u6700\u8fd1\u200b\u90bb\u200b\u67e5\u627e\u200b\uff08\u200b\u4f8b\u5982\u200bFLANN\uff09\u3002\u200b\u8fd9\u4e9b\u200b\u7b97\u6cd5\u200b\u5141\u8bb8\u200b\u5728\u200b\u6700\u8fd1\u200b\u90bb\u200b\u68c0\u7d22\u200b\u7684\u200b\u6b63\u786e\u6027\u200b\u4e0e\u200b\u68c0\u7d22\u200b\u671f\u95f4\u200b\u7684\u200b\u7a7a\u95f4\u200b/\u200b\u65f6\u95f4\u200b\u590d\u6742\u6027\u200b\u4e4b\u95f4\u200b\u8fdb\u884c\u200b\u6743\u8861\u200b\uff0c\u200b\u5e76\u200b\u901a\u5e38\u200b\u4f9d\u8d56\u4e8e\u200b\u4e00\u4e2a\u200b\u6d89\u53ca\u200b\u6784\u5efa\u200bkdtree\u200b\u6216\u200b\u8fd0\u884c\u200bk\u200b\u5747\u503c\u200b\u7b97\u6cd5\u200b\u7684\u200b\u9884\u5904\u7406\u200b/\u200b\u7d22\u5f15\u200b\u9636\u6bb5\u200b\u3002</p> <p>\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u9519\u200b\u7684\u200b\u9009\u62e9\u200b\uff08\u200b\u7279\u522b\u200b\u662f\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u7ef4\u5ea6\u200b\u8f83\u200b\u4f4e\u200b\uff09\uff0c\u200b\u4f46\u200b\u5728\u200b\u5b9e\u9645\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u73af\u5883\u200b\u4e2d\u200b\u5f88\u5c11\u200b\u5408\u9002\u200b\u3002\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u662f\u200b\u56fe\u50cf\u200b\u662f\u200b\u9ad8\u7ef4\u200b\u5bf9\u8c61\u200b\uff08\u200b\u5373\u200b\u5b83\u4eec\u200b\u901a\u5e38\u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u50cf\u7d20\u200b\uff09\uff0c\u200b\u9ad8\u200b\u7ef4\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u8ddd\u79bb\u200b\u53ef\u80fd\u200b\u975e\u5e38\u200b\u4e0d\u200b\u76f4\u89c2\u200bcounter-intuitive\u3002\u200b\u4e0b\u9762\u200b\u7684\u200b\u56fe\u50cf\u200b\u8bf4\u660e\u200b\u4e86\u200b\u57fa\u4e8e\u200b\u50cf\u7d20\u200b\u7684\u200bL2\u200b\u76f8\u4f3c\u6027\u200b\u4e0e\u200b\u611f\u77e5\u200b\u76f8\u4f3c\u6027\u200bperceptual similarities\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\uff1a</p> <p> </p> <p>\u200b\u57fa\u4e8e\u200b\u50cf\u7d20\u200b\u7684\u200b\u8ddd\u79bb\u200b\u5728\u200b\u9ad8\u7ef4\u200b\u6570\u636e\u200b\uff08\u200b\u5c24\u5176\u200b\u662f\u200b\u56fe\u50cf\u200b\uff09\u200b\u4e0a\u200b\u53ef\u80fd\u200b\u975e\u5e38\u200b\u4e0d\u200b\u76f4\u89c2\u200b\u3002\u200b\u5de6\u4fa7\u200b\u662f\u200b\u4e00\u4e2a\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\uff0c\u200b\u65c1\u8fb9\u200b\u662f\u200b\u53e6\u5916\u200b\u4e09\u4e2a\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5728\u200b\u57fa\u4e8e\u200b\u50cf\u7d20\u200b\u7684\u200bL2\u200b\u8ddd\u79bb\u200b\u4e0a\u200b\u90fd\u200b\u4e0e\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u7b49\u200b\u8ddd\u79bb\u200b\u3002\u200b\u663e\u7136\u200b\uff0c\u200b\u50cf\u7d20\u200b\u7ea7\u200b\u7684\u200b\u8ddd\u79bb\u200b\u4e0e\u200b\u611f\u77e5\u200bperceptual\u200b\u6216\u200b\u8bed\u4e49\u200bsemantic\u200b\u76f8\u4f3c\u6027\u200b\u6beb\u65e0\u5173\u7cfb\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u53ef\u89c6\u5316\u200b\u793a\u4f8b\u200b\uff0c\u200b\u65e8\u5728\u200b\u8bf4\u670d\u200b\u60a8\u200b\u4f7f\u7528\u200b\u50cf\u7d20\u200b\u5dee\u5f02\u200b\u6765\u200b\u6bd4\u8f83\u200b\u56fe\u50cf\u200b\u662f\u200b\u4e0d\u200b\u8db3\u591f\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e00\u79cd\u200b\u540d\u4e3a\u200bt-SNE\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u6280\u672f\u200b\uff0c\u200b\u5c06\u200bCIFAR-10\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200b\u5230\u200b\u4e8c\u7ef4\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6700\u5927\u200b\u7a0b\u5ea6\u200b\u5730\u200b\u4fdd\u7559\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\uff08\u200b\u5c40\u90e8\u200b\uff09\u200b\u6210\u200b\u5bf9\u200b\u8ddd\u79bb\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u53ef\u89c6\u5316\u200b\u4e2d\u200b\uff0c\u200b\u9644\u8fd1\u200b\u663e\u793a\u200b\u7684\u200b\u56fe\u50cf\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u5f00\u53d1\u200b\u7684\u200bL2\u200b\u50cf\u7d20\u200b\u8ddd\u79bb\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\uff1a</p> <p> </p> <p>\u200b\u4f7f\u7528\u200bt-SNE\u200b\u5c06\u200bCIFAR-10\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200b\u5230\u200b\u4e8c\u7ef4\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u56fe\u50cf\u200b\u4e2d\u200b\uff0c\u200b\u6839\u636e\u200bL2\u200b\u50cf\u7d20\u200b\u8ddd\u79bb\u200b\u9644\u8fd1\u200b\u663e\u793a\u200b\u7684\u200b\u56fe\u50cf\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u63a5\u8fd1\u200b\u7684\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u80cc\u666f\u200b\u6bd4\u200b\u8bed\u4e49\u200b\u7c7b\u522b\u200b\u5dee\u5f02\u200b\u66f4\u5177\u200b\u663e\u8457\u6027\u200b\u3002\u200b\u70b9\u51fb\u200b\u8fd9\u91cc\u200b\u67e5\u770b\u200b\u66f4\u5927\u200b\u7248\u672c\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u3002</p> <p>\u200b\u7279\u522b\u200b\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u9644\u8fd1\u200b\u7684\u200b\u56fe\u50cf\u200b\u66f4\u591a\u5730\u200b\u53d6\u51b3\u4e8e\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e00\u822c\u200b\u989c\u8272\u200b\u5206\u5e03\u200b\u6216\u200b\u80cc\u666f\u200b\u7c7b\u578b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5b83\u4eec\u200b\u7684\u200b\u8bed\u4e49\u200b\u8eab\u4efd\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u53ea\u200b\u72d7\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u4e0e\u200b\u4e00\u53ea\u200b\u9752\u86d9\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u78b0\u5de7\u5728\u200b\u767d\u8272\u200b\u80cc\u666f\u200b\u4e0a\u200b\u3002\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6240\u6709\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u56fe\u50cf\u200b\u90fd\u200b\u5f62\u6210\u200b\u81ea\u5df1\u200b\u7684\u200b\u805a\u7c7b\u200b\uff0c\u200b\u8fd9\u6837\u200b\u76f8\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u56fe\u50cf\u200b\u53ef\u4ee5\u200b\u5f7c\u6b64\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u53d7\u200b\u4e0d\u200b\u76f8\u5173\u200b\u7279\u5f81\u200b\u548c\u200b\u53d8\u5316\u200b\uff08\u200b\u4f8b\u5982\u200b\u80cc\u666f\u200b\uff09\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8981\u200b\u5b9e\u73b0\u200b\u8fd9\u4e00\u200b\u5c5e\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8d85\u8d8a\u200b\u539f\u59cb\u200b\u50cf\u7d20\u200bgo beyond raw pixels\u3002</p> <p></p>"},{"location":"courses/neural_network/classification/#summary","title":"\u603b\u7ed3\u200b Summary","text":"<p>\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6211\u4eec\u200b\u88ab\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u7ec4\u200b\u5e26\u6709\u200b\u5355\u4e00\u200b\u7c7b\u522b\u200b\u6807\u7b7e\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u88ab\u200b\u8981\u6c42\u200b\u4e3a\u200b\u4e00\u7ec4\u200b\u65b0\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u8fd9\u4e9b\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5e76\u200b\u6d4b\u91cf\u200b\u9884\u6d4b\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u8fd9\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u6709\u200b\u591a\u4e2a\u200b\u4e0e\u200b\u4e4b\u200b\u5173\u8054\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u4f8b\u5982\u200bk\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u6216\u200b\u7528\u4e8e\u200b\u6bd4\u8f83\u200b\u793a\u4f8b\u200b\u7684\u200b\u8ddd\u79bb\u200b\u7c7b\u578b\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u6ca1\u6709\u200b\u660e\u663e\u200b\u7684\u200b\u9009\u62e9\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u8bbe\u7f6e\u200b\u8fd9\u4e9b\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u6b63\u786e\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5206\u6210\u200b\u4e24\u200b\u90e8\u5206\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u4e00\u4e2a\u200b\u6211\u4eec\u200b\u79f0\u4e4b\u4e3a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u4f2a\u200b\u6d4b\u8bd5\u200b\u96c6\u200bfake test se\u3002\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u503c\u200b\uff0c\u200b\u5e76\u200b\u4fdd\u7559\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u503c\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u62c5\u5fc3\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0d\u8db3\u200b\uff0c\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u4e00\u79cd\u200b\u79f0\u4e3a\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u51cf\u5c11\u200b \u200b\u4f30\u8ba1\u200b\u54ea\u4e9b\u200b\u8d85\u200b\u53c2\u6570\u200b\u6700\u200b\u6709\u6548\u200b\u7684\u200b \u200b\u566a\u58f0\u200b\u3002</li> <li>\u200b\u4e00\u65e6\u200b\u627e\u5230\u200b\u6700\u4f73\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u56fa\u5b9a\u200b\u5b83\u4eec\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5b9e\u9645\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u6267\u884c\u200b\u5355\u4e00\u200b\u7684\u200b\u8bc4\u4f30\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u6700\u8fd1\u200b\u90bb\u200b\u65b9\u6cd5\u200b\u5728\u200bCIFAR-10\u200b\u4e0a\u200b\u53ef\u4ee5\u200b\u8fbe\u5230\u200b\u5927\u7ea6\u200b40%\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u3002\u200b\u5b83\u200b\u7b80\u5355\u200b\u6613\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4f46\u200b\u8981\u6c42\u200b\u6211\u4eec\u200b\u5b58\u50a8\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u800c\u4e14\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u7684\u200b\u4ee3\u4ef7\u200b\u5f88\u200b\u9ad8\u200b\u3002</li> <li>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4f7f\u7528\u200bL1\u200b\u6216\u200bL2\u200b\u8ddd\u79bb\u200b\u6765\u200b\u8861\u91cf\u200b\u539f\u59cb\u200b\u50cf\u7d20\u200b\u503c\u200b\u662f\u200b\u4e0d\u200b\u8db3\u591f\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u4e9b\u200b\u8ddd\u79bb\u200b\u4e0e\u200b\u56fe\u50cf\u200b\u7684\u200b\u80cc\u666f\u200b\u548c\u200b\u989c\u8272\u200b\u5206\u5e03\u200b\u7684\u200b\u76f8\u5173\u6027\u200b\u66f4\u5f3a\u200b\uff0c\u200b\u800c\u200b\u4e0e\u200b\u5b83\u4eec\u200b\u7684\u200b\u8bed\u4e49\u200b\u5185\u5bb9\u200b\u76f8\u5173\u6027\u200b\u8f83\u5f31\u200b\u3002</li> </ul> <p>\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7740\u624b\u200b\u89e3\u51b3\u200b\u8fd9\u4e9b\u200b\u6311\u6218\u200b\uff0c\u200b\u6700\u7ec8\u200b\u627e\u5230\u200b\u53ef\u4ee5\u200b\u8fbe\u5230\u200b90%\u200b\u51c6\u786e\u6027\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff0c\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u5728\u200b\u5b66\u4e60\u200b\u5b8c\u6210\u200b\u540e\u200b\u5b8c\u5168\u200b\u4e22\u5f03\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0d\u5230\u200b\u4e00\u200b\u6beb\u79d2\u200b\u7684\u200b\u65f6\u95f4\u200b\u5185\u200b\u8bc4\u4f30\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u3002</p> <p></p>"},{"location":"courses/neural_network/classification/#knn","title":"\u603b\u7ed3\u200b\uff1a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u5e94\u7528\u200bkNN","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u5e94\u7528\u200bkNN\uff08\u200b\u5e0c\u671b\u200b\u4e0d\u662f\u200b\u5728\u200b\u56fe\u50cf\u200b\u4e0a\u200b\uff0c\u200b\u6216\u8005\u200b\u4ec5\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u57fa\u51c6\u200bbaseline\uff09\uff0c\u200b\u8bf7\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u8fdb\u884c\u200b\uff1a</p> <ol> <li>\u200b\u9884\u5904\u7406\u200b\u6570\u636e\u200b\uff1a\u200b\u5c06\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u7279\u5f81\u200b\uff08\u200b\u4f8b\u5982\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u50cf\u7d20\u200b\uff09\u200b\u6807\u51c6\u5316\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u5177\u6709\u200b\u96f6\u200b\u5747\u503c\u200b\u548c\u200b\u5355\u4f4d\u200b\u65b9\u5dee\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u4e2d\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u5730\u200b\u4ecb\u7ecd\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5e76\u200b\u9009\u62e9\u200b\u4e0d\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u6570\u636e\u200b\u6807\u51c6\u5316\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u50cf\u7d20\u200b\u901a\u5e38\u200b\u662f\u200b\u5747\u5300\u200b\u7684\u200b\uff0c\u200b\u4e0d\u200b\u5177\u6709\u200b\u5e7f\u6cdb\u200b\u4e0d\u540c\u200b\u7684\u200b\u5206\u5e03\u200b\uff0c\u200b\u4ece\u800c\u200b\u51cf\u8f7b\u200b\u4e86\u200b\u6570\u636e\u200b\u6807\u51c6\u5316\u200b\u7684\u200b\u9700\u8981\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u9ad8\u7ef4\u200b\u7684\u200b\uff0c\u200b\u8bf7\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b\u964d\u7ef4\u200b\u6280\u672f\u200b\uff0c\u200b\u5982\u200bPCA\uff08\u200b\u4e3b\u200b\u6210\u5206\u200b\u5206\u6790\u200bwiki ref, CS229ref, blog ref\uff09\uff0cNCA\uff08\u200b\u90bb\u57df\u200b\u6210\u5206\u200b\u5206\u6790\u200bwiki ref, blog ref\uff09\u200b\u6216\u200b\u751a\u81f3\u200b\u968f\u673a\u200b\u6295\u5f71\u200bRandom Projections\u3002</li> <li>\u200b\u968f\u673a\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u62c6\u200b\u5206\u6210\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u3002\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7ea6\u200b70-90%\u200b\u7684\u200b\u6570\u636e\u200b\u901a\u5e38\u200b\u5206\u914d\u200b\u7ed9\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u8bbe\u7f6e\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u6709\u200b\u591a\u5c11\u200b\u8d85\u200b\u53c2\u6570\u200b\u4ee5\u53ca\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5b83\u4eec\u200b\u5177\u6709\u200b\u591a\u5927\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u5982\u679c\u200b\u9700\u8981\u200b\u4f30\u8ba1\u200b\u8bb8\u591a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5e94\u8be5\u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u5177\u6709\u200b\u8f83\u5927\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\uff0c\u200b\u4ee5\u200b\u6709\u6548\u200b\u5730\u200b\u4f30\u8ba1\u200b\u5b83\u4eec\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u62c5\u5fc3\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6700\u597d\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u62c6\u200b\u5206\u4e3a\u200b\u591a\u4e2a\u200b\u6298\u53e0\u200b\u5e76\u200b\u6267\u884c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u8ba1\u7b97\u200b\u9884\u7b97\u200b\uff0c\u200b\u6700\u597d\u200b\u59cb\u7ec8\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\uff08\u200b\u6298\u53e0\u200b\u8d8a\u591a\u8d8a\u597d\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u8d8a\u200b\u6602\u8d35\u200b\uff09\u3002</li> <li>\u200b\u4e3a\u200b\u591a\u4e2a\u200bk\u200b\u503c\u200b\uff08\u200b\u8d8a\u591a\u8d8a\u597d\u200b\uff09\u200b\u4ee5\u53ca\u200b\u4e0d\u540c\u200b\u7684\u200b\u8ddd\u79bb\u200b\u7c7b\u578b\u200b\uff08L1\u200b\u548c\u200bL2\u200b\u662f\u200b\u4e0d\u9519\u200b\u7684\u200b\u9009\u62e9\u200b\uff09\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200bkNN\u200b\u5206\u7c7b\u5668\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200bkNN\u200b\u5206\u7c7b\u5668\u200b\u8fd0\u884c\u200b\u65f6\u95f4\u200b\u592a\u200b\u957f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b\u8fd1\u4f3c\u200b\u6700\u8fd1\u200b\u90bb\u5e93\u200b\uff08\u200b\u4f8b\u5982\u200bFLANN\uff09\u200b\u6765\u200b\u52a0\u901f\u200b\u68c0\u7d22\u200b\uff08\u200b\u4ee5\u200b\u4e00\u4e9b\u200b\u51c6\u786e\u6027\u200b\u4e3a\u200b\u4ee3\u4ef7\u200b\uff09\u3002</li> <li>\u200b\u6ce8\u610f\u200b\u8bb0\u5f55\u200b\u7ed9\u51fa\u200b\u6700\u4f73\u200b\u7ed3\u679c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u5173\u4e8e\u200b\u662f\u5426\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u6700\u4f73\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u5b8c\u6574\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5982\u679c\u200b\u5c06\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\u5408\u5e76\u200b\u5230\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\uff0c\u200b\u6700\u4f73\u200b\u8d85\u200b\u53c2\u6570\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6570\u636e\u200b\u7684\u200b\u5927\u5c0f\u200b\u4f1a\u200b\u66f4\u200b\u5927\u200b\uff09\u3002\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u6700\u597d\u200b\u4e0d\u200b\u5728\u200b\u6700\u7ec8\u200b\u5206\u7c7b\u5668\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u9a8c\u8bc1\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u89c6\u4e3a\u200b\u4f30\u8ba1\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u8017\u5c3d\u200bburned \u3002\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u3002\u200b\u62a5\u544a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\uff0c\u200b\u5e76\u200b\u5ba3\u5e03\u200b\u8fd9\u4e2a\u200b\u7ed3\u679c\u200b\u662f\u200bkNN\u200b\u5206\u7c7b\u5668\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</li> </ol> <p></p>"},{"location":"courses/neural_network/classification/#_2","title":"\u8fdb\u4e00\u6b65\u200b\u9605\u8bfb","text":"<p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\uff08\u200b\u53ef\u9009\u200b\u7684\u200b\uff09\u200b\u94fe\u63a5\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bf9\u200b\u8fdb\u4e00\u6b65\u200b\u9605\u8bfb\u200b\u611f\u5174\u8da3\u200b\uff1a</p> <ul> <li> <p>\u200b\u6709\u5173\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u4e9b\u200b\u6709\u7528\u200b\u4fe1\u606f\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u7b2c\u200b6\u200b\u8282\u200b\u76f8\u5173\u200b\uff0c\u200b\u4f46\u200b\u6574\u7bc7\u200b\u8bba\u6587\u200b\u90fd\u200b\u5f3a\u70c8\u63a8\u8350\u200b\u9605\u8bfb\u200b\u3002</p> </li> <li> <p>\u200b\u8bc6\u522b\u200b\u548c\u200b\u5b66\u4e60\u200b\u7269\u4f53\u200b\u7c7b\u522b\u200b\uff0c\u200b\u8fd9\u662f\u200b2005\u200b\u5e74\u200bICCV\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u7269\u4f53\u200b\u5206\u7c7b\u200b\u7684\u200b\u77ed\u671f\u200b\u8bfe\u7a0b\u200b\u3002</p> </li> </ul>"},{"location":"courses/neural_network/linear-classify/","title":"\u7ebf\u6027\u200b\u5206\u7c7b","text":""},{"location":"courses/neural_network/linear-classify/#_1","title":"\u7ebf\u6027\u200b\u5206\u7c7b","text":"<p>\u200b\u5728\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u8fd9\u200b\u662f\u4ece\u200b\u56fa\u5b9a\u200b\u7c7b\u522b\u200b\u96c6\u4e2d\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5206\u914d\u200b\u5355\u4e00\u200b\u6807\u7b7e\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u63cf\u8ff0\u200b\u4e86\u200bk-\u200b\u6700\u8fd1\u200b\u90bb\u200b\uff08kNN\uff09\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u8be5\u200b\u5206\u7c7b\u5668\u200b\u901a\u8fc7\u200b\u5c06\u200b\u56fe\u50cf\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\uff08\u200b\u5e26\u200b\u6ce8\u91ca\u200b\u7684\u200b\uff09\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u6765\u200b\u4e3a\u200b\u5b83\u4eec\u200b\u8d34\u4e0a\u6807\u7b7e\u200b\u3002\u200b\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u89c1\u200b\uff0ckNN\u200b\u6709\u200b\u5f88\u591a\u200b\u7f3a\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u5206\u7c7b\u5668\u200b\u5fc5\u987b\u200b\u8bb0\u4f4f\u200b\u6240\u6709\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5b58\u50a8\u200b\u4ee5\u4f9b\u200b\u5c06\u6765\u200b\u4e0e\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002\u200b\u8fd9\u200b\u5728\u200b\u7a7a\u95f4\u200b\u4e0a\u200b\u662f\u200b\u4f4e\u6548\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5927\u5c0f\u200b\u53ef\u80fd\u200b\u8f7b\u677e\u200b\u8fbe\u5230\u200b\u6570\u5341\u4e2a\u200bG\u3002</li> <li>\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u662f\u200b\u6602\u8d35\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u9700\u8981\u200b\u4e0e\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</li> </ul> <p>\u200b\u6982\u8ff0\u200b\u3002\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5c06\u200b\u5f00\u53d1\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5f3a\u5927\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u6700\u7ec8\u200b\u5c06\u200b\u81ea\u7136\u200b\u5730\u200b\u6269\u5c55\u200b\u5230\u200b\u6574\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u6709\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u8bc4\u5206\u200b\u51fd\u6570\u200bscore function\uff0c\u200b\u5c06\u200b\u539f\u59cb\u6570\u636e\u200b\u6620\u5c04\u200b\u5230\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff0c\u200b\u548c\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200bloss function\uff0c\u200b\u91cf\u5316\u200b\u9884\u6d4b\u200b\u5206\u6570\u200b\u4e0e\u200b\u771f\u503c\u200b\u6807\u7b7e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e00\u81f4\u6027\u200bagreement\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6b64\u200b\u95ee\u9898\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4f18\u5316\u200b\u95ee\u9898\u200b\uff0c\u200b\u5728\u200b\u8be5\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u8bc4\u5206\u200b\u51fd\u6570\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_2","title":"\u53c2\u6570\u200b\u5316\u200b\u7684\u200b\u56fe\u50cf\u200b\u5230\u200b\u6807\u7b7e\u200b\u5206\u6570\u200b\u7684\u200b\u6620\u5c04","text":"<p>\u200b\u6b64\u200b\u65b9\u6cd5\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u7ec4\u6210\u90e8\u5206\u200b\u662f\u200b\u5b9a\u4e49\u200b\u8bc4\u5206\u200b\u51fd\u6570\u200bscore function\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u5c06\u200b\u56fe\u50cf\u200b\u7684\u200b\u50cf\u7d20\u200b\u503c\u200b\u6620\u5c04\u200b\u5230\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u7f6e\u4fe1\u5ea6\u200b\u8bc4\u5206\u200bconfidence scores\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u5177\u4f53\u200b\u793a\u4f8b\u200b\u6765\u200b\u5c55\u793a\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u3002\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6709\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u56fe\u50cf\u200b \\(x_i \\in R^D\\)\uff0c\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u90fd\u200b\u4e0e\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b \\(y_i\\) \u200b\u76f8\u5173\u200b\u3002\u200b\u8fd9\u91cc\u200b \\(i = 1 \\dots N\\) \u200b\u4e14\u200b \\(y_i \\in 1 \\dots K\\)\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b N \u200b\u4e2a\u200b\u6837\u672c\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u4e3a\u200b D\uff09\u200b\u548c\u200b K \u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b CIFAR-10 \u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b N = 50,000 \u200b\u7684\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u6709\u200b D = 32 x 32 x 3 = 3072 \u200b\u4e2a\u200b\u50cf\u7d20\u200b\uff0c\u200b\u548c\u200b K = 10\uff0c\u200b\u56e0\u4e3a\u200b\u6709\u200b10\u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\uff08\u200b\u5982\u72d7\u200b\u3001\u200b\u732b\u200b\u3001\u200b\u6c7d\u8f66\u200b\u7b49\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5c06\u200b\u5b9a\u4e49\u200b\u8bc4\u5206\u200b\u51fd\u6570\u200b \\(f: R^D \\mapsto R^K\\)\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u5c06\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u6620\u5c04\u200b\u5230\u200b\u7c7b\u200b\u8bc4\u5206\u200bclass scores\u3002</p> <p>\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u3002 \u200b\u5728\u200b\u8fd9\u4e2a\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u53ef\u80fd\u200b\u662f\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u51fd\u6570\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\uff1a</p> \\[ f(x_i, W, b) =  W x_i + b \\] <p>\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u7b49\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u56fe\u50cf\u200b \\(x_i\\) \u200b\u7684\u200b\u6240\u6709\u200b\u50cf\u7d20\u200b\u90fd\u200b\u5c55\u5e73\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b [D x 1] \u200b\u7684\u200b\u5355\u5217\u200b\u5411\u91cf\u200b\u3002\u200b\u77e9\u9635\u200b W\uff08\u200b\u5927\u5c0f\u200b\u4e3a\u200b [K x D]\uff09\uff0c\u200b\u548c\u200b\u5411\u91cf\u200b b\uff08\u200b\u5927\u5c0f\u200b\u4e3a\u200b [K x 1]\uff09\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u7684\u200b\u53c2\u6570\u200bparameters\u3002\u200b\u5728\u200b CIFAR-10 \u200b\u4e2d\u200b\uff0c\\(x_i\\) \u200b\u5305\u542b\u200b\u7b2c\u200b i \u200b\u5f20\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u6240\u6709\u200b\u5c55\u5e73\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b [3072 x 1] \u200b\u5217\u200b\u7684\u200b\u50cf\u7d20\u200b\uff0cW \u200b\u662f\u200b [10 x 3072]\uff0cb \u200b\u662f\u200b [10 x 1]\uff0c\u200b\u56e0\u6b64\u200b3072\u200b\u4e2a\u200b\u6570\u5b57\u200b\u8f93\u5165\u200b\u51fd\u6570\u200b\uff08\u200b\u539f\u59cb\u200b\u50cf\u7d20\u200b\u503c\u200b\uff09\uff0c\u200b\u5e76\u200b\u8f93\u51fa\u200b10\u200b\u4e2a\u200b\u6570\u5b57\u200b\uff08\u200b\u7c7b\u200b\u5f97\u5206\u200b\uff09\u3002\u200b\u5728\u200b W \u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u6743\u91cd\u200bweights\uff0c\u200b\u800c\u200b b \u200b\u88ab\u200b\u79f0\u4e3a\u200b\u504f\u7f6e\u200b\u5411\u91cf\u200bbias vector\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4f1a\u200b\u5f71\u54cd\u200b\u8f93\u51fa\u200b\u5f97\u5206\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u6570\u636e\u200b \\(x_i\\) \u200b\u4ea4\u4e92\u200b\u3002\u200b\u4f46\u200b\u4f60\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u542c\u5230\u200b\u4eba\u4eec\u200b\u4ea4\u66ff\u200b\u4f7f\u7528\u6743\u200b\u91cd\u200b\u548c\u200b\u53c2\u6570\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u672f\u8bed\u200b\u3002</p> <p>\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u51e0\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u9996\u5148\u200b\uff0c\u200b\u6ce8\u610f\u200b\u5355\u4e2a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b \\(W x_i\\) \u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u5e76\u884c\u200b\u8bc4\u4f30\u200b10\u200b\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u4e00\u4e2a\u200b\uff09\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u662f\u200b W \u200b\u7684\u200b\u4e00\u884c\u200b\u3002</li> <li>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u8f93\u5165\u200b\u6570\u636e\u200b \\((x_i, y_i)\\) \u200b\u662f\u200b\u7ed9\u5b9a\u200b\u548c\u200b\u56fa\u5b9a\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u63a7\u5236\u53c2\u6570\u200b W,b \u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u4ee5\u200b\u8fd9\u6837\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8bbe\u7f6e\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u7684\u200b\u5f97\u5206\u200b\u5728\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u4e0e\u200b\u771f\u503c\u200b\u6807\u7b7e\u200b\u5339\u914d\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u8be6\u7ec6\u200b\u8ba8\u8bba\u200b\u5982\u4f55\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f46\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6b63\u786e\u200b\u7684\u200b\u7c7b\u522b\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f97\u5206\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5f97\u5206\u200b\u9ad8\u4e8e\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5f97\u5206\u200b\u3002</li> <li>\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4e00\u4e2a\u200b\u4f18\u70b9\u200b\u662f\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7528\u4e8e\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b W,b\uff0c\u200b\u4f46\u200b\u4e00\u65e6\u200b\u5b66\u4e60\u200b\u5b8c\u6210\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4e22\u5f03\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u53ea\u200b\u4fdd\u7559\u200b\u5b66\u5230\u200b\u7684\u200b\u53c2\u6570\u200b\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u65b0\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u53ef\u4ee5\u200b\u7b80\u5355\u200b\u5730\u200b\u901a\u8fc7\u200b\u8be5\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u8f6c\u53d1\u200bforward\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u8ba1\u7b97\u200b\u5f97\u5206\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u3002</li> <li>\u200b\u6700\u540e\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u6d89\u53ca\u200b\u5355\u4e2a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u548c\u200b\u52a0\u6cd5\u200b\uff0c\u200b\u8fd9\u6bd4\u200b\u5c06\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u4e0e\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u8981\u200b\u5feb\u5f97\u591a\u200b\u3002</li> </ul> <p>\u200b\u4f0f\u7b14\u200bforeshadowing:\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c06\u4f1a\u200b\u50cf\u200b\u4e0a\u9762\u200b\u6240\u200b\u5c55\u793a\u200b\u7684\u200b\u90a3\u6837\u200b\u5c06\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u6620\u5c04\u200b\u5230\u200b\u5206\u6570\u200b\uff0c\u200b\u4f46\u200b\u6620\u5c04\u200b\uff08f\uff09\u200b\u5c06\u4f1a\u200b\u66f4\u52a0\u200b\u590d\u6742\u200b\u4e14\u200b\u5305\u542b\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_3","title":"\u89e3\u91ca\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668","text":"<p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u8ba1\u7b97\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5f97\u5206\u200b\u662f\u200b\u901a\u8fc7\u200b\u5176\u200b\u6240\u6709\u200b3\u200b\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u4e0a\u200b\u7684\u200b\u6240\u6709\u200b\u50cf\u7d20\u200b\u503c\u200b\u7684\u200b\u52a0\u6743\u200b\u548c\u200b\u6765\u200b\u5b8c\u6210\u200b\u7684\u200b\u3002\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u4e3a\u200b\u8fd9\u4e9b\u200b\u6743\u91cd\u200b\u8bbe\u5b9a\u200b\u7684\u200b\u5177\u4f53\u200b\u503c\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u6709\u200b\u80fd\u529b\u200b\u559c\u6b22\u200b\u6216\u200b\u4e0d\u200b\u559c\u6b22\u200b\uff08\u200b\u53d6\u51b3\u4e8e\u200b\u6bcf\u4e2a\u200b\u6743\u91cd\u200b\u7684\u200b\u7b26\u53f7\u200b\uff09\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u67d0\u4e2a\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u67d0\u79cd\u200b\u989c\u8272\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u60f3\u8c61\u200b\u5982\u679c\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e24\u4fa7\u200b\u6709\u200b\u5f88\u591a\u200b\u84dd\u8272\u200b\uff08\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u5bf9\u5e94\u200b\u4e8e\u6c34\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u201c\u200b\u8239\u200b\u201d\u200b\u7c7b\u200b\u53ef\u80fd\u200b\u66f4\u200b\u6709\u200b\u53ef\u80fd\u200b\u88ab\u200b\u5206\u7c7b\u200b\u3002\u200b\u90a3\u4e48\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u671f\u671b\u200b\u201c\u200b\u8239\u200b\u201d\u200b\u5206\u7c7b\u5668\u200b\u5728\u200b\u5176\u200b\u84dd\u8272\u200b\u901a\u9053\u200b\u6743\u91cd\u200b\u4e0a\u200b\u6709\u200b\u5f88\u591a\u200b\u6b63\u200b\u6743\u91cd\u200b\uff08\u200b\u84dd\u8272\u200b\u7684\u200b\u5b58\u5728\u200b\u589e\u52a0\u200b\u4e86\u200b\u8239\u200b\u7684\u200b\u5f97\u5206\u200b\uff09\uff0c\u200b\u800c\u200b\u5728\u200b\u7ea2\u8272\u200b/\u200b\u7eff\u8272\u901a\u9053\u200b\u4e0a\u200b\u6709\u200b\u8d1f\u200b\u6743\u91cd\u200b\uff08\u200b\u7ea2\u8272\u200b/\u200b\u7eff\u8272\u200b\u7684\u200b\u5b58\u5728\u200b\u964d\u4f4e\u200b\u4e86\u200b\u8239\u200b\u7684\u200b\u5f97\u5206\u200b\uff09\u3002</p> <p> </p> <p>\u200b\u4e00\u4e2a\u200b\u5c06\u200b\u56fe\u50cf\u200b\u6620\u5c04\u200b\u5230\u200b\u7c7b\u200b\u5f97\u5206\u200b\u7684\u200b\u4f8b\u5b50\u200b\u3002\u200b\u4e3a\u4e86\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u56fe\u50cf\u200b\u53ea\u6709\u200b4\u200b\u4e2a\u200b\u50cf\u7d20\u200b\uff084\u200b\u4e2a\u200b\u5355\u8272\u200b\u50cf\u7d20\u200b\uff0c\u200b\u4e3a\u200b\u7b80\u6d01\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u6b64\u200b\u793a\u4f8b\u200b\u4e2d\u200b\u6ca1\u6709\u200b\u8003\u8651\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u6709\u200b3\u200b\u4e2a\u7c7b\u200b\uff08\u200b\u7ea2\u8272\u200b\uff08\u200b\u732b\u200b\uff09\u3001\u200b\u7eff\u8272\u200b\uff08\u200b\u72d7\u200b\uff09\u3001\u200b\u84dd\u8272\u200b\uff08\u200b\u8239\u200b\uff09\u200b\u7c7b\u200b\uff09\u3002\uff08\u200b\u6f84\u6e05\u200b\uff1a\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u7684\u200b\u989c\u8272\u200b\u4ec5\u200b\u8868\u793a\u200b3\u200b\u4e2a\u7c7b\u200b\uff0c\u200b\u4e0e\u200bRGB\u200b\u901a\u9053\u200b\u65e0\u5173\u200b\u3002\uff09\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u62c9\u4f38\u200b\u6210\u200b\u4e00\u5217\u200b\u5e76\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5f97\u5206\u200b\u3002\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u7279\u5b9a\u200b\u7684\u200b\u6743\u91cd\u200b\u96c6\u200bW\u200b\u4e00\u70b9\u200b\u4e5f\u200b\u4e0d\u597d\u200b\uff1a\u200b\u6743\u91cd\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b\u732b\u200b\u56fe\u50cf\u200b\u5206\u914d\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u4f4e\u200b\u7684\u200b\u732b\u200b\u5f97\u5206\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u7ec4\u200b\u6743\u91cd\u200b\u4f3c\u4e4e\u200b\u786e\u4fe1\u200b\u5b83\u200b\u770b\u5230\u200b\u7684\u200b\u662f\u200b\u4e00\u53ea\u200b\u72d7\u200b\u3002</p> <p>\u200b\u5c06\u200b\u56fe\u50cf\u200b\u89c6\u4e3a\u200b\u9ad8\u7ef4\u200b\u70b9\u200bhigh-dimensional points\u200b\u7684\u200b\u7c7b\u6bd4\u200banalogy\u3002 \u200b\u7531\u4e8e\u200b\u56fe\u50cf\u200b\u88ab\u200b\u5c55\u5f00\u200b\u6210\u200b\u9ad8\u7ef4\u200b\u5217\u200b\u5411\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u8fd9\u4e2a\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u70b9\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cCIFAR-10\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u662f\u200b32x32x3\u200b\u50cf\u7d20\u200b\u7684\u200b3072\u200b\u7ef4\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u70b9\u200b\uff09\u3002\u200b\u7c7b\u4f3c\u200b\u5730\u200b\uff0c\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u4e00\u7ec4\u200b\uff08\u200b\u5e26\u200b\u6807\u7b7e\u200b\u7684\u200b\uff09\u200b\u70b9\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5f97\u200b\u5206\u4e3a\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u7684\u200b\u52a0\u6743\u200b\u548c\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5f97\u5206\u200b\u90fd\u200b\u662f\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u7a7a\u95f4\u200b\u4e0a\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u4e0d\u80fd\u200b\u53ef\u89c6\u5316\u200b3072\u200b\u7ef4\u200b\u7684\u200b\u7a7a\u95f4\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8c61\u200b\u5c06\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u7ef4\u5ea6\u200b\u538b\u7f29\u200b\u5230\u200b\u4ec5\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u53ef\u89c6\u5316\u200b\u5206\u7c7b\u5668\u200b\u53ef\u80fd\u200b\u5728\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff1a</p> <p> </p> <p>\u200b\u5728\u200b\u56fe\u50cf\u200b\u7a7a\u95f4\u200b\u7684\u200b\u5361\u901a\u200b\u8868\u793a\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u70b9\u200b\uff0c\u200b\u4e09\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u88ab\u200b\u53ef\u89c6\u5316\u200b\u3002\u200b\u4f7f\u7528\u200b\u6c7d\u8f66\u200b\u5206\u7c7b\u5668\u200b\uff08\u200b\u7ea2\u8272\u200b\uff09\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u7ea2\u7ebf\u200b\u663e\u793a\u200b\u4e86\u200b\u5728\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u6240\u6709\u200b\u5f97\u5230\u200b\u6c7d\u8f66\u200b\u7c7b\u5f97\u200b\u5206\u4e3a\u200b\u96f6\u200b\u7684\u200b\u70b9\u200b\u3002\u200b\u7ea2\u8272\u200b\u7bad\u5934\u200b\u663e\u793a\u200b\u4e86\u200b\u589e\u52a0\u200b\u7684\u200b\u65b9\u5411\u200b\uff0c\u200b\u6240\u4ee5\u200b\u7ea2\u7ebf\u200b\u53f3\u4fa7\u200b\u7684\u200b\u6240\u6709\u200b\u70b9\u200b\u90fd\u200b\u6709\u200b\u6b63\u200b\u7684\u200b\uff08\u200b\u5e76\u4e14\u200b\u7ebf\u6027\u200b\u589e\u52a0\u200b\uff09\u200b\u5f97\u5206\u200b\uff0c\u200b\u800c\u200b\u5de6\u4fa7\u200b\u7684\u200b\u6240\u6709\u200b\u70b9\u200b\u90fd\u200b\u6709\u200b\u8d1f\u200b\u7684\u200b\uff08\u200b\u5e76\u4e14\u200b\u7ebf\u6027\u200b\u51cf\u5c11\u200b\uff09\u200b\u5f97\u5206\u200b\u3002</p> <p>\u200b\u5982\u4e0a\u6240\u8ff0\u200b\uff0c\\(W\\) \u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u90fd\u200b\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u7684\u200b\u51e0\u4f55\u200b\u89e3\u91ca\u200b\u662f\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u6539\u53d8\u200b \\(W\\) \u200b\u7684\u200b\u5176\u4e2d\u200b\u4e00\u884c\u200b\u65f6\u200b\uff0c\u200b\u50cf\u7d20\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u76f8\u5e94\u200b\u7ebf\u6761\u200b\u4f1a\u671d\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u5411\u200b\u65cb\u8f6c\u200b\u3002\u200b\u53e6\u4e00\u65b9\u9762\u200b\uff0c\u200b\u504f\u7f6e\u200b \\(b\\) \u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u5e73\u79fb\u200b\u8fd9\u4e9b\u200b\u7ebf\u200b\u3002\u200b\u7279\u522b\u200b\u5730\u200b\uff0c\u200b\u6ce8\u610f\u200b\u6ca1\u6709\u200b\u504f\u7f6e\u200b\u9879\u200b\uff0c\u200b\u63d2\u5165\u200b \\(x_i = 0\\) \u200b\u603b\u662f\u200b\u4f1a\u200b\u5f97\u5230\u200b\u5f97\u200b\u5206\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u4e0d\u7ba1\u200b\u6743\u91cd\u200b\u5982\u4f55\u200b\uff0c\u200b\u6240\u6709\u200b\u7684\u200b\u7ebf\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u8feb\u200b\u7a7f\u8fc7\u200b\u539f\u70b9\u200b\u3002</p> <p>\u200b\u5c06\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6a21\u677f\u200b\u5339\u914d\u200btemplate matching\uff1a\u200b\u6743\u91cd\u200b \\(W\\) \u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u89e3\u91ca\u200b\u662f\u200b \\(W\\) \u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u6a21\u677f\u200b\uff08\u200b\u6216\u200b\u6709\u65f6\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u539f\u578b\u200bprototype\uff09\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u5185\u79ef\u200binner product\uff08\u200b\u6216\u70b9\u79ef\u200bdot product\uff09\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u6a21\u677f\u200b\u4e0e\u200b\u56fe\u50cf\u200b\u4e00\u4e00\u200b\u6bd4\u8f83\u200b\u6765\u200b\u83b7\u5f97\u200b\u56fe\u50cf\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u5f97\u5206\u200b\uff0c\u200b\u4ee5\u200b\u627e\u5230\u200b\u6700\u5408\u9002\u200b\u7684\u200b\u6a21\u677f\u200b\u3002\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u672f\u8bed\u200b\uff0c\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u6a21\u677f\u200b\u5339\u914d\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6a21\u677f\u200b\u662f\u200b\u5b66\u4e60\u200b\u5f97\u5230\u200b\u7684\u200b\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u601d\u8003\u200b\u65b9\u5f0f\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b9e\u9645\u4e0a\u200b\u4ecd\u7136\u200b\u6709\u6548\u200b\u5730\u200b\u8fdb\u884c\u200b\u6700\u8fd1\u200b\u90bb\u200b\u5339\u914d\u200b\uff0c\u200b\u4f46\u200b\u4e0e\u200b\u6709\u200b\u6210\u5343\u4e0a\u4e07\u200b\u7684\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5b66\u4e60\u200b\u5b83\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5b83\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u662f\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\uff08\u200b\u8d1f\u200b\uff09\u200b\u5185\u79ef\u200b\u4f5c\u4e3a\u200b\u8ddd\u79bb\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200bL1\u200b\u6216\u200bL2\u200b\u8ddd\u79bb\u200b\u3002</p> <p> </p> <p>CIFAR-10\u200b\u5b66\u4e60\u200b\u7ed3\u675f\u200b\u65f6\u200b\u7684\u200b\u793a\u4f8b\u200b\u5b66\u4e60\u200b\u6743\u91cd\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8239\u200b\u7684\u200b\u6a21\u677f\u200b\u5305\u542b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u84dd\u8272\u200b\u50cf\u7d20\u200b\uff0c\u200b\u6b63\u5982\u200b\u9884\u671f\u200b\u7684\u200b\u90a3\u6837\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e00\u65e6\u200b\u4e0e\u200b\u6d77\u4e0a\u200b\u8239\u53ea\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5185\u79ef\u200b\u5339\u914d\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6a21\u677f\u200b\u5c06\u4f1a\u200b\u83b7\u5f97\u200b\u9ad8\u5206\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u9a6c\u200b\u7684\u200b\u6a21\u677f\u200b\u4f3c\u4e4e\u200b\u5305\u542b\u200b\u4e86\u200b\u4e00\u5339\u200b\u53cc\u5934\u200b\u9a6c\u200b\uff0c\u200b\u8fd9\u662f\u200b\u7531\u4e8e\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u5b58\u5728\u200b\u5de6\u53f3\u200b\u671d\u5411\u200b\u7684\u200b\u9a6c\u200b\u3002\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u5c06\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u9a6c\u200b\u7684\u200b\u6a21\u5f0f\u200b\u5408\u5e76\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u6a21\u677f\u200b\u3002\u200b\u7c7b\u4f3c\u200b\u5730\u200b\uff0c\u200b\u6c7d\u8f66\u200b\u5206\u7c7b\u5668\u200b\u4f3c\u4e4e\u200b\u5df2\u200b\u5c06\u200b\u591a\u79cd\u200b\u6a21\u5f0f\u200b\u5408\u5e76\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u6a21\u677f\u200b\uff0c\u200b\u8be5\u200b\u6a21\u677f\u200b\u5fc5\u987b\u200b\u8bc6\u522b\u200b\u5404\u79cd\u200b\u89d2\u5ea6\u200b\u548c\u200b\u989c\u8272\u200b\u7684\u200b\u6c7d\u8f66\u200b\u3002\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6a21\u677f\u200b\u6700\u7ec8\u200b\u662f\u200b\u7ea2\u8272\u200b\u7684\u200b\uff0c\u200b\u8fd9\u200b\u6697\u793a\u200b\u4e86\u200bCIFAR-10\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7ea2\u8272\u200b\u6c7d\u8f66\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u989c\u8272\u200b\u7684\u200b\u6c7d\u8f66\u200b\u66f4\u200b\u591a\u200b\u3002\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u592a\u5f31\u200b\u4e86\u200b\uff0c\u200b\u4e0d\u80fd\u200b\u6b63\u786e\u5904\u7406\u200b\u4e0d\u540c\u200b\u989c\u8272\u200b\u7684\u200b\u6c7d\u8f66\u200b\uff0c\u200b\u4f46\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u540e\u9762\u200b\u5c06\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c06\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u6267\u884c\u200b\u8fd9\u4e2a\u200b\u4efb\u52a1\u200b\u3002\u200b\u7a0d\u5fae\u200b\u5c55\u671b\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c06\u200b\u80fd\u591f\u200b\u5728\u200b\u5176\u200b\u9690\u85cf\u200b\u5c42\u4e2d\u200b\u53d1\u5c55\u200b\u4e2d\u95f4\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u795e\u7ecf\u5143\u200b\u53ef\u4ee5\u200b\u68c0\u6d4b\u200b\u7279\u5b9a\u200b\u7c7b\u578b\u200b\u7684\u200b\u6c7d\u8f66\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u671d\u200b\u5de6\u200b\u7684\u200b\u7eff\u8272\u200b\u6c7d\u8f66\u200b\u3001\u200b\u6b63\u9762\u200b\u7684\u200b\u84dd\u8272\u200b\u6c7d\u8f66\u200b\u7b49\u200b\uff09\uff0c\u200b\u4e0b\u200b\u4e00\u5c42\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5404\u4e2a\u200b\u6c7d\u8f66\u200b\u68c0\u6d4b\u5668\u200b\u7684\u200b\u52a0\u6743\u200b\u6c42\u548c\u200b\u6765\u200b\u7ec4\u5408\u200b\u8fd9\u4e9b\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u4ece\u800c\u200b\u5f97\u5230\u200b\u66f4\u200b\u51c6\u786e\u200b\u7684\u200b\u6c7d\u8f66\u200b\u8bc4\u5206\u200b\u3002</p> <p>\u200b\u504f\u7f6e\u200bBias\u200b\u6280\u5de7\u200b\u3002 \u200b\u5728\u200b\u7ee7\u7eed\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u60f3\u200b\u63d0\u5230\u200b\u4e00\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u7b80\u5316\u200b\u6280\u5de7\u200b\uff0c\u200b\u5c06\u200b\u4e24\u4e2a\u200b\u53c2\u6570\u200b\\(W\\)\u200b\u548c\u200b\\(b\\)\u200b\u8868\u793a\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u3002\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u5f97\u5206\u200b\u51fd\u6570\u200b\u5982\u4e0b\u200b\uff1a</p> \\[ f(x_i, W, b) =  W x_i + b \\] <p>\u200b\u968f\u7740\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5355\u72ec\u200b\u8ddf\u8e2a\u200b\u4e24\u7ec4\u200b\u53c2\u6570\u200b\uff08\u200b\u504f\u7f6e\u200b\\(b\\)\u200b\u548c\u200b\u6743\u91cd\u200b\\(W\\)\uff09\u200b\u6709\u70b9\u200b\u7e41\u7410\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5e38\u7528\u200b\u7684\u200b\u6280\u5de7\u200b\u662f\u200b\u5c06\u200b\u8fd9\u200b\u4e24\u7ec4\u200b\u53c2\u6570\u200b\u5408\u5e76\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5728\u200b\u5411\u91cf\u200b\\(x_i\\)\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u59cb\u7ec8\u4fdd\u6301\u200b\u5e38\u6570\u200b\\(1\\) \u2014\u2014\u200b\u4e00\u4e2a\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u504f\u7f6e\u200b\u7ef4\u5ea6\u200bbias dimension\u3002\u200b\u6709\u200b\u4e86\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u65b0\u200b\u7684\u200b\u5f97\u5206\u200b\u51fd\u6570\u200b\u5c06\u200b\u7b80\u5316\u200b\u4e3a\u200b\u5355\u4e00\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff1a</p> \\[ f(x_i, W) =  W x_i \\] <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200bCIFAR-10\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\\(x_i\\)\u200b\u73b0\u5728\u200b\u662f\u200b[3073 x 1]\u200b\u800c\u200b\u4e0d\u662f\u200b[3072 x 1] \u2014\u2014\uff08\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u5305\u542b\u200b\u5e38\u6570\u200b1\uff09\uff0c\\(W\\)\u200b\u73b0\u5728\u200b\u662f\u200b[10 x 3073]\u200b\u800c\u200b\u4e0d\u662f\u200b[10 x 3072]\u3002\u200b\u73b0\u5728\u200b\u7684\u200b\\(W\\)\u200b\u77e9\u9635\u200b\u7684\u200b\u989d\u5916\u200b\u5217\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u504f\u5dee\u200b\\(b\\)\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u56fe\u200b\u53ef\u80fd\u200b\u6709\u52a9\u4e8e\u200b\u89e3\u91ca\u200b\uff1a</p> <p></p> <p>\u200b\u504f\u7f6e\u200b\u6280\u5de7\u200b\u7684\u200b\u793a\u610f\u56fe\u200b\u3002\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7136\u540e\u200b\u6dfb\u52a0\u200b\u504f\u7f6e\u200b\u5411\u91cf\u200b\uff08\u200b\u5de6\u4fa7\u200b\uff09\u200b\u7b49\u6548\u4e8e\u200b\u5411\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u5411\u91cf\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u5e38\u6570\u200b\u4e3a\u200b1\u200b\u7684\u200b\u504f\u7f6e\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u6269\u5c55\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u4e3a\u200b1\u200b\u5217\u200b \u2014\u2014 \u200b\u4e00\u4e2a\u200b\u504f\u7f6e\u200b\u5217\u200b\uff08\u200b\u53f3\u4fa7\u200b\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5c06\u200b\u6240\u6709\u200b\u5411\u91cf\u200b\u9644\u52a0\u200b\u5230\u200b1\u200b\u6765\u200b\u9884\u5904\u7406\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\u4e00\u4e2a\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5305\u542b\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u7f6e\u200b\u7684\u200b\u4e24\u4e2a\u200b\u77e9\u9635\u200b\u3002</p> <p>\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u3002 \u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u8bf4\u660e\u200b\uff0c\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200b\u539f\u59cb\u200b\u50cf\u7d20\u200b\u503c\u200b\uff08\u200b\u8303\u56f4\u200b\u4ece\u200b[0...255]\uff09\u3002\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u79cd\u200b\u975e\u5e38\u200b\u5e38\u89c1\u200b\u7684\u200b\u505a\u6cd5\u200b\u662f\u200b\u59cb\u7ec8\u200b\u5bf9\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u8fdb\u884c\u200b\u6807\u51c6\u5316\u200b\u5904\u7406\u200b\uff08\u200b\u5728\u200b\u56fe\u50cf\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u50cf\u7d20\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u4e00\u4e2a\u200b\u7279\u5f81\u200b\uff09\u3002\u200b\u7279\u522b\u200b\u5730\u200b\uff0c\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u901a\u8fc7\u200b\u51cf\u53bb\u200b\u6bcf\u4e2a\u200b\u7279\u5f81\u200b\u7684\u200b\u5747\u503c\u200b\u6765\u200b\u5c45\u4e2d\u200b\u6570\u636e\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u56fe\u50cf\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u76f8\u5f53\u4e8e\u200b\u8ba1\u7b97\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u5747\u503c\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u4ece\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u51cf\u53bb\u200b\u5b83\u200b\uff0c\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u50cf\u7d20\u200b\u8303\u56f4\u200b\u5927\u7ea6\u200b\u4e3a\u200b[-127...127]\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002\u200b\u66f4\u200b\u5e38\u89c1\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u662f\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u8fdb\u884c\u200b\u7f29\u653e\u200b\uff0c\u200b\u4f7f\u200b\u5176\u503c\u200b\u8303\u56f4\u200b\u5728\u200b[-1, 1]\u200b\u4e4b\u95f4\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c\u200b\u96f6\u200b\u5747\u503c\u200b\u4e2d\u5fc3\u5316\u200b\u53ef\u80fd\u200b\u66f4\u200b\u91cd\u8981\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u6211\u4eec\u200b\u7406\u89e3\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u7684\u200b\u52a8\u6001\u200b\uff08dynamics of gradient descent\uff09\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7b49\u5f85\u200b\u5bf9\u200b\u5b83\u200b\u7684\u200b\u6b63\u5f53\u7406\u7531\u200b\uff08justification \uff09\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_4","title":"\u635f\u5931\u200b\u51fd\u6570","text":"<p>\u200b\u5728\u200b\u524d\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4ece\u200b\u50cf\u7d20\u200b\u503c\u5230\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u5176\u200b\u53c2\u6570\u200b\u5316\u4e3a\u200b\u4e00\u7ec4\u200b\u6743\u91cd\u200b\\(W\\)\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u5bf9\u200b\u6570\u636e\u200b\\((x_i, y_i)\\)\u200b\u6ca1\u6709\u200b\u63a7\u5236\u6743\u200b\uff08\u200b\u5b83\u200b\u662f\u200b\u56fa\u5b9a\u200b\u7684\u200b\u4e14\u200b\u5df2\u200b\u7ed9\u5b9a\u200b\uff09\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u6743\u91cd\u200b\u6709\u200b\u63a7\u5236\u6743\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u8bbe\u7f6e\u200b\u5b83\u4eec\u200b\u4ee5\u200b\u4f7f\u200b\u9884\u6d4b\u200b\u7684\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u56de\u5230\u200b\u732b\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u53ca\u5176\u200b\u7c7b\u522b\u200b\u201c\u200b\u732b\u200b\u201d\u3001\u201c\u200b\u72d7\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u8239\u200b\u201d\u200b\u7684\u200b\u5206\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u8be5\u200b\u793a\u4f8b\u200b\u4e2d\u200b\u7684\u200b\u6743\u91cd\u200b\u96c6\u5408\u200b\u5b9e\u9645\u4e0a\u200b\u4e0d\u592a\u597d\u200b\uff1a\u200b\u6211\u4eec\u200b\u8f93\u5165\u200b\u4e86\u200b\u63cf\u8ff0\u200b\u732b\u200b\u7684\u200b\u50cf\u7d20\u200b\uff0c\u200b\u4f46\u200b\u732b\u200b\u7684\u200b\u5206\u6570\u200b\u975e\u5e38\u4f4e\u200b\uff08-96.8\uff09\uff0c\u200b\u800c\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u7c7b\u522b\u200b\u76f8\u6bd4\u200b\uff08\u200b\u72d7\u200b\u5206\u6570\u200b437.9\u200b\u548c\u200b\u8239\u200b\u5206\u6570\u200b61.95\uff09\uff0c\u200b\u5206\u6570\u200b\u5f88\u200b\u4f4e\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u7528\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200bloss function\uff08\u200b\u6709\u65f6\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u6210\u672c\u200b\u51fd\u6570\u200bcost function\u200b\u6216\u200b\u76ee\u6807\u200b\u51fd\u6570\u200bobjective\uff09\u200b\u6765\u200b\u8861\u91cf\u200b\u6211\u4eec\u200b\u5bf9\u200b\u8fd9\u79cd\u200b\u7ed3\u679c\u200b\u7684\u200b\u4e0d\u6ee1\u200b\uff08unhappiness\uff09\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5728\u200b\u5206\u7c7b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u65b9\u9762\u200b\u8868\u73b0\u200b\u4e0d\u4f73\u200b\uff0c\u200b\u635f\u5931\u200b\u5c06\u200b\u5f88\u200b\u9ad8\u200b\uff0c\u200b\u5982\u679c\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\uff0c\u200b\u635f\u5931\u200b\u5c06\u200b\u5f88\u200b\u4f4e\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_5","title":"\u591a\u200b\u7c7b\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\u635f\u5931","text":"<p>\u200b\u6709\u200b\u591a\u79cd\u200b\u65b9\u5f0f\u200b\u6765\u200b\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u8be6\u7ec6\u200b\u7ec6\u8282\u200b\u3002\u200b\u4f5c\u4e3a\u200b\u7b2c\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u4ecb\u7ecd\u200b\u4e00\u79cd\u200b\u5e38\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u591a\u200b\u7c7b\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200bMulticlass Support Vector Machine\uff08SVM\uff09\u200b\u635f\u5931\u200b\u3002SVM\u200b\u635f\u5931\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u662f\u200b\uff0cSVM\u201c\u200b\u5e0c\u671b\u200b\u201d\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u7684\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u5177\u6709\u200b\u6bd4\u200b\u4e00\u4e9b\u200b\u56fa\u5b9a\u200b\u95f4\u9694\u200bfixed margin\\(\\Delta\\)\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u5206\u6570\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6709\u65f6\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u62df\u4eba\u5316\u200b\u662f\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff1aSVM\u201c\u200b\u5e0c\u671b\u200b\u201d\u200b\u67d0\u79cd\u200b\u7ed3\u679c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8be5\u200b\u7ed3\u679c\u200b\u5c06\u200b\u4ea7\u751f\u200b\u8f83\u200b\u4f4e\u200b\u7684\u200b\u635f\u5931\u200b\uff08\u200b\u8fd9\u662f\u200b\u597d\u4e8b\u200b\uff09\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u66f4\u52a0\u200b\u7cbe\u786e\u200b\u3002\u200b\u56de\u60f3\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u88ab\u200b\u7ed9\u51fa\u200b\u56fe\u50cf\u200b\\(x_i\\)\u200b\u7684\u200b\u50cf\u7d20\u200b\u548c\u200b\u6307\u5b9a\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u6807\u7b7e\u200b\\(y_i\\)\uff0c\u200b\u8be5\u200b\u6807\u7b7e\u200b\u6307\u5b9a\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002\u200b\u5f97\u5206\u200b\u51fd\u6570\u200b\u83b7\u53d6\u200b\u50cf\u7d20\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u5411\u91cf\u200b\\(f(x_i, W)\\)\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u7f29\u5199\u200b\u4e3a\u200b\\(s\\)\uff08\u200b\u8868\u793a\u200b\u5206\u6570\u200b\uff09\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7b2c\u200bj\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u662f\u200bj\u200b\u4e2a\u200b\u5143\u7d20\u200b\uff1a\\(s_j = f(x_i, W)_j\\)\u3002\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u591a\u7c7b\u200bSVM\u200b\u635f\u5931\u200b\u5982\u4e0b\u200b\u5f62\u5f0f\u5316\u200b\uff1a</p> \\[ L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta) \\] <p>\u200b\u793a\u4f8b\u200b\uff1a \u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u6765\u200b\u89e3\u91ca\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\u3002\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6709\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u5b83\u4eec\u200b\u63a5\u6536\u200b\u5230\u200b\u5206\u6570\u200b\\(s = [13, -7, 11]\\)\uff0c\u200b\u5e76\u4e14\u200b\u7b2c\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\u662f\u200b\u771f\u5b9e\u200b\u7c7b\u522b\u200b\uff08\u200b\u5373\u200b\\(y_i = 0\\)\uff09\u3002\u200b\u8fd8\u200b\u5047\u8bbe\u200b\\(\\Delta\\)\uff08\u200b\u7a0d\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u8be6\u7ec6\u200b\u8ba8\u8bba\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff09\u200b\u4e3a\u200b10\u3002\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u5bf9\u200b\u6240\u6709\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u7c7b\u522b\u200b\uff08\\(j \\neq y_i\\)\uff09\u200b\u6c42\u548c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u4e24\u9879\u200b\uff1a</p> \\[ L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10) \\] <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u7b2c\u4e00\u9879\u200b\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u56e0\u4e3a\u200b[-7 - 13 + 10]\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u8d1f\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\\(max(0,-)\\)\u200b\u51fd\u6570\u200b\u5c06\u200b\u5176\u200b\u9608\u503c\u200b\u5316\u4e3a\u200b\u96f6\u200b\u3002\u200b\u6211\u4eec\u200b\u5bf9\u200b\u8fd9\u200b\u4e00\u5bf9\u200b\u7684\u200b\u635f\u5931\u200b\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\uff0813\uff09\u200b\u6bd4\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\uff08-7\uff09\u200b\u9ad8\u51fa\u200b\u81f3\u5c11\u200b10\u3002\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u5dee\u8ddd\u200b\u4e3a\u200b20\uff0c\u200b\u8fdc\u8fdc\u200b\u5927\u4e8e\u200b10\uff0c\u200b\u4f46\u200bSVM\u200b\u53ea\u200b\u5173\u5fc3\u200b\u5dee\u8ddd\u200b\u81f3\u5c11\u200b\u4e3a\u200b10\uff1b\u200b\u4efb\u4f55\u200b\u8d85\u8fc7\u200b\u95f4\u9694\u200bmargin\u200b\u7684\u200b\u989d\u5916\u200b\u5dee\u8ddd\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u200b\\(max\\)\u200b\u64cd\u4f5c\u200b\u622a\u65ad\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u7b2c\u4e8c\u9879\u200b\u8ba1\u7b97\u200b\u7684\u200b\u662f\u200b[11 - 13 + 10]\uff0c\u200b\u5b83\u200b\u7b49\u4e8e\u200b8\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\uff0813 &gt; 11\uff09\u200b\u9ad8\u4e8e\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u6ca1\u6709\u200b\u8fbe\u5230\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u95f4\u9694\u200b10\u3002\u200b\u5dee\u8ddd\u200b\u53ea\u6709\u200b2\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u635f\u5931\u200b\u4e3a\u200b8\u200b\u7684\u200b\u539f\u56e0\u200b\uff08\u200b\u5373\u200b\u6ee1\u8db3\u200b\u95f4\u9694\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5dee\u8ddd\u200b\u66f4\u5927\u200b\uff09\u3002\u200b\u603b\u4e4b\u200b\uff0cSVM\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5e0c\u671b\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\\(y_i\\)\u200b\u7684\u200b\u5206\u6570\u200b\u6bd4\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u81f3\u5c11\u200b\u9ad8\u51fa\u200b\\(\\Delta\\)\uff08delta\uff09\u200b\u7684\u200b\u5206\u6570\u200b\u3002\u200b\u5982\u679c\u200b\u60c5\u51b5\u200b\u4e0d\u662f\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7d2f\u79ef\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u672c\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7ebf\u6027\u200b\u5206\u6570\u200b\u51fd\u6570\u200b\uff08\\(f(x_i; W) =  W x_i\\) \uff09\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4ee5\u200b\u4ee5\u4e0b\u200b\u7b49\u6548\u200b\u5f62\u5f0f\u200b\u91cd\u65b0\u200b\u7f16\u5199\u200b\uff1a</p> \\[ L_i =\\sum_{j\\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta) \\] <p>\u200b\u5176\u4e2d\u200b\\(w_j\\)\u200b\u662f\u200b\u5c06\u200b\\(W\\)\u200b\u91cd\u65b0\u200b\u6574\u5f62\u200b\u4e3a\u200b\u5217\u540e\u200b\u7684\u200b\u7b2c\u200bj\u200b\u884c\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u8003\u8651\u200b\u66f4\u200b\u590d\u6742\u200b\u5f62\u5f0f\u200b\u7684\u200b\u5206\u6570\u200b\u51fd\u6570\u200b\\(f\\)\uff0c\u200b\u60c5\u51b5\u200b\u5c31\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u5982\u6b64\u200b\u3002</p> <p>\u200b\u5728\u200b\u5b8c\u6210\u200b\u672c\u200b\u8282\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u8981\u200b\u63d0\u5230\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u672f\u8bed\u200b\u662f\u200b\u96f6\u200b\u9608\u503c\u200bhreshold at zero \\(max(0,-)\\)\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u79f0\u4e3a\u200b\u5408\u9875\u200b\u635f\u5931\u200b\uff08hinge loss\uff09\u3002\u200b\u6709\u65f6\u5019\u200b\u4f60\u200b\u4f1a\u200b\u542c\u5230\u200b\u4eba\u4eec\u200b\u4f7f\u7528\u200b\u5e73\u65b9\u200b\u5408\u9875\u200b\u635f\u5931\u200bSVM\uff08\u200b\u6216\u200bL2-SVM\uff09\uff0c\u200b\u5b83\u200b\u4f7f\u7528\u200b\u5f62\u5f0f\u200b\\(max(0,-)^2\\)\uff0c\u200b\u66f4\u200b\u5f3a\u70c8\u200b\u5730\u200b\u60e9\u7f5a\u200bpenalize\u200b\u8fdd\u53cd\u200b\u95f4\u9694\u200b\u7684\u200b\u60c5\u51b5\u200b\uff08\u200b\u4e8c\u6b21\u200b\u800c\u200b\u4e0d\u662f\u200b\u7ebf\u6027\u200b\uff09\u3002\u200b\u672a\u200b\u5e73\u65b9\u200b\u7248\u672c\u200b\u66f4\u200b\u6807\u51c6\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u67d0\u4e9b\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\uff0c\u200b\u5e73\u65b9\u200b\u5408\u9875\u200b\u635f\u5931\u200b\u53ef\u80fd\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\u3002\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u786e\u5b9a\u200b\u3002</p> <p>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u91cf\u5316\u200b\u4e86\u200b\u6211\u4eec\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u96c6\u4e0a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u7684\u200b\u4e0d\u200b\u6ee1\u610f\u200b\u7a0b\u5ea6\u200b\u3002</p> <p> </p> <p>\u200b\u591a\u200b\u7c7b\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\uff08Multiclass Support Vector Machine\uff09\u200b\u5e0c\u671b\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u81f3\u5c11\u200b\u8981\u200b\u6bd4\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u9ad8\u51fa\u200b\u4e00\u4e2a\u200b\u95f4\u9694\u200bmargin\uff08delta\uff09\u3002\u200b\u5982\u679c\u200b\u4efb\u4f55\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u5728\u200b\u7ea2\u8272\u200b\u533a\u57df\u200b\u5185\u200b\uff08\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u5c06\u200b\u7d2f\u79ef\u200b\u635f\u5931\u200b\u3002\u200b\u5426\u5219\u200b\uff0c\u200b\u635f\u5931\u200b\u5c06\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u627e\u5230\u200b\u6743\u91cd\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u540c\u65f6\u200b\u6ee1\u8db3\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u7684\u200b\u8fd9\u200b\u4e00\u200b\u7ea6\u675f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f7f\u200b\u603b\u200b\u635f\u5931\u200b\u5c3d\u53ef\u80fd\u200b\u4f4e\u200b\u3002</p> <p>\u200b\u6b63\u5219\u200b\u5316\u200b\u3002\u200b\u4e0a\u9762\u200b\u6211\u4eec\u200b\u63d0\u51fa\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u3002\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u4e00\u7ec4\u200b\u53c2\u6570\u200bW\uff0c\u200b\u53ef\u4ee5\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u6bcf\u4e2a\u200b\u793a\u4f8b\u200b\uff08\u200b\u5373\u200b\uff0c\u200b\u6240\u6709\u200b\u5206\u6570\u200b\u90fd\u200b\u6ee1\u8db3\u200b\u6240\u6709\u200b\u95f4\u9694\u200b\u6761\u4ef6\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200bi\u200b\u90fd\u200b\u6709\u200b\\(L_i = 0\\)\uff09\u3002\u200b\u95ee\u9898\u200b\u5728\u4e8e\u200b\uff0c\u200b\u8fd9\u7ec4\u200bW\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u662f\u200b\u552f\u4e00\u200b\u7684\u200b\uff1a\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u8bb8\u591a\u200b\u7c7b\u4f3c\u200b\u7684\u200bW\uff0c\u200b\u53ef\u4ee5\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u793a\u4f8b\u200b\u3002\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e9b\u200b\u53c2\u6570\u200bW\u200b\u53ef\u4ee5\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u635f\u5931\u200b\u90fd\u200b\u4e3a\u200b\u96f6\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u4efb\u4f55\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u7684\u200b\u500d\u6570\u200b\\(\\lambda W\\)\uff08\u200b\u5176\u4e2d\u200b\\(\\lambda &gt; 1\\)\uff09\u200b\u4e5f\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u96f6\u200b\u635f\u5931\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u79cd\u200b\u53d8\u6362\u200b\u4f1a\u200b\u5747\u5300\u200b\u62c9\u4f38\u200b\u6240\u6709\u200b\u5206\u6570\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e5f\u200b\u4f1a\u200b\u62c9\u4f38\u200b\u5b83\u4eec\u200b\u7684\u200b\u7edd\u5bf9\u200b\u5dee\u5f02\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u548c\u200b\u6700\u8fd1\u200b\u7684\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5206\u6570\u200b\u5dee\u4e3a\u200b15\uff0c\u200b\u90a3\u4e48\u200b\u5c06\u200bW\u200b\u7684\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u4e58\u4ee5\u200b2\u200b\u5c06\u200b\u4f7f\u200b\u65b0\u200b\u7684\u200b\u5dee\u503c\u200b\u53d8\u4e3a\u200b30\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u67d0\u200b\u4e00\u7ec4\u200b\u6743\u91cd\u200b\\(W\\)\u200b\u7684\u200b\u67d0\u4e9b\u200b\u504f\u597d\u200b\u8fdb\u884c\u200b\u7f16\u7801\u200b\uff0c\u200b\u4ee5\u200b\u6d88\u9664\u200b\u8fd9\u79cd\u200b\u6a21\u7cca\u6027\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6269\u5c55\u200b\u4e3a\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u9879\u200bregularization penalty\\(R(W)\\)\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u662f\u200b\u5e73\u65b9\u200b\\(L2\\)\u200b\u8303\u6570\u200b\uff0c\u200b\u5b83\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u7684\u200b\u5143\u7d20\u200b\u4e8c\u6b21\u200b\u60e9\u7f5a\u200b\u6765\u200b\u963b\u6b62\u200b\u5927\u200b\u7684\u200b\u6743\u91cd\u200b\uff1a</p> \\[ R(W) = \\sum_k\\sum_l W_{k,l}^2 \\] <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5bf9\u200bW\u200b\u7684\u200b\u6240\u6709\u200b\u5e73\u65b9\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6b63\u5219\u200b\u5316\u200b\u51fd\u6570\u200b\u4e0d\u662f\u200b\u6570\u636e\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b83\u200b\u4ec5\u200b\u57fa\u4e8e\u200b\u6743\u91cd\u200b\u3002\u200b\u5305\u62ec\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u5b8c\u6210\u200b\u5b8c\u6574\u200b\u7684\u200b\u591a\u7c7b\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\u635f\u5931\u200b\uff0c\u200b\u5b83\u200b\u7531\u200b\u4e24\u4e2a\u200b\u7ec4\u6210\u90e8\u5206\u200b\u7ec4\u6210\u200b\uff1a\u200b\u6570\u636e\u200b\u635f\u5931\u200b\uff08\u200b\u5373\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u7684\u200b\u5e73\u5747\u200b\u635f\u5931\u200b\\(L_i\\)\uff09\u200b\u548c\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5b8c\u6574\u200b\u7684\u200b\u591a\u7c7b\u200bSVM\u200b\u635f\u5931\u200b\u53d8\u4e3a\u200b\uff1a</p> \\[ L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{\u200b\u6570\u636e\u200b\u635f\u5931\u200b} + \\underbrace{ \\lambda R(W) }_\\text{\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b} \\\\\\\\ \\] <p>\u200b\u6216\u8005\u200b\u5c55\u5f00\u200b\u4e3a\u200b\u5176\u200b\u5b8c\u6574\u200b\u5f62\u5f0f\u200b\uff1a</p> \\[   L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2 \\] <p>\u200b\u5176\u4e2d\u200b\\(N\\)\u200b\u662f\u200b\u8bad\u7ec3\u200b\u793a\u4f8b\u200b\u7684\u200b\u6570\u91cf\u200b\u3002\u200b\u6b63\u5982\u200b\u60a8\u200b\u6240\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u9644\u52a0\u200b\u5230\u200b\u635f\u5931\u200b\u76ee\u6807\u200b\u4e2d\u200b\uff0c\u200b\u7531\u8d85\u200b\u53c2\u6570\u200b\\(\\lambda\\)\u200b\u52a0\u6743\u200b\u3002\u200b\u6ca1\u6709\u200b\u7b80\u5355\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u8fd9\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u901a\u5e38\u200b\u901a\u8fc7\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u786e\u5b9a\u200b\u3002</p> <p>\u200b\u9664\u4e86\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\u52a8\u673a\u200b\u4e4b\u5916\u200b\uff0c\u200b\u5305\u62ec\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u4ee4\u4eba\u6ee1\u610f\u200b\u7684\u200b\u6027\u8d28\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u56de\u5230\u200b\u5176\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u6027\u8d28\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5305\u62ec\u200bL2\u200b\u60e9\u7f5a\u200b\u4f1a\u200b\u5bfc\u81f4\u200bSVM\u200b\u5177\u6709\u200b\u5438\u5f15\u200b\u4eba\u200b\u7684\u200b\u6700\u5927\u200b\u95f4\u9694\u200bmax margin\u200b\u5c5e\u6027\u200b\uff08\u200b\u5982\u679c\u200b\u60a8\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200bCS229\u200b\u8bb2\u4e49\u200b\u4e2d\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff09\u3002</p> <p>\u200b\u6700\u200b\u5438\u5f15\u200b\u4eba\u200b\u7684\u200b\u5c5e\u6027\u200b\u4e4b\u4e00\u200b\u662f\u200b\uff0c\u200b\u5bf9\u5927\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u60e9\u7f5a\u200b\u901a\u5e38\u200b\u6709\u52a9\u4e8e\u200b\u63d0\u9ad8\u200b\u6cdb\u5316\u200b\u6027\u80fd\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6ca1\u6709\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u53ef\u4ee5\u200b\u5355\u72ec\u200b\u5bf9\u200b\u5206\u6570\u200b\u4ea7\u751f\u200b\u975e\u5e38\u200b\u5927\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e9b\u200b\u8f93\u5165\u200b\u5411\u91cf\u200b\\(x = [1,1,1,1]\\)\u200b\u548c\u200b\u4e24\u4e2a\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\\(w_1 = [1,0,0,0]\\)\uff0c\\(w_2 = [0.25,0.25,0.25,0.25]\\)\u3002\u200b\u7136\u540e\u200b\\(w_1^Tx = w_2^Tx = 1\\)\uff0c\u200b\u6240\u4ee5\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u90fd\u200b\u5bfc\u81f4\u200b\u76f8\u540c\u200b\u7684\u200b\u70b9\u79ef\u200b\uff0c\u200b\u4f46\u200b\\(w_1\\)\u200b\u7684\u200bL2\u200b\u60e9\u7f5a\u200b\u662f\u200b1.0\uff0c\u200b\u800c\u200b\\(w_2\\)\u200b\u7684\u200bL2\u200b\u60e9\u7f5a\u200b\u4ec5\u4e3a\u200b0.5\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6839\u636e\u200bL2\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\\(w_2\\)\u200b\u5c06\u200b\u66f4\u200b\u53d7\u6b22\u8fce\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u66f4\u200b\u4f4e\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\\(w_2\\)\u200b\u4e2d\u200b\u7684\u200b\u6743\u91cd\u200b\u66f4\u200b\u5c0f\u200b\u3001\u200b\u66f4\u200b\u5206\u6563\u200b\u3002\u200b\u7531\u4e8e\u200bL2\u200b\u60e9\u7f5a\u200b\u66f4\u200b\u559c\u6b22\u200b\u8f83\u200b\u5c0f\u200b\u548c\u200b\u66f4\u200b\u5206\u6563\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6700\u7ec8\u200b\u5206\u7c7b\u5668\u200b\u88ab\u200b\u9f13\u52b1\u200b\u8003\u8651\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5c0f\u91cf\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5c11\u91cf\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u548c\u200b\u975e\u5e38\u200b\u5f3a\u70c8\u200b\u7684\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u6548\u5e94\u200b\u53ef\u4ee5\u200b\u63d0\u9ad8\u200b\u5206\u7c7b\u5668\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u7684\u200b\u6cdb\u5316\u200b\u6027\u80fd\u200b\uff0c\u200b\u5e76\u200b\u51cf\u5c11\u200b\u8fc7\u200b\u62df\u5408\u200boverfitting\u3002</p> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u504f\u7f6e\u200b\u6ca1\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4e0e\u200b\u6743\u91cd\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4e0d\u200b\u63a7\u5236\u200b\u8f93\u5165\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5f71\u54cd\u529b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u901a\u5e38\u200b\u53ea\u200b\u5bf9\u200b\u6743\u91cd\u200b\\(W\\)\u200b\u8fdb\u884c\u200b\u6b63\u5219\u200b\u5316\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u5bf9\u200b\u504f\u7f6e\u200b\\(b\\)\u200b\u8fdb\u884c\u200b\u6b63\u5219\u200b\u5316\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u901a\u5e38\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u5f71\u54cd\u200b\u4e0d\u200b\u5927\u200b\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u6211\u4eec\u200b\u6c38\u8fdc\u200b\u65e0\u6cd5\u200b\u5728\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u4e0a\u200b\u5b9e\u73b0\u200b\u5b8c\u5168\u200b\u4e3a\u200b\u96f6\u200b\u7684\u200b\u635f\u5931\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u53ea\u200b\u5728\u200b\\(W = 0\\)\u200b\u7684\u200b\u75c5\u6001\u200bpathological\u200b\u8bbe\u7f6e\u200b\u4e0b\u200b\u53ef\u80fd\u200b\u53d1\u751f\u200b\u3002</p> <p>\u200b\u4ee3\u7801\u200b:\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u5728\u200bPython\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u4e0d\u200b\u5305\u62ec\u200b\u6b63\u5219\u200b\u5316\u200b\uff09\uff0c\u200b\u5206\u522b\u200b\u4ee5\u975e\u200b\u5411\u200b\u91cf\u5316\u200b\u548c\u200b\u534a\u5411\u200b\u91cf\u5316\u200b\u5f62\u5f0f\u200b\uff1a</p> <pre><code>def L_i(x, y, W):\n\"\"\"\n  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n  \"\"\"\n  delta = 1.0 # see notes about delta later in this section\n  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n  correct_class_score = scores[y]\n  D = W.shape[0] # number of classes, e.g. 10\n  loss_i = 0.0\n  for j in range(D): # iterate over all wrong classes\n    if j == y:\n      # skip for the true class to only loop over incorrect classes\n      continue\n    # accumulate loss for the i-th example\n    loss_i += max(0, scores[j] - correct_class_score + delta)\n  return loss_i\n\ndef L_i_vectorized(x, y, W):\n\"\"\"\n  A faster half-vectorized implementation. half-vectorized\n  refers to the fact that for a single example the implementation contains\n  no for loops, but there is still one loop over the examples (outside this function)\n  \"\"\"\n  delta = 1.0\n  scores = W.dot(x)\n  # compute the margins for all classes in one vector operation\n  margins = np.maximum(0, scores - scores[y] + delta)\n  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n  # to ignore the y-th position and only consider margin on max wrong class\n  margins[y] = 0\n  loss_i = np.sum(margins)\n  return loss_i\n\ndef L(X, y, W):\n\"\"\"\n  fully-vectorized implementation :\n  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n  - y is array of integers specifying correct class (e.g. 50,000-D array)\n  - W are weights (e.g. 10 x 3073)\n  \"\"\"\n  # evaluate loss over all examples in X without using any for loops\n  # left as exercise to reader in the assignment\n</code></pre> <p>\u200b\u672c\u8282\u200b\u7684\u200b\u8981\u70b9\u200b\u662f\u200b\uff0c\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\uff08SVM\uff09\u200b\u635f\u5931\u200b\u91c7\u7528\u200b\u4e00\u79cd\u200b\u7279\u5b9a\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8861\u91cf\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u6807\u7b7e\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8fdb\u884c\u200b\u826f\u597d\u200b\u7684\u200b\u9884\u6d4b\u200b\u7b49\u540c\u4e8e\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u60f3\u200b\u529e\u6cd5\u200b\u627e\u5230\u200b\u80fd\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u7684\u200b\u6743\u91cd\u200b\u5373\u53ef\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_6","title":"\u5b9e\u9645\u200b\u8003\u8651\u200b\u56e0\u7d20","text":"<p>\u200b\u8bbe\u7f6e\u200bDelta\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5ffd\u7565\u200b\u4e86\u200b\u8d85\u200b\u53c2\u6570\u200b\\(\\Delta\\)\u200b\u53ca\u5176\u200b\u8bbe\u7f6e\u200b\u3002\u200b\u5b83\u200b\u5e94\u8be5\u200b\u8bbe\u7f6e\u200b\u4e3a\u4ec0\u4e48\u200b\u503c\u200b\uff0c\u200b\u662f\u5426\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\uff1f\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6240\u6709\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5b89\u5168\u200b\u5730\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\\(\\Delta = 1.0\\)\u3002\u200b\u8d85\u200b\u53c2\u6570\u200b\\(\\Delta\\)\u200b\u548c\u200b\\(\\lambda\\)\u200b\u4f3c\u4e4e\u200b\u662f\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u63a7\u5236\u200b\u76f8\u540c\u200b\u7684\u200b\u6743\u8861\u200b\uff1a\u200b\u76ee\u6807\u200b\u4e2d\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u548c\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6743\u8861\u200btradeoff\u3002\u200b\u7406\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u7684\u200b\u5173\u952e\u5728\u4e8e\u200b\u6743\u91cd\u200b\\(W\\)\u200b\u7684\u200b\u5927\u5c0f\u200b\u76f4\u63a5\u200b\u5f71\u54cd\u200b\u5206\u6570\u200b\uff08\u200b\u56e0\u6b64\u200b\u4e5f\u200b\u5f71\u54cd\u200b\u5b83\u4eec\u200b\u7684\u200b\u5dee\u5f02\u200b\uff09\uff1a\u200b\u5f53\u200b\u6211\u4eec\u200b\u7f29\u5c0f\u200b\\(W\\)\u200b\u5185\u200b\u7684\u200b\u6240\u6709\u200b\u503c\u65f6\u200b\uff0c\u200b\u5206\u6570\u200b\u5dee\u5f02\u200b\u5c06\u200b\u53d8\u5c0f\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u6269\u5927\u200b\u6743\u91cd\u200b\u65f6\u200b\uff0c\u200b\u6240\u6709\u200b\u5206\u6570\u200b\u5dee\u5f02\u200b\u90fd\u200b\u5c06\u200b\u53d8\u5927\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5206\u6570\u200b\u4e4b\u95f4\u200b\u7684\u200b\u786e\u5207\u200b\u5dee\u503c\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\\(\\Delta = 1\\)\u200b\u6216\u200b\\(\\Delta = 100\\)\uff09\u200b\u5728\u200b\u67d0\u79cd\u610f\u4e49\u200b\u4e0a\u200b\u662f\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6743\u91cd\u200b\u53ef\u4ee5\u200b\u4efb\u610f\u200b\u7f29\u5c0f\u200b\u6216\u200b\u62c9\u4f38\u200b\u5dee\u5f02\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u552f\u4e00\u200b\u7684\u200b\u771f\u6b63\u200b\u6743\u8861\u200b\u662f\u200b\u6211\u4eec\u200b\u5141\u8bb8\u200b\u6743\u91cd\u200b\u589e\u957f\u200b\u5230\u200b\u591a\u200b\u5927\u7a0b\u5ea6\u200b\uff08\u200b\u901a\u8fc7\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\\(\\lambda\\)\uff09\u3002</p> <p>\u200b\u4e0e\u200b\u4e8c\u5143\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002\u200b\u60a8\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4f7f\u7528\u200b\u4e8c\u5143\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\uff08Binary Support Vector Machines\uff09\u200b\u7684\u200b\u5148\u9a8c\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u635f\u5931\u200b\u53ef\u4ee5\u200b\u5199\u6210\u200b\uff1a</p> \\[ L_i = C \\max(0, 1 - y_i w^Tx_i) + R(W) \\] <p>\u200b\u5176\u4e2d\u200b\\(C\\)\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\\(y_i \\in \\{ -1,1 \\}\\)\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u9a8c\u8bc1\u200b\uff0c\u200b\u5728\u200b\u53ea\u6709\u200b\u4e24\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\u5448\u73b0\u200b\u7684\u200b\u516c\u5f0f\u200b\u662f\u200b\u4e8c\u5143\u200bSVM\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u53ea\u6709\u200b\u4e24\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u90a3\u4e48\u200b\u635f\u5931\u200b\u5c06\u200b\u51cf\u5c11\u200b\u5230\u200b\u4e0a\u9762\u200b\u663e\u793a\u200b\u7684\u200b\u4e8c\u5143\u200bSVM\u200b\u4e2d\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200b\\(C\\)\u200b\u548c\u200b\u6211\u4eec\u200b\u516c\u5f0f\u200b\u4e2d\u200b\u7684\u200b\\(\\lambda\\)\u200b\u63a7\u5236\u200b\u76f8\u540c\u200b\u7684\u200b\u6743\u8861\u200b\uff0c\u200b\u5e76\u4e14\u200b\u901a\u8fc7\u200b\u5012\u6570\u200b\u5173\u7cfb\u200b\\(C \\propto \\frac{1}{\\lambda}\\)\u200b\u76f8\u5173\u8054\u200b\u3002</p> <p>\u200b\u65c1\u6ce8\u200b\uff1a\u200b\u539f\u59cb\u200b\u95ee\u9898\u200b\u4e2d\u200b\u7684\u200b\u4f18\u5316\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u7ecf\u200b\u5177\u5907\u200bSVM\u200b\u7684\u200b\u5148\u524d\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u542c\u8bf4\u200b\u8fc7\u6838\u200b\u51fd\u6570\u200b\u3001\u200b\u5bf9\u5076\u200b\u95ee\u9898\u200b\u3001SMO\u200b\u7b97\u6cd5\u200b\u7b49\u200b\u3002\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff08\u200b\u4e0e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e00\u822c\u200b\u60c5\u51b5\u200b\u4e00\u6837\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u59cb\u7ec8\u200b\u4f7f\u7528\u200b\u5b83\u4eec\u200b\u5728\u200b\u672a\u200b\u7ea6\u675f\u200b\u7684\u200b\u539f\u59cb\u200b\u5f62\u5f0f\u200b\u4e2d\u200b\u7684\u200b\u4f18\u5316\u200b\u76ee\u6807\u200b\u3002\u200b\u8bb8\u591a\u200b\u8fd9\u4e9b\u200b\u76ee\u6807\u200b\u5728\u6280\u672f\u4e0a\u200b\u4e0d\u662f\u200b\u53ef\u5fae\u200b\u7684\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cmax(x,y)\u200b\u51fd\u6570\u200b\u4e0d\u53ef\u200b\u5fae\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5f53\u200bx=y\u200b\u65f6\u6709\u200b\u4e00\u4e2a\u200b\u62d0\u70b9\u200bkink\uff09\uff0c\u200b\u4f46\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u4e0d\u662f\u200b\u95ee\u9898\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u6b21\u68af\u5ea6\u200b\uff08subgradient\uff09\u3002</p> <p>\u200b\u65c1\u6ce8\u200b\uff1a\u200b\u5176\u4ed6\u200b\u591a\u200b\u7c7b\u200bSVM\u200b\u5f62\u5f0f\u200b\u3002\u200b\u503c\u5f97\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u672c\u200b\u8282\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u591a\u7c7b\u200bSVM\u200b\u662f\u200b\u591a\u7c7b\u200b\u95ee\u9898\u200b\u4e2d\u200b\u5236\u5b9a\u200bSVM\u200b\u7684\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u4e4b\u4e00\u200b\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u5e38\u7528\u200b\u7684\u200b\u5f62\u5f0f\u200b\u662f\u200b\u201c\u200b\u4e00\u5bf9\u200b\u591a\u200b\u201d\uff08One-Vs-All\uff0cOVA\uff09SVM\uff0c\u200b\u5b83\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u4e0e\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u7c7b\u522b\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u4e8c\u5143\u200bSVM\u3002\u200b\u76f8\u5173\u200b\u7684\u200b\u4f46\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u4e0d\u592a\u200b\u5e38\u89c1\u200b\u7684\u200b\u8fd8\u6709\u200b\u201c\u200b\u5168\u5bf9\u200b\u5168\u200b\u201d\uff08All-vs-All\uff0cAVA\uff09\u200b\u7b56\u7565\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u516c\u5f0f\u200b\u9075\u5faa\u200b\u4e86\u200bWeston\u200b\u548c\u200bWatkins 1999\uff08pdf\uff09\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u6bd4\u200bOVA\u200b\u66f4\u200b\u5f3a\u5927\u200b\u7684\u200b\u7248\u672c\u200b\uff08\u200b\u5728\u200b\u67d0\u79cd\u610f\u4e49\u200b\u4e0a\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u6784\u5efa\u200b\u591a\u200b\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8fd9\u4e2a\u200b\u7248\u672c\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u96f6\u200b\u6570\u636e\u200b\u635f\u5931\u200b\uff0c\u200b\u4f46\u200bOVA\u200b\u4e0d\u80fd\u200b\u3002\u200b\u5982\u679c\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff09\u3002\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u770b\u5230\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u516c\u5f0f\u200b\u662f\u200b*\u200b\u7ed3\u6784\u5316\u200bStructured SVM\uff0c\u200b\u5b83\u200b\u6700\u5927\u5316\u200b\u4e86\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5206\u6570\u200b\u4e0e\u200b\u6700\u9ad8\u200b\u5f97\u5206\u200b\u7684\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u4e9a\u519b\u200brunner-up\u200b\u7c7b\u522b\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u8ddd\u200b\u3002\u200b\u4e86\u89e3\u200b\u8fd9\u4e9b\u200b\u516c\u5f0f\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u8d85\u51fa\u200b\u4e86\u200b\u8bfe\u7a0b\u200b\u8303\u56f4\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u7b14\u8bb0\u200b\u4e2d\u200b\u5448\u73b0\u200b\u7684\u200b\u7248\u672c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5b89\u5168\u200b\u7684\u200b\u9009\u62e9\u200b\uff0c\u200b\u4f46\u200b\u53ef\u4ee5\u200b\u8bf4\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200bOVA\u200b\u7b56\u7565\u200b\u53ef\u80fd\u200b\u540c\u6837\u200b\u6709\u6548\u200b\uff08\u200b\u5982\u200bRikin\u200b\u7b49\u200b\u4eba\u200b\u5728\u200b\u634d\u536b\u200b\u4e00\u5bf9\u200b\u591a\u200b\u5206\u7c7b\u200b\uff08pdf\uff09\u200b\u4e2d\u200b\u6240\u200b\u8ba8\u8bba\u200b\u7684\u200b\uff09\u3002</p>"},{"location":"courses/neural_network/linear-classify/#softmax","title":"Softmax\u200b\u5206\u7c7b\u5668","text":"<p>\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\uff08SVM\uff09\u200b\u662f\u200b\u4e24\u79cd\u200b\u5e38\u89c1\u200b\u5206\u7c7b\u5668\u200b\u4e2d\u200b\u7684\u200b\u4e00\u79cd\u200b\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u5e38\u89c1\u200b\u9009\u62e9\u200b\u662f\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u5b83\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u4e4b\u524d\u200b\u542c\u8bf4\u200b\u8fc7\u200b\u4e8c\u5143\u200bLogistic\u200b\u56de\u5f52\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u90a3\u4e48\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u662f\u200b\u5b83\u200b\u5bf9\u200b\u591a\u7c7b\u200b\u95ee\u9898\u200b\u7684\u200b\u6cdb\u5316\u200b\u3002\u200b\u4e0e\u200bSVM\u200b\u4e0d\u540c\u200b\uff0cSVM\u200b\u5c06\u200b\u8f93\u51fa\u200b\\(f(x_i,W)\\)\u200b\u89c6\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\uff08\u200b\u672a\u200b\u6821\u51c6\u200b\u4e14\u200b\u53ef\u80fd\u200b\u96be\u4ee5\u200b\u89e3\u91ca\u200b\uff09\u200b\u5206\u6570\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u7a0d\u5fae\u200b\u76f4\u89c2\u200b\u4e00\u4e9b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\u7c7b\u522b\u200b\u6982\u7387\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u5177\u6709\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7a0d\u540e\u200b\u63cf\u8ff0\u200b\u7684\u200b\u6982\u7387\u200b\u89e3\u91ca\u200b\u3002\u200b\u5728\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u4e2d\u200b\uff0c\u200b\u51fd\u6570\u200b\u6620\u5c04\u200b\\(f(x_i; W) =  W x_i\\)\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u4f46\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5206\u6570\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u672a\u200b\u5f52\u4e00\u5316\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\uff0c\u200b\u5e76\u7528\u200b\u5177\u6709\u200b\u4ee5\u4e0b\u200b\u5f62\u5f0f\u200b\u7684\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200bcross-entropy loss\u200b\u66ff\u6362\u200b\u4e86\u200b\u5408\u9875\u200b\u635f\u5931\u200b\uff1a</p> \\[ L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{\u200b\u6216\u200b\u7b49\u4ef7\u200b\u5730\u200b} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j} \\] <p>\u200b\u5176\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7b26\u53f7\u200b\\(f_j\\)\u200b\u6765\u200b\u8868\u793a\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u5411\u91cf\u200b\\(f\\)\u200b\u7684\u200b\u7b2c\u200bj\u200b\u4e2a\u200b\u5143\u7d20\u200b\u3002\u200b\u4e0e\u200b\u4ee5\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5b8c\u6574\u200b\u635f\u5931\u200b\u662f\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u793a\u4f8b\u200b\u4e2d\u200b\\(L_i\\)\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\uff0c\u200b\u518d\u200b\u52a0\u4e0a\u200b\u6b63\u5219\u200b\u5316\u9879\u200b\\(R(W)\\)\u3002\u200b\u51fd\u6570\u200b\\(f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\\)\u200b\u88ab\u200b\u79f0\u4e3a\u200bsoftmax\u200b\u51fd\u6570\u200b\uff1a\u200b\u5b83\u200b\u63a5\u53d7\u200b\u5177\u6709\u200b\u4efb\u610f\u200b\u5b9e\u6570\u200b\u5206\u6570\u200b\uff08\u200b\u5728\u200b\\(z\\)\u200b\u4e2d\u200b\uff09\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u538b\u7f29\u200b\u5230\u200b\u4ecb\u4e8e\u200b\u96f6\u200b\u548c\u200b\u4e00\u200b\u4e4b\u95f4\u200b\u7684\u200b\u503c\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u4f7f\u200b\u5b83\u4eec\u200b\u603b\u548c\u200b\u4e3a\u200b\u4e00\u200b\u3002\u200b\u6d89\u53ca\u200bsoftmax\u200b\u51fd\u6570\u200b\u7684\u200b\u5b8c\u6574\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u53ef\u80fd\u200b\u5728\u200b\u60a8\u200b\u7b2c\u4e00\u6b21\u200b\u770b\u5230\u200b\u65f6\u200b\u770b\u8d77\u6765\u200b\u4ee4\u4eba\u751f\u754f\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u76f8\u5bf9\u200b\u5bb9\u6613\u200b\u89e3\u91ca\u200b\u3002</p> <p>\u200b\u4fe1\u606f\u8bba\u200b\u89c6\u89d2\u200b\uff1a\u200b\u5728\u200b\u201c\u200b\u771f\u5b9e\u200b\u201d\u200b\u5206\u5e03\u200b\\(p\\)\u200b\u548c\u200b\u4f30\u8ba1\u200b\u7684\u200b\u5206\u5e03\u200b\\(q\\)\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u88ab\u200b\u5b9a\u4e49\u200b\u4e3a\u200b\uff1a</p> \\[ H(p,q) = - \\sum_x p(x) \\log q(x) \\] <p>\u200b\u56e0\u6b64\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u6700\u5c0f\u5316\u200b\u4e86\u200b\u4f30\u8ba1\u200b\u7684\u200b\u7c7b\u522b\u200b\u6982\u7387\u200b\uff08\u200b\u5982\u200b\u4e0a\u9762\u200b\u6240\u793a\u200b\u7684\u200b\\(q = e^{f_{y_i}}  / \\sum_j e^{f_j}\\)\uff09\u200b\u4e0e\u200b\u201c\u200b\u771f\u5b9e\u200b\u201d\u200b\u5206\u5e03\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u89e3\u91ca\u200b\u4e2d\u200b\uff0c\u200b\u771f\u5b9e\u200b\u5206\u5e03\u200b\u662f\u200b\u6240\u6709\u200b\u6982\u7387\u200b\u8d28\u91cf\u200bmass\u200b\u90fd\u200b\u5728\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u4e0a\u200b\u7684\u200b\u5206\u5e03\u200b\uff08\u200b\u5373\u200b\\(p = [0, \\ldots 1, \\ldots, 0]\\)\u200b\u5728\u200b\\(y_i\\)\u200b\u5904\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b1\uff09\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u7531\u4e8e\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u53ef\u4ee5\u200b\u5199\u6210\u200b\u71b5\u200b\u548c\u200bKullback-Leibler\u200b\u6563\u5ea6\u200b\u7684\u200b\u5f62\u5f0f\u200b\\(H(p,q) = H(p) + D_{KL}(p||q)\\)\uff0c\u200b\u5e76\u4e14\u200bdelta\u200b\u51fd\u6570\u200b\\(p\\)\u200b\u7684\u200b\u71b5\u200b\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u7b49\u6548\u4e8e\u200b\u6700\u5c0f\u5316\u200b\u4e24\u4e2a\u200b\u5206\u5e03\u200b\u4e4b\u95f4\u200b\u7684\u200bKL\u200b\u6563\u5ea6\u200b\uff08\u200b\u8ddd\u79bb\u200b\u5ea6\u91cf\u200b\uff09\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u76ee\u6807\u200b\u5e0c\u671b\u200b\u9884\u6d4b\u200b\u7684\u200b\u5206\u5e03\u200b\u5c06\u200b\u5176\u200b\u6240\u6709\u200b\u6982\u7387\u200b\u8d28\u91cf\u200b\u653e\u5728\u200b\u6b63\u786e\u200b\u7b54\u6848\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6982\u7387\u200b\u89e3\u91ca\u200b\uff1a\u200b\u4ece\u200b\u8868\u8fbe\u5f0f\u200b\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b</p> \\[ P(y_i \\mid x_i; W) = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j} } \\] <p>\u200b\u53ef\u4ee5\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u7ed9\u5b9a\u200b\u56fe\u50cf\u200b\\(x_i\\)\u200b\u548c\u200b\u7531\u200b\\(W\\)\u200b\u53c2\u6570\u200b\u5316\u200b\u7684\u200b\\(y_i\\)\u200b\u7684\u200b\u6b63\u786e\u200b\u6807\u7b7e\u200b\u7684\u200b\uff08\u200b\u5f52\u4e00\u5316\u200b\uff09\u200b\u6982\u7387\u200b\u3002\u200b\u4ece\u200b\u8fd9\u4e2a\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u5c06\u200b\u8f93\u51fa\u200b\u5411\u91cf\u200b\\(f\\)\u200b\u4e2d\u200b\u7684\u200b\u5206\u6570\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u672a\u200b\u5f52\u4e00\u5316\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5bf9\u6570\u200b\u5316\u200b\u8fd9\u4e9b\u200b\u6570\u91cf\u200b\u4f1a\u200b\u7ed9\u51fa\u200b\uff08\u200b\u672a\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\uff09\u200b\u6982\u7387\u200b\uff0c\u200b\u800c\u200b\u9664\u200b\u6cd5\u5219\u200b\u6267\u884c\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u4ee5\u200b\u4f7f\u200b\u6982\u7387\u200b\u603b\u548c\u200b\u4e3a\u200b\u4e00\u200b\u3002\u200b\u5728\u200b\u6982\u7387\u200b\u89e3\u91ca\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u56e0\u6b64\u200b\u6700\u5c0f\u5316\u200b\u4e86\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u8d1f\u200b\u5bf9\u6570\u200b\u4f3c\u7136\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6267\u884c\u200b\u6700\u5927\u200b\u4f3c\u7136\u200b\u4f30\u8ba1\u200bMaximum Likelihood Estimation\uff08MLE\uff09\u3002\u200b\u8fd9\u4e2a\u200b\u89c2\u70b9\u200b\u7684\u200b\u4e00\u4e2a\u200b\u597d\u5904\u200b\u662f\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b8c\u6574\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u9879\u200b\\(R(W)\\)\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6765\u81ea\u200b\u4e8e\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\\(W\\)\u200b\u4e0a\u200b\u7684\u200b\u9ad8\u65af\u200b\u5148\u9a8c\u200bGaussian prior\uff0c\u200b\u5176\u4e2d\u200b\u6211\u4eec\u200b\u4e0d\u518d\u200b\u6267\u884c\u200bMLE\uff0c\u200b\u800c\u662f\u200b\u6267\u884c\u200b\u6700\u5927\u200b\u540e\u9a8c\u200b\u6982\u7387\u200bMaximum a posteriori\uff08MAP\uff09\u200b\u4f30\u8ba1\u200b\u3002\u200b\u6211\u4eec\u200b\u63d0\u5230\u200b\u8fd9\u4e9b\u200b\u89e3\u91ca\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u7406\u89e3\u200b\uff0c\u200b\u4f46\u200b\u5b8c\u6574\u200b\u7684\u200b\u63a8\u5bfc\u200b\u7ec6\u8282\u200b\u8d85\u51fa\u200b\u4e86\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u8303\u56f4\u200b\u3002</p> <p>\u200b\u5b9e\u9645\u200b\u95ee\u9898\u200b\uff1a\u200b\u6570\u503c\u200b\u7a33\u5b9a\u6027\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u7f16\u5199\u200b\u8ba1\u7b97\u200bSoftmax\u200b\u51fd\u6570\u200b\u7684\u200b\u4ee3\u7801\u200b\u65f6\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6307\u6570\u200b\u7684\u200b\u5b58\u5728\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u9879\u200b\\(e^{f_{y_i}}\\)\u200b\u548c\u200b\\(\\sum_j e^{f_j}\\)\u200b\u53ef\u80fd\u200b\u975e\u5e38\u200b\u5927\u200b\u3002\u200b\u7531\u4e8e\u200b\u5927\u6570\u200b\u76f8\u9664\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u6570\u503c\u200b\u4e0d\u200b\u7a33\u5b9a\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f7f\u7528\u200b\u5f52\u4e00\u5316\u200b\u6280\u5de7\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5206\u6570\u200b\u7684\u200b\u5206\u5b50\u200b\u548c\u200b\u5206\u6bcd\u200b\u90fd\u200b\u4e58\u4ee5\u200b\u4e00\u4e2a\u200b\u5e38\u6570\u200b\\(C\\)\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u7eb3\u5165\u200b\u603b\u548c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f97\u5230\u200b\u4ee5\u4e0b\u200b\uff08\u200b\u5728\u200b\u6570\u5b66\u200b\u4e0a\u200b\u7b49\u4ef7\u200b\u7684\u200b\uff09\u200b\u8868\u8fbe\u5f0f\u200b\uff1a</p> \\[   \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}   = \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}   = \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}} \\] <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u81ea\u7531\u9009\u62e9\u200b\\(C\\)\u200b\u7684\u200b\u503c\u200b\u3002\u200b\u8fd9\u200b\u4e0d\u4f1a\u200b\u6539\u53d8\u200b\u4efb\u4f55\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u503c\u6765\u200b\u63d0\u9ad8\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6570\u503c\u200b\u7a33\u5b9a\u6027\u200b\u3002\u200b\u5e38\u89c1\u200b\u7684\u200b\u9009\u62e9\u200b\u662f\u200b\u8bbe\u7f6e\u200b\\(\\log C = -\\max_j f_j\\)\u3002\u200b\u8fd9\u200b\u7b80\u5355\u200b\u5730\u200b\u8868\u793a\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5c06\u200b\u5411\u91cf\u200b\\(f\\)\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\u8fdb\u884c\u200b\u5e73\u79fb\u200b\uff0c\u200b\u4f7f\u200b\u6700\u5927\u503c\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u8868\u73b0\u200b\u4e3a\u200b\uff1a</p> <pre><code>f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\np = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n\n# instead: first shift the values of f so that the highest number is 0:\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n</code></pre> <p>\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f15\u8d77\u200b\u6df7\u6dc6\u200b\u7684\u200b\u547d\u540d\u200b\u7ea6\u5b9a\u200b\u3002\u200b\u786e\u5207\u200b\u5730\u8bf4\u200b\uff0cSVM\u200b\u5206\u7c7b\u5668\u200b\u4f7f\u7528\u200b\u5408\u9875\u200b\u635f\u5931\u200b\uff0c\u200b\u6709\u65f6\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u6700\u5927\u200b\u95f4\u9694\u200bmax-margin\u200b\u635f\u5931\u200b\u3002Softmax\u200b\u5206\u7c7b\u5668\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u3002Softmax\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u540d\u79f0\u200b\u6765\u81ea\u200bSoftmax\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u7528\u4e8e\u200b\u5c06\u200b\u539f\u59cb\u200b\u7684\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u538b\u7f29\u6210\u200b\u89c4\u8303\u5316\u200b\u7684\u200b\u6b63\u503c\u200b\uff0c\u200b\u4f7f\u200b\u5b83\u4eec\u200b\u603b\u548c\u200b\u4e3a\u200b1\uff0c\u200b\u4ee5\u4fbf\u200b\u5e94\u7528\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u3002\u200b\u7279\u522b\u200b\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5728\u6280\u672f\u4e0a\u200b\u8ba8\u8bba\u200b\"softmax\u200b\u635f\u5931\u200b\"\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\uff0c\u200b\u56e0\u4e3a\u200bsoftmax\u200b\u53ea\u662f\u200b\u538b\u7f29\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u76f8\u5bf9\u200b\u5e38\u7528\u200b\u7684\u200b\u7b80\u5199\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#svm-vs-softmax","title":"SVM vs. Softmax","text":"<p>\u200b\u4e00\u5f20\u200b\u56fe\u200b\u53ef\u80fd\u200b\u6709\u52a9\u4e8e\u200b\u8bf4\u660e\u200bSoftmax\u200b\u548c\u200bSVM\u200b\u5206\u7c7b\u5668\u200b\u4e4b\u95f4\u200b\u7684\u200b\u533a\u522b\u200b\uff1a</p> <p> </p> <p>SVM\u200b\u548c\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u5728\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u70b9\u4e0a\u200b\u7684\u200b\u533a\u522b\u200b\u793a\u4f8b\u200b\u3002\u200b\u5728\u200b\u4e24\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u8ba1\u7b97\u200b\u76f8\u540c\u200b\u7684\u200b\u5206\u6570\u200b\u5411\u91cf\u200bf\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\u901a\u8fc7\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff09\u3002\u200b\u533a\u522b\u200b\u5728\u4e8e\u200bf\u200b\u4e2d\u200b\u5206\u6570\u200b\u7684\u200b\u89e3\u91ca\u200b\uff1aSVM\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5206\u6570\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5176\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u9f13\u52b1\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\uff08\u200b\u84dd\u8272\u200b\u7684\u200b\u7c7b\u522b\u200b2\uff09\u200b\u5177\u6709\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u9ad8\u51fa\u200b\u4e00\u5b9a\u200b\u95f4\u9694\u200b\u7684\u200b\u5206\u6570\u200b\u3002\u200b\u800c\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u5219\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5206\u6570\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\uff08\u200b\u672a\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\uff09\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\uff0c\u200b\u7136\u540e\u200b\u9f13\u52b1\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\uff08\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\uff09\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\u8f83\u200b\u9ad8\u200b\uff08\u200b\u7b49\u6548\u200b\u5730\u200b\uff0c\u200b\u5176\u200b\u8d1f\u503c\u200b\u8f83\u200b\u4f4e\u200b\uff09\u3002\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\uff0cSVM\u200b\u7684\u200b\u6700\u7ec8\u200b\u635f\u5931\u200b\u4e3a\u200b1.58\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u635f\u5931\u200b\u4e3a\u200b1.04\uff08\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4f7f\u7528\u200b\u81ea\u7136\u5bf9\u6570\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4ee5\u200b2\u200b\u4e3a\u5e95\u200b\u6216\u200b\u4ee5\u200b10\u200b\u4e3a\u5e95\u200b\u7684\u200b\u5bf9\u6570\u200b\uff09\uff0c\u200b\u4f46\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u4e0d\u53ef\u200b\u6bd4\u8f83\u200b\uff1b\u200b\u5b83\u4eec\u200b\u53ea\u200b\u5728\u200b\u4e0e\u200b\u540c\u4e00\u200b\u5206\u7c7b\u5668\u200b\u548c\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u5185\u200b\u8ba1\u7b97\u200b\u7684\u200b\u635f\u5931\u200b\u76f8\u5173\u200b\u65f6\u6709\u200b\u610f\u4e49\u200b\u3002</p> <p>Softmax\u200b\u5206\u7c7b\u5668\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u63d0\u4f9b\u200b\u201c\u200b\u6982\u7387\u200b\u201d\u3002 \u200b\u4e0e\u200bSVM\u200b\u4e0d\u540c\u200b\uff0cSVM\u200b\u8ba1\u7b97\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\u7684\u200b\u672a\u200b\u6821\u51c6\u200b\u4e14\u200b\u4e0d\u200b\u5bb9\u6613\u200b\u89e3\u91ca\u200b\u7684\u200b\u5206\u6570\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u8ba1\u7b97\u200b\u6240\u6709\u200b\u6807\u7b7e\u200b\u7684\u200b\u201c\u200b\u6982\u7387\u200b\u201d\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7ed9\u5b9a\u200b\u4e00\u5e45\u200b\u56fe\u50cf\u200b\uff0cSVM\u200b\u5206\u7c7b\u5668\u200b\u53ef\u80fd\u200b\u4f1a\u4e3a\u200b\u7c7b\u522b\u200b\u201c\u200b\u732b\u200b\u201d\u3001\u201c\u200b\u72d7\u200b\u201d\u200b\u548c\u200b\u201c\u200b\u8239\u200b\u201d\u200b\u5206\u522b\u200b\u7ed9\u51fa\u200b\u5f97\u5206\u200b[12.5\u30010.6\u3001-23.0]\u3002\u200b\u800c\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u8fd9\u200b\u4e09\u4e2a\u200b\u6807\u7b7e\u200b\u7684\u200b\u6982\u7387\u200b\uff0c\u200b\u4f8b\u5982\u200b[0.9\u30010.09\u30010.01]\uff0c\u200b\u8fd9\u4f7f\u200b\u60a8\u200b\u80fd\u591f\u200b\u89e3\u91ca\u200b\u5b83\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u7f6e\u4fe1\u5ea6\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u201c\u200b\u6982\u7387\u200b\u201d\u200b\u4e00\u8bcd\u200b\u653e\u5728\u200b\u5f15\u53f7\u200b\u4e2d\u200b\uff0c\u200b\u539f\u56e0\u200b\u662f\u200b\u8fd9\u4e9b\u200b\u6982\u7387\u200b\u7684\u200b\u5cf0\u503c\u200bpeaky\u200b\u6216\u200b\u6269\u6563\u200b\u7a0b\u5ea6\u200bdiffuse\u200b\u76f4\u63a5\u200b\u53d6\u51b3\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\\(\\lambda\\)\uff0c\u200b\u800c\u200b\u60a8\u200b\u8d1f\u8d23\u200b\u5c06\u200b\u5176\u200b\u8f93\u5165\u200b\u5230\u200b\u7cfb\u7edf\u200b\u4e2d\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u67d0\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u672a\u200b\u6807\u51c6\u5316\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\u4e3a\u200b[1\u3001-2\u30010]\u3002\u200b\u7136\u540e\u200b\uff0cSoftmax\u200b\u51fd\u6570\u200b\u5c06\u200b\u8ba1\u7b97\u200b\uff1a</p> \\[ [1\u3001-2\u30010] \\rightarrow [e^1\u3001e^{-2}\u3001e^0] = [2.71\u30010.14\u30011] \\rightarrow [0.7\u30010.04\u30010.26] \\] <p>\u200b\u5176\u4e2d\u200b\u6240\u200b\u91c7\u53d6\u200b\u7684\u200b\u6b65\u9aa4\u200b\u662f\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u6307\u6570\u5316\u200b\u548c\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u603b\u548c\u200b\u4e3a\u200b1\u3002\u200b\u73b0\u5728\u200b\uff0c\u200b\u5982\u679c\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\\(\\lambda\\)\u200b\u66f4\u9ad8\u200b\uff0c\u200b\u5219\u200b\u6743\u91cd\u200b\\(W\\)\u200b\u5c06\u200b\u53d7\u5230\u200b\u66f4\u200b\u591a\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u4ece\u800c\u200b\u5bfc\u81f4\u200b\u6743\u91cd\u200b\u66f4\u200b\u5c0f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6743\u91cd\u200b\u53d8\u5c0f\u200b\u4e86\u200b\u4e00\u534a\u200b\uff08[0.5\u3001-1\u30010]\uff09\u3002Softmax\u200b\u73b0\u5728\u200b\u5c06\u200b\u8ba1\u7b97\u200b\uff1a</p> \\[ [0.5\u3001-1\u30010] \\rightarrow [e^{0.5}\u3001e^{-1}\u3001e^0] = [1.65\u30010.37\u30011] \\rightarrow [0.55\u30010.12\u30010.33] \\] <p>\u200b\u5176\u4e2d\u200b\u6982\u7387\u200b\u73b0\u5728\u200b\u66f4\u52a0\u200b\u6269\u6563\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u5728\u200b\u7531\u4e8e\u200b\u975e\u5e38\u200b\u5f3a\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\\(\\lambda\\)\u200b\u5bfc\u81f4\u200b\u6743\u91cd\u200b\u53d8\u5f97\u200b\u5fae\u5c0f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8f93\u51fa\u200b\u6982\u7387\u200b\u5c06\u200b\u63a5\u8fd1\u200b\u5747\u5300\u5206\u5e03\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6982\u7387\u200b\u66f4\u200b\u9002\u5408\u200b\u89c6\u4e3a\u200b\u7f6e\u4fe1\u5ea6\u200b\uff0c\u200b\u4e0e\u200bSVM\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5206\u6570\u200b\u7684\u200b\u987a\u5e8f\u200b\u662f\u200b\u53ef\u4ee5\u200b\u89e3\u91ca\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u7edd\u5bf9\u200b\u6570\u5b57\u200b\uff08\u200b\u6216\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\uff09\u200b\u5728\u6280\u672f\u4e0a\u200b\u662f\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0cSVM\u200b\u548c\u200bSoftmax\u200b\u901a\u5e38\u200b\u662f\u200b\u53ef\u200b\u6bd4\u8f83\u200b\u7684\u200b\u3002 SVM\u200b\u548c\u200bSoftmax\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6027\u80fd\u200b\u5dee\u5f02\u200b\u901a\u5e38\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7684\u200b\u4eba\u200b\u5bf9\u200b\u54ea\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\u4f1a\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u770b\u6cd5\u200b\u3002\u200b\u4e0e\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u76f8\u6bd4\u200b\uff0cSVM\u200b\u662f\u200b\u4e00\u4e2a\u200b\u66f4\u200b \u200b\u5c40\u90e8\u200blocal \u200b\u7684\u200b\u76ee\u6807\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u89c6\u4e3a\u200b\u7f3a\u9677\u200b\u6216\u200b\u7279\u5f81\u200b\u3002\u200b\u8003\u8651\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u5176\u5f97\u200b\u5206\u4e3a\u200b[10\u3001-2\u30013]\uff0c\u200b\u5176\u4e2d\u200b\u7b2c\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\u3002SVM\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5e0c\u671b\u200b\u7684\u200b\u95f4\u9694\u200b\u4e3a\u200b\\(\\Delta = 1\\)\uff09\u200b\u5c06\u200b\u770b\u5230\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u5f97\u5206\u200b\u5df2\u7ecf\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u7c7b\u522b\u200b\u9ad8\u51fa\u200b\u95f4\u9694\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u4e3a\u200b\u96f6\u200b\u3002SVM\u200b\u4e0d\u200b\u5173\u5fc3\u200b\u4e2a\u522b\u200b\u5f97\u5206\u200b\u7684\u200b\u7ec6\u8282\u200b\uff1a\u200b\u5982\u679c\u200b\u5b83\u4eec\u200b\u6539\u4e3a\u200b[10\u3001-100\u3001-100]\u200b\u6216\u200b[10\u30019\u30019]\uff0cSVM\u200b\u5c06\u200b\u4e0d\u52a0\u533a\u5206\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6ee1\u8db3\u200b\u95f4\u9694\u200b1\uff0c\u200b\u56e0\u6b64\u200b\u635f\u5931\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u60c5\u51b5\u200b\u5bf9\u4e8e\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u6765\u8bf4\u200b\u5e76\u200b\u4e0d\u200b\u7b49\u540c\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5f97\u5206\u200b[10\u30019\u30019]\uff0cSoftmax\u200b\u5c06\u200b\u79ef\u7d2f\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u635f\u5931\u200b\uff0c\u200b\u800c\u200b\u5bf9\u4e8e\u200b[10\u3001-100\u3001-100]\uff0c\u200b\u635f\u5931\u200b\u8f83\u200b\u4f4e\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0cSoftmax\u200b\u5206\u7c7b\u5668\u200b\u603b\u662f\u200b\u4e0d\u592a\u200b\u6ee1\u610f\u200b\u751f\u6210\u200b\u7684\u200b\u5f97\u5206\u200b\uff1a\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u6982\u7387\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u66f4\u9ad8\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u7684\u200b\u6982\u7387\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u635f\u5931\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u66f4\u200b\u5c0f\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u4e00\u65e6\u200b\u6ee1\u8db3\u200b\u95f4\u9694\u200b\uff0cSVM\u200b\u5c31\u200b\u4f1a\u200b\u6ee1\u610f\u200b\uff0c\u200b\u5b83\u200b\u4e0d\u4f1a\u200b\u5728\u200b\u8d85\u51fa\u200b\u6b64\u200b\u7ea6\u675f\u200b\u7684\u200b\u786e\u5207\u200b\u5f97\u5206\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u5fae\u89c2\u200b\u7ba1\u7406\u200b\u3002\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u76f4\u89c2\u200b\u5730\u200b\u770b\u4f5c\u200b\u662f\u200b\u4e00\u79cd\u200b\u7279\u6027\u200b\uff1a\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6c7d\u8f66\u200b\u5206\u7c7b\u5668\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u82b1\u8d39\u200b\u5927\u90e8\u5206\u200b\u201c\u200b\u7cbe\u529b\u200b\u201d\u200b\u6765\u200b\u89e3\u51b3\u200b\u5c06\u200b\u6c7d\u8f66\u200b\u4e0e\u200b\u5361\u8f66\u200b\u5206\u5f00\u200b\u7684\u200b\u96be\u9898\u200b\uff0c\u200b\u4e0d\u200b\u5e94\u200b\u53d7\u5230\u200b\u9752\u86d9\u200b\u793a\u4f8b\u200b\u7684\u200b\u5f71\u54cd\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5df2\u7ecf\u200b\u5206\u914d\u200b\u4e86\u200b\u975e\u5e38\u4f4e\u200b\u7684\u200b\u5206\u6570\u200b\uff0c\u200b\u800c\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u805a\u96c6\u200b\u5728\u200b\u6570\u636e\u200b\u4e91\u200b\u7684\u200b\u5b8c\u5168\u200b\u4e0d\u540c\u200b\u7684\u200b\u4e00\u4fa7\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_7","title":"\u4ea4\u4e92\u5f0f\u200b\u7f51\u7edc\u200b\u6f14\u793a","text":"<p>\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4ea4\u4e92\u5f0f\u200b\u7f51\u7edc\u200b\u6f14\u793a\u200b\uff0c\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u7406\u89e3\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u8be5\u200b\u6f14\u793a\u200b\u4f7f\u7528\u200b2D\u200b\u6570\u636e\u200b\u4e0a\u200b\u7684\u200b\u73a9\u5177\u200b\u4e09\u7c7b\u200b\u5206\u7c7b\u200b\u53ef\u89c6\u5316\u200b\u4e86\u200b\u672c\u8282\u200b\u8ba8\u8bba\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u6f14\u793a\u200b\u8fd8\u200b\u7a0d\u5fae\u200b\u8d85\u524d\u200b\u5730\u200b\u6267\u884c\u200b\u4e86\u200b\u4f18\u5316\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\u8be6\u7ec6\u200b\u8ba8\u8bba\u200b\u8fd9\u4e00\u200b\u8fc7\u7a0b\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_8","title":"\u603b\u7ed3","text":"<p>\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\uff0c</p> <ul> <li>\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5206\u6570\u200b\u51fd\u6570\u200b\uff0c\u200b\u5c06\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u6620\u5c04\u200b\u5230\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff08\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u4f9d\u8d56\u4e8e\u200b\u6743\u91cd\u200bW\u200b\u548c\u200b\u504f\u5dee\u200bb\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff09\u3002</li> <li>\u200b\u4e0e\u200bkNN\u200b\u5206\u7c7b\u5668\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u53c2\u6570\u200b\u5316\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4f18\u70b9\u200b\u5728\u4e8e\u200b\uff0c\u200b\u4e00\u65e6\u200b\u5b66\u4e60\u200b\u4e86\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e22\u5f03\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u65b0\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u5f88\u5feb\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u53ea\u200b\u9700\u8981\u200b\u4e0e\u200bW\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4e0e\u200b\u6bcf\u4e2a\u200b\u8bad\u7ec3\u200b\u793a\u4f8b\u200b\u8fdb\u884c\u200b\u8be6\u5c3d\u200b\u7684\u200b\u6bd4\u8f83\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u4e86\u200b\u504f\u7f6e\u200b\u6280\u5de7\u200b\uff0c\u200b\u5b83\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u5c06\u200b\u504f\u5dee\u200b\u5411\u91cf\u200b\u5408\u5e76\u200b\u5230\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u53ea\u200b\u9700\u200b\u8ddf\u8e2a\u200b\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u77e9\u9635\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4e24\u79cd\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u5e38\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff1aSVM\u200b\u548c\u200bSoftmax\uff09\uff0c\u200b\u7528\u4e8e\u200b\u8861\u91cf\u200b\u7ed9\u5b9a\u200b\u53c2\u6570\u200b\u96c6\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u771f\u503c\u200b\u6807\u7b7e\u200b\u7684\u200b\u517c\u5bb9\u6027\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u770b\u5230\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u662f\u200b\u4ee5\u200b\u8fd9\u6837\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9a\u4e49\u200b\u7684\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u826f\u597d\u200b\u7684\u200b\u9884\u6d4b\u200b\u7b49\u4ef7\u200b\u4e8e\u200b\u5177\u6709\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u635f\u5931\u200b\u3002</li> </ul> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u4e00\u79cd\u200b\u5c06\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u96c6\u200b\u6620\u5c04\u200b\u5230\u200b\u57fa\u4e8e\u200b\u4e00\u7ec4\u200b\u53c2\u6570\u200b\u7684\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u8861\u91cf\u200b\u9884\u6d4b\u200b\u8d28\u91cf\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u793a\u4f8b\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5982\u4f55\u200b\u6709\u6548\u200b\u5730\u200b\u786e\u5b9a\u200b\u63d0\u4f9b\u200b\u6700\u4f73\u200b\uff08\u200b\u6700\u4f4e\u200b\uff09\u200b\u635f\u5931\u200b\u7684\u200b\u53c2\u6570\u200b\uff1f\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u662f\u200b\u4f18\u5316\u200boptimization\u200b\u7684\u200b\u4e3b\u9898\u200b\uff0c\u200b\u5c06\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u8ba8\u8bba\u200b\u3002</p>"},{"location":"courses/neural_network/linear-classify/#_9","title":"\u66f4\u200b\u591a\u200b\u9605\u8bfb","text":"<p>\u200b\u4ee5\u4e0b\u200b\u9605\u8bfb\u200b\u662f\u200b\u53ef\u9009\u200b\u7684\u200b\uff0c\u200b\u5305\u542b\u200b\u4e00\u4e9b\u200b\u6709\u8da3\u200b\u7684\u200b\u6307\u5f15\u200b\u3002</p> <ul> <li>Charlie Tang\u200b\u5728\u200b2013\u200b\u5e74\u200b\u53d1\u8868\u200b\u7684\u200bDeep Learning using Linear Support Vector Machines\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u58f0\u79f0\u200bL2SVM\u200b\u4f18\u4e8e\u200bSoftmax\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-1/","title":"\u521b\u5efa\u200b\u7ed3\u6784","text":""},{"location":"courses/neural_network/neural-networks-1/#_1","title":"\u5feb\u901f\u200b\u4ecb\u7ecd","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0d\u200b\u501f\u52a9\u200b\u8111\u90e8\u200b\u7c7b\u6bd4\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u4ecb\u7ecd\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u5728\u200b\u5173\u4e8e\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u516c\u5f0f\u200b \\(s = W x\\) \u200b\u6839\u636e\u200b\u56fe\u50cf\u200b\u8ba1\u7b97\u200b\u4e0d\u540c\u200b\u89c6\u89c9\u200b\u7c7b\u522b\u200b\u7684\u200b\u5f97\u5206\u200b\uff0c\u200b\u5176\u4e2d\u200b \\(W\\) \u200b\u662f\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b\uff0c\\(x\\) \u200b\u662f\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u5217\u200b\u5411\u91cf\u200b\uff0c\u200b\u5305\u542b\u200b\u56fe\u50cf\u200b\u7684\u200b\u6240\u6709\u200b\u50cf\u7d20\u200b\u6570\u636e\u200b\u3002\u200b\u5728\u200bCIFAR-10\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\\(x\\) \u200b\u662f\u200b\u4e00\u4e2a\u200b [3072x1] \u200b\u7684\u200b\u5217\u200b\u5411\u91cf\u200b\uff0c\\(W\\) \u200b\u662f\u200b\u4e00\u4e2a\u200b [10x3072] \u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8f93\u51fa\u200b\u5f97\u5206\u200b\u662f\u200b10\u200b\u4e2a\u7c7b\u200b\u5f97\u5206\u200b\u7684\u200b\u5411\u91cf\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4f1a\u200b\u8ba1\u7b97\u200b \\(s = W_2 \\max(0, W_1 x)\\)\u3002\u200b\u8fd9\u91cc\u200b\uff0c\\(W_1\\) \u200b\u53ef\u4ee5\u200b\u662f\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b [100x3072] \u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b100\u200b\u7ef4\u200b\u7684\u200b\u4e2d\u95f4\u200b\u5411\u91cf\u200b\u3002\u200b\u51fd\u6570\u200b $max(0,-) $ \u200b\u662f\u200b\u9010\u200b\u5143\u7d20\u200b\u5e94\u7528\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4e3a\u200b\u975e\u7ebf\u6027\u200b\u9009\u62e9\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u4e0b\u9762\u200b\u7814\u7a76\u200b\uff09\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u9009\u62e9\u200b\uff0c\u200b\u7b80\u5355\u200b\u5730\u200b\u5c06\u200b\u6240\u6709\u200b\u4f4e\u4e8e\u200b\u96f6\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u9608\u503c\u200b\u8bbe\u4e3a\u200b\u96f6\u200b\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u77e9\u9635\u200b \\(W_2\\) \u200b\u7684\u200b\u5927\u5c0f\u200b\u5c06\u200b\u662f\u200b [10x100]\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u518d\u6b21\u200b\u5f97\u5230\u200b10\u200b\u4e2a\u200b\u6211\u4eec\u200b\u89e3\u91ca\u200b\u4e3a\u7c7b\u200b\u5f97\u5206\u200b\u7684\u200b\u6570\u5b57\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u975e\u7ebf\u6027\u200b\u5728\u200b\u8ba1\u7b97\u200b\u4e0a\u200b\u662f\u200b\u5173\u952e\u200b\u7684\u200b\u2014\u2014\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7701\u7565\u200b\u5b83\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u77e9\u9635\u200b\u53ef\u4ee5\u200b\u5408\u5e76\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9884\u6d4b\u200b\u7684\u200b\u7c7b\u200b\u5f97\u5206\u200b\u5c06\u200b\u518d\u6b21\u200b\u662f\u200b\u8f93\u5165\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\u3002\u200b\u975e\u7ebf\u6027\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u7684\u200b\u6ce2\u52a8\u200bwiggle\u3002\u200b\u53c2\u6570\u200b \\(W_2, W_1\\) \u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5b83\u4eec\u200b\u7684\u200b\u68af\u5ea6\u200b\u4f7f\u7528\u200b\u94fe\u5f0f\u200b\u89c4\u5219\u200b\u63a8\u5bfc\u200b\uff08\u200b\u5e76\u200b\u4f7f\u7528\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8ba1\u7b97\u200b\uff09\u3002</p> <p>\u200b\u4e00\u4e2a\u4e09\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ef\u4ee5\u200b\u7c7b\u4f3c\u200b\u5730\u200b\u770b\u8d77\u6765\u200b\u50cf\u200b \\(s = W_3 \\max(0, W_2 \\max(0, W_1 x))\\)\uff0c\u200b\u5176\u4e2d\u200b\u6240\u6709\u200b\u7684\u200b \\(W_3, W_2, W_1\\) \u200b\u90fd\u200b\u662f\u200b\u8981\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\u3002\u200b\u4e2d\u95f4\u200b\u9690\u85cf\u200b\u5411\u91cf\u200b\u7684\u200b\u5927\u5c0f\u200b\u662f\u200b\u7f51\u7edc\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u5982\u4f55\u200b\u8bbe\u7f6e\u200b\u5b83\u4eec\u200b\u3002\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u4ece\u200b\u795e\u7ecf\u5143\u200b/\u200b\u7f51\u7edc\u200b\u7684\u200b\u89d2\u5ea6\u200b\u89e3\u91ca\u200b\u8fd9\u4e9b\u200b\u8ba1\u7b97\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_2","title":"\u5efa\u6a21\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143","text":"<p>\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9886\u57df\u200b\u6700\u521d\u200b\u7684\u200b\u4e3b\u8981\u200b\u7075\u611f\u200b\u6765\u6e90\u4e8e\u200b\u6a21\u62df\u200b\u751f\u7269\u200b\u795e\u7ecf\u7cfb\u7edf\u200b\u7684\u200b\u76ee\u6807\u200b\uff0c\u200b\u4f46\u200b\u540e\u6765\u200b\u51fa\u73b0\u200b\u4e86\u200b\u5206\u6b67\u200b\uff0c\u200b\u6210\u4e3a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5de5\u7a0b\u200b\u95ee\u9898\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u53d6\u5f97\u200b\u4e86\u200b\u826f\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u5c3d\u7ba1\u5982\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u662f\u200b\u4ee5\u200b\u5bf9\u200b\u751f\u7269\u200b\u7cfb\u7edf\u200b\u7684\u200b\u975e\u5e38\u200b\u7b80\u77ed\u200b\u548c\u200b\u9ad8\u6c34\u5e73\u200b\u7684\u200b\u63cf\u8ff0\u200b\u5f00\u59cb\u200b\u8ba8\u8bba\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u9886\u57df\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u53d7\u5230\u200b\u4e86\u200b\u5176\u200b\u542f\u53d1\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_3","title":"\u751f\u7269\u200b\u52a8\u673a\u200b\u548c\u200b\u8054\u7cfb","text":"<p>\u200b\u5927\u8111\u200b\u7684\u200b\u57fa\u672c\u200b\u8ba1\u7b97\u200b\u5355\u5143\u200b\u662f\u200b\u795e\u7ecf\u5143\u200bneuron\u3002\u200b\u5728\u200b\u4eba\u7c7b\u200b\u795e\u7ecf\u7cfb\u7edf\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u5927\u7ea6\u200b860\u200b\u4ebf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4e0e\u200b\u5927\u7ea6\u200b\\(10^{14} - 10^{15}\\) \u200b\u7a81\u89e6\u200bsynapses\u200b\u76f8\u8fde\u63a5\u200b\u3002\u200b\u4e0b\u9762\u200b\u7684\u200b\u56fe\u8868\u200b\u663e\u793a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u751f\u7269\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u5361\u901a\u56fe\u200b\uff08\u200b\u5de6\u200b\uff09\u200b\u548c\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u6570\u5b66\u6a21\u578b\u200b\uff08\u200b\u53f3\u200b\uff09\u3002\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u4ece\u200b\u5176\u200b\u6811\u7a81\u200bdendrites\u200b\u63a5\u6536\u200b\u8f93\u5165\u200b\u4fe1\u53f7\u200b\uff0c\u200b\u5e76\u200b\u6cbf\u5176\u200b\uff08\u200b\u5355\u4e00\u200b\uff09\u200b\u8f74\u7a81\u200baxon\u200b\u4ea7\u751f\u200b\u8f93\u51fa\u200b\u4fe1\u53f7\u200b\u3002\u200b\u8f74\u7a81\u200b\u6700\u7ec8\u200b\u5206\u53c9\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u7a81\u89e6\u200b\u8fde\u63a5\u200b\u5230\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6811\u7a81\u200b\u3002\u200b\u5728\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u6cbf\u200b\u8f74\u7a81\u200b\u4f20\u64ad\u200b\u7684\u200b\u4fe1\u53f7\u200b\uff08\u200b\u4f8b\u5982\u200b \\(x_0\\)\uff09\u200b\u4e0e\u200b\u57fa\u4e8e\u200b\u8be5\u200b\u7a81\u89e6\u200b\u7684\u200b\u7a81\u89e6\u200b\u5f3a\u5ea6\u200b\uff08\u200b\u4f8b\u5982\u200b \\(w_0\\)\uff09\u200b\u7684\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6811\u7a81\u200b\u4e58\u6027\u200b\u5730\u200b\u4ea4\u4e92\u200b\uff08\u200b\u4f8b\u5982\u200b \\(w_0 x_0\\)\uff09\u3002\u200b\u5176\u200b\u601d\u60f3\u200b\u662f\u200b\u7a81\u89e6\u200b\u7684\u200b\u5f3a\u5ea6\u200b\uff08\u200b\u6743\u91cd\u200b \\(w\\)\uff09\u200b\u662f\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u7684\u200b\uff0c\u200b\u5e76\u200b\u63a7\u5236\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u5bf9\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u5f71\u54cd\u200b\u7684\u200b\u5f3a\u5ea6\u200b\uff08\u200b\u53ca\u5176\u200b\u65b9\u5411\u200b\uff1a\u200b\u5174\u594b\u200b\uff08\u200b\u6b63\u200b\u6743\u91cd\u200b\uff09\u200b\u6216\u200b\u6291\u5236\u200b\uff08\u200b\u8d1f\u200b\u6743\u91cd\u200b\uff09\uff09\u3002\u200b\u5728\u200b\u57fa\u672c\u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u6811\u7a81\u200b\u5c06\u200b\u4fe1\u53f7\u4f20\u200b\u9001\u5230\u200b\u7ec6\u80de\u4f53\u200b\uff0c\u200b\u5b83\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u200b\u6c42\u548c\u200b\u3002\u200b\u5982\u679c\u200b\u6700\u7ec8\u200b\u7684\u200b\u603b\u548c\u200b\u8d85\u8fc7\u200b\u67d0\u4e2a\u200b\u9608\u503c\u200b\uff0c\u200b\u795e\u7ecf\u5143\u200b\u53ef\u4ee5\u200b\u653e\u7535\u200bfire\uff0c\u200b\u6cbf\u5176\u200b\u8f74\u7a81\u200b\u53d1\u9001\u200b\u4e00\u4e2a\u200b\u8109\u51b2\u200b\u3002\u200b\u5728\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u8109\u51b2\u200b\u7684\u200b\u7cbe\u786e\u200b\u65f6\u673a\u200b\u5e76\u200b\u4e0d\u200b\u91cd\u8981\u200b\uff0c\u200b\u53ea\u6709\u200b\u653e\u7535\u200b\u7684\u200b\u9891\u7387\u200b\u4f20\u9012\u4fe1\u606f\u200b\u3002\u200b\u6839\u636e\u200b\u901f\u7387\u200b\u4ee3\u7801\u200brate code\u200b\u7684\u200b\u89e3\u91ca\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200bactivation functio \\(f\\) \u200b\u6765\u200b\u5efa\u6a21\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u653e\u7535\u200b\u901f\u7387\u200bfiring rate\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u4ee3\u8868\u200b\u6cbf\u200b\u8f74\u7a81\u200b\u7684\u200b\u8109\u51b2\u200b\u9891\u7387\u200b\u3002\u200b\u5386\u53f2\u200b\u4e0a\u200b\uff0c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u5e38\u89c1\u200b\u9009\u62e9\u200b\u662f\u200bsigmoid\u200b\u51fd\u6570\u200b \\(\\sigma\\)\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u5b9e\u503c\u200b\u8f93\u5165\u200b\uff08\u200b\u6c42\u548c\u200b\u540e\u200b\u7684\u200b\u4fe1\u53f7\u5f3a\u5ea6\u200b\uff09\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u538b\u7f29\u200b\u5230\u200b0\u200b\u548c\u200b1\u200b\u4e4b\u95f4\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u8fd9\u4e9b\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u7ec6\u8282\u200b\u3002</p> <p> </p> <p>\u200b\u751f\u7269\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u5361\u901a\u753b\u200b\uff08\u200b\u5de6\u200b\uff09\u200b\u53ca\u5176\u200b\u6570\u5b66\u6a21\u578b\u200b\uff08\u200b\u53f3\u200b\uff09</p> <p>\u200b\u7528\u4e8e\u200b\u6b63\u5411\u200b\u4f20\u64ad\u200b\u5355\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code>class Neuron(object):\n  # ... \n  def forward(self, inputs):\n\"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n    cell_body_sum = np.sum(inputs * self.weights) + self.bias\n    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function\n    return firing_rate\n</code></pre> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u4e0e\u200b\u8f93\u5165\u200b\u548c\u200b\u5b83\u200b\u7684\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u70b9\u79ef\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u504f\u5dee\u200b\u5e76\u200b\u5e94\u7528\u200b\u975e\u7ebf\u6027\u200b\uff08\u200b\u6216\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff09\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u662f\u200bsigmoid\u200b\u51fd\u6570\u200b \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\)\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u8282\u200b\u7ed3\u675f\u200b\u65f6\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u5730\u200b\u8ba8\u8bba\u200b\u4e0d\u540c\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u7c97\u7565\u200b\u6a21\u578b\u200b\u3002\u200b\u9700\u8981\u200b\u5f3a\u8c03\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u751f\u7269\u5b66\u200b\u795e\u7ecf\u5143\u200b\u6a21\u578b\u200b\u975e\u5e38\u200b\u7c97\u7565\u200b\uff1a\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5b58\u5728\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u6bcf\u79cd\u200b\u90fd\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u6027\u8d28\u200b\u3002\u200b\u751f\u7269\u5b66\u200b\u795e\u7ecf\u5143\u200b\u4e2d\u200b\u7684\u200b\u6811\u7a81\u200b\u6267\u884c\u200b\u590d\u6742\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\u8ba1\u7b97\u200b\u3002\u200b\u7a81\u89e6\u200b\u4e0d\u4ec5\u4ec5\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u5b83\u4eec\u200b\u662f\u200b\u4e00\u4e2a\u200b\u590d\u6742\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\u52a8\u6001\u200b\u7cfb\u7edf\u200b\u3002\u200b\u8bb8\u591a\u200b\u7cfb\u7edf\u200b\u4e2d\u200b\u8f93\u51fa\u200b\u8109\u51b2\u200b\u7684\u200b\u786e\u5207\u200b\u65f6\u95f4\u200b\u5df2\u77e5\u200b\u662f\u200b\u5f88\u200b\u91cd\u8981\u200b\u7684\u200b\uff0c\u200b\u8fd9\u200b\u8868\u660e\u200b\u901f\u7387\u200b\u7f16\u7801\u200b\u8fd1\u4f3c\u200b\u53ef\u80fd\u200b\u4e0d\u200b\u6210\u7acb\u200b\u3002\u200b\u7531\u4e8e\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u548c\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u7684\u200b\u7b80\u5316\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u771f\u5b9e\u200b\u5927\u8111\u200b\u4e4b\u95f4\u200b\u8fdb\u884c\u200b\u7c7b\u6bd4\u200b\uff0c\u200b\u4efb\u4f55\u200b\u5177\u6709\u200b\u795e\u7ecf\u79d1\u5b66\u200b\u80cc\u666f\u200b\u7684\u200b\u4eba\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u51fa\u200b\u62b1\u6028\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u611f\u5174\u8da3\u200b\u7684\u8bdd\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u8fd9\u7bc7\u200b\u7efc\u8ff0\u200b\uff08pdf\uff09\uff0c\u200b\u6216\u8005\u200b\u6700\u8fd1\u200b\u8fd9\u7bc7\u200b\u7efc\u8ff0\u200b</p>"},{"location":"courses/neural_network/neural-networks-1/#_4","title":"\u5355\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u4f5c\u4e3a\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668","text":"<p>\u200b\u6a21\u578b\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6570\u5b66\u200b\u5f62\u5f0f\u200b\u53ef\u80fd\u200b\u5bf9\u200b\u4f60\u200b\u6765\u8bf4\u200b\u5f88\u200b\u719f\u6089\u200b\u3002\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5728\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u4e2d\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u6709\u200b\u80fd\u529b\u200b\u201c\u200b\u559c\u6b22\u200b\u201d\uff08\u200b\u6fc0\u6d3b\u200b\u63a5\u8fd1\u200b1\uff09\u200b\u6216\u200b\u201c\u200b\u4e0d\u200b\u559c\u6b22\u200b\u201d\uff08\u200b\u6fc0\u6d3b\u200b\u63a5\u8fd1\u200b0\uff09\u200b\u5176\u200b\u8f93\u5165\u200b\u7a7a\u95f4\u200b\u7684\u200b\u67d0\u4e9b\u200b\u7ebf\u6027\u200b\u533a\u57df\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5728\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u9002\u5f53\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5355\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\uff1a</p> <p>\u200b\u4e8c\u5143\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u89e3\u91ca\u200b\\(\\sigma(\\sum_iw_ix_i + b)\\)\u200b\u4e3a\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u6982\u7387\u200b\\(P(y_i = 1 \\mid x_i; w)\\)\u3002\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u6982\u7387\u200b\u4e3a\u200b\\(P(y_i = 0 \\mid x_i; w) = 1 - P(y_i = 1 \\mid x_i; w)\\)\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u7684\u200b\u603b\u548c\u200b\u5fc5\u987b\u200b\u4e3a\u200b1\u3002\u200b\u6709\u200b\u4e86\u200b\u8fd9\u79cd\u200b\u89e3\u91ca\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u5728\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\u90e8\u5206\u200b\u4e2d\u200b\u770b\u5230\u200b\u7684\u200b\u90a3\u6837\u200b\u5236\u5b9a\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\uff0c\u200b\u4f18\u5316\u200b\u5b83\u200b\u5c06\u200b\u5bfc\u81f4\u200b\u4e00\u4e2a\u4e8c\u5143\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u903b\u8f91\u200b\u56de\u5f52\u200blogistic regression\uff09\u3002\u200b\u7531\u4e8e\u200bSigmoid\u200b\u51fd\u6570\u200b\u88ab\u200b\u9650\u5236\u200b\u5728\u200b0-1\u200b\u4e4b\u95f4\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u9884\u6d4b\u200b\u57fa\u4e8e\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u662f\u5426\u200b\u5927\u4e8e\u200b0.5\u3002</p> <p>\u200b\u4e8c\u5143\u200bSVM\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u6216\u8005\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6700\u5927\u200b\u8fb9\u7f18\u200b\u94f0\u94fe\u200b\u635f\u5931\u200b\u9644\u52a0\u200b\u5230\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e0a\u200b\uff0c\u200b\u5e76\u200b\u8bad\u7ec3\u200b\u5b83\u200b\u6210\u4e3a\u200b\u4e00\u4e2a\u4e8c\u5143\u200b\u652f\u6301\u200b\u5411\u91cf\u200b\u673a\u200b\u3002</p> <p>\u200b\u6b63\u5219\u200b\u5316\u200b\u89e3\u91ca\u200b\u3002\u200b\u5728\u200bSVM/Softmax\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u5728\u200b\u8fd9\u79cd\u200b\u751f\u7269\u200b\u89c2\u70b9\u200b\u4e0b\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u9010\u6e10\u200b\u9057\u5fd8\u200bgradual forgetting\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4f1a\u200b\u5728\u200b\u6bcf\u6b21\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u540e\u200b\u4f7f\u200b\u6240\u6709\u200b\u7a81\u89e6\u200b\u6743\u91cd\u200b\\(w\\)\u200b\u8d8b\u5411\u4e8e\u200b\u96f6\u200b\u3002</p> <p>\u200b\u5355\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u5b9e\u73b0\u200b\u4e00\u4e2a\u4e8c\u5143\u200b\u5206\u7c7b\u5668\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e8c\u5143\u200bSoftmax\u200b\u6216\u200b\u4e8c\u5143\u200bSVM\u200b\u5206\u7c7b\u5668\u200b\uff09</p>"},{"location":"courses/neural_network/neural-networks-1/#_5","title":"\u5e38\u7528\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570","text":"<p>\u200b\u6bcf\u4e2a\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u6216\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff09\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u67d0\u79cd\u200b\u56fa\u5b9a\u200b\u7684\u200b\u6570\u5b66\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u51e0\u79cd\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff1a</p> <p> </p> <p>\u200b\u5de6\u200b\uff1a Sigmoid \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u5c06\u200b\u5b9e\u6570\u200b\u538b\u7f29\u200b\u5230\u200b [0,1] \u200b\u8303\u56f4\u200b\u5185\u200b \u200b\u53f3\u200b\uff1a tanh \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u5c06\u200b\u5b9e\u6570\u200b\u538b\u7f29\u200b\u5230\u200b [-1,1] \u200b\u8303\u56f4\u200b\u5185\u200b\u3002</p> <p>Sigmoid\u3002 Sigmoid \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u7684\u200b\u6570\u5b66\u200b\u5f62\u5f0f\u200b\u4e3a\u200b \\(\\sigma(x) = 1 / (1 + e^{-x})\\)\uff0c\u200b\u5982\u4e0a\u56fe\u200b\u5de6\u4fa7\u200b\u6240\u793a\u200b\u3002\u200b\u6b63\u5982\u200b\u524d\u200b\u4e00\u8282\u200b\u6240\u200b\u63d0\u5230\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u5b9e\u503c\u200b\u6570\u5b57\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u201c\u200b\u538b\u7f29\u200b\u201d\u200b\u5230\u200b0\u200b\u548c\u200b1\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8303\u56f4\u200b\u3002\u200b\u7279\u522b\u200b\u5730\u200b\uff0c\u200b\u5927\u200b\u7684\u200b\u8d1f\u6570\u200b\u53d8\u4e3a\u200b0\uff0c\u200b\u5927\u200b\u7684\u200b\u6b63\u6570\u200b\u53d8\u4e3a\u200b1\u3002\u200b\u7531\u4e8e\u200bSigmoid\u200b\u51fd\u6570\u200b\u5728\u5386\u53f2\u4e0a\u200b\u7ecf\u5e38\u200b\u88ab\u200b\u4f7f\u7528\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u53d1\u5c04\u7387\u200b\uff1a\u200b\u4ece\u200b\u5b8c\u5168\u200b\u4e0d\u200b\u53d1\u5c04\u200b\uff080\uff09\u200b\u5230\u200b\u5b8c\u5168\u200b\u9971\u548c\u200b\u7684\u200b\u6700\u5927\u200b\u9891\u7387\u200b\u53d1\u5c04\u200b\uff081\uff09\u3002\u200b\u7136\u800c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0cSigmoid\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u8fd1\u671f\u200b\u5df2\u7ecf\u200b\u4e0d\u518d\u200b\u53d7\u6b22\u8fce\u200b\uff0c\u200b\u5f88\u5c11\u200b\u88ab\u200b\u4f7f\u7528\u200b\u3002\u200b\u5b83\u200b\u6709\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u7684\u200b\u7f3a\u70b9\u200b\uff1a</p> <ul> <li>Sigmoids \u200b\u9971\u548c\u200b\u5e76\u200b\u6740\u6b7b\u200b\u68af\u5ea6\u200b\u3002Sigmoid\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u4e0d\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u5c5e\u6027\u200b\u662f\u200b\uff0c\u200b\u5f53\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u5728\u200b0\u200b\u6216\u200b1\u200b\u7684\u200b\u4e24\u7aef\u200b\u9971\u548c\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u533a\u57df\u200b\u7684\u200b\u68af\u5ea6\u200b\u51e0\u4e4e\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u56de\u60f3\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\uff08\u200b\u5c40\u90e8\u200b\uff09\u200b\u68af\u5ea6\u200b\u5c06\u200b\u4e58\u4ee5\u200b\u6574\u4e2a\u200b\u76ee\u6807\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u95e8\u200b\u7684\u200b\u8f93\u51fa\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u6709\u6548\u200b\u5730\u200b\u201c\u200b\u6740\u6b7b\u200b\u201d\u200b\u68af\u5ea6\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u6ca1\u6709\u200b\u4fe1\u53f7\u200b\u4f1a\u200b\u6d41\u7ecf\u200b\u795e\u7ecf\u5143\u200b\u5230\u200b\u5176\u200b\u6743\u91cd\u200b\uff0c\u200b\u9012\u5f52\u200b\u5730\u5230\u200b\u5176\u200b\u6570\u636e\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u521d\u59cb\u5316\u200bSigmoid\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6743\u91cd\u200b\u65f6\u200b\u5fc5\u987b\u200b\u7279\u522b\u200b\u5c0f\u5fc3\u200b\uff0c\u200b\u4ee5\u200b\u9632\u6b62\u200b\u9971\u548c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u521d\u59cb\u200b\u6743\u91cd\u200b\u8fc7\u5927\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5927\u591a\u6570\u200b\u795e\u7ecf\u5143\u200b\u5c06\u200b\u53d8\u5f97\u200b\u9971\u548c\u200b\uff0c\u200b\u7f51\u7edc\u200b\u51e0\u4e4e\u200b\u4e0d\u4f1a\u200b\u5b66\u4e60\u200b\u3002</li> <li>Sigmoid \u200b\u8f93\u51fa\u200b\u4e0d\u662f\u200b\u96f6\u200b\u4e2d\u5fc3\u200b\u7684\u200b\u3002\u200b\u8fd9\u200b\u662f\u200b\u4e0d\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u540e\u7eed\u200b\u5904\u7406\u200b\u5c42\u4e2d\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u8bb2\u200b\u5230\u200b\uff09\u200b\u5c06\u200b\u63a5\u6536\u200b\u5230\u200b\u7684\u200b\u6570\u636e\u200b\u4e0d\u662f\u200b\u96f6\u200b\u4e2d\u5fc3\u200b\u7684\u200b\u3002\u200b\u8fd9\u200b\u5bf9\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u7684\u200b\u52a8\u6001\u6027\u200b\u6709\u200b\u5f71\u54cd\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5982\u679c\u200b\u8fdb\u5165\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6570\u636e\u200b\u603b\u662f\u200b\u6b63\u200b\u7684\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b \\(f = w^Tx + b\\) \u200b\u4e2d\u200b \\(x &gt; 0\\) \u200b\u9010\u200b\u5143\u7d20\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6743\u91cd\u200b \\(w\\) \u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c06\u200b\u53d8\u5f97\u200b\u5168\u90e8\u200b\u4e3a\u200b\u6b63\u200b\u6216\u200b\u5168\u90e8\u200b\u4e3a\u200b\u8d1f\u200b\uff08\u200b\u53d6\u51b3\u4e8e\u200b\u6574\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b \\(f\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\uff09\u3002\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u6743\u91cd\u200b\u7684\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u4e2d\u200b\u5f15\u5165\u200b\u4e0d\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u952f\u9f7f\u72b6\u200b\u52a8\u6001\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4e00\u65e6\u200b\u8fd9\u4e9b\u200b\u68af\u5ea6\u200b\u5728\u200b\u4e00\u6279\u200b\u6570\u636e\u200b\u4e0a\u52a0\u200b\u8d77\u6765\u200b\uff0c\u200b\u6743\u91cd\u200b\u7684\u200b\u6700\u7ec8\u200b\u66f4\u65b0\u200b\u53ef\u4ee5\u200b\u6709\u200b\u53ef\u53d8\u200b\u7684\u200b\u7b26\u53f7\u200b\uff0c\u200b\u8fd9\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u7f13\u89e3\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u4fbf\u200b\uff0c\u200b\u4f46\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u7684\u200b\u9971\u548c\u200b\u6fc0\u6d3b\u200b\u95ee\u9898\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u540e\u679c\u200b\u8f83\u4e3a\u200b\u8f7b\u5fae\u200b\u3002</li> </ul> <p>Tanh\u3002 Tanh \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u5982\u4e0a\u56fe\u200b\u53f3\u4fa7\u200b\u6240\u793a\u200b\u3002\u200b\u5b83\u200b\u5c06\u200b\u5b9e\u503c\u200b\u6570\u5b57\u200b\u538b\u7f29\u200b\u5230\u200b[-1, 1]\u200b\u8303\u56f4\u200b\u3002\u200b\u50cf\u200bSigmoid\u200b\u795e\u7ecf\u5143\u200b\u4e00\u6837\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u4f1a\u200b\u9971\u548c\u200b\uff0c\u200b\u4f46\u200b\u4e0e\u200bSigmoid\u200b\u795e\u7ecf\u5143\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u8f93\u51fa\u200b\u662f\u200b\u96f6\u200b\u4e2d\u5fc3\u200b\u7684\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0ctanh \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u603b\u662f\u200b\u4f18\u4e8e\u200b Sigmoid \u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u3002\u200b\u8fd8\u8981\u200b\u6ce8\u610f\u200b\uff0ctanh \u200b\u795e\u7ecf\u5143\u200b\u53ea\u662f\u200b\u4e00\u4e2a\u200b\u7f29\u653e\u200b\u7684\u200bSigmoid\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u7279\u522b\u200b\u5730\u200b\uff0c\u200b\u4ee5\u4e0b\u200b\u5173\u7cfb\u200b\u6210\u7acb\u200b\uff1a\\(\\tanh(x) = 2 \\sigma(2x) -1\\)\u3002</p> <p> </p> <p>\u200b\u5de6\u200b\uff1a \u200b\u4fee\u6b63\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b\uff08ReLU\uff09\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u5f53\u200b x &lt; 0 \u200b\u65f6\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u5f53\u200b x &gt; 0 \u200b\u65f6\u200b\u4e0e\u200b\u659c\u7387\u200b\u4e3a\u200b1\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\u76f8\u540c\u200b\u3002 \u200b\u53f3\u200b\uff1a \u200b\u6765\u81ea\u200b Krizhevsky \u200b\u7b49\u200b\u4eba\u200b\uff08pdf\uff09\u200b\u7684\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u56fe\u8868\u200b\uff0c\u200b\u663e\u793a\u200b\u4e0e\u200b tanh \u200b\u5355\u5143\u200b\u76f8\u6bd4\u200b\uff0cReLU \u200b\u5355\u5143\u200b\u5728\u200b\u6536\u655b\u200b\u4e0a\u200b\u6709\u200b6\u200b\u500d\u200b\u7684\u200b\u6539\u8fdb\u200b\u3002</p> <p>ReLU\u3002 \u200b\u5728\u200b\u8fc7\u53bb\u200b\u7684\u200b\u51e0\u5e74\u200b\u4e2d\u200b\uff0c\u200b\u4fee\u6b63\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b\uff08ReLU\uff09\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u53d7\u6b22\u8fce\u200b\u3002\u200b\u5b83\u200b\u8ba1\u7b97\u200b\u7684\u200b\u51fd\u6570\u200b\u4e3a\u200b \\(f(x) = \\max(0, x)\\)\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6fc0\u6d3b\u200b\u7b80\u5355\u200b\u5730\u200b\u5728\u200b\u96f6\u5904\u200b\u9608\u503c\u200b\u5316\u200b\uff08\u200b\u5982\u4e0a\u56fe\u200b\u5de6\u4fa7\u200b\u6240\u793a\u200b\uff09\u3002\u200b\u4f7f\u7528\u200bReLU\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u4f18\u7f3a\u70b9\u200b\uff1a</p> <ul> <li>(+) \u200b\u4e0e\u200bsigmoid/tanh\u200b\u51fd\u6570\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u53d1\u73b0\u200bReLU\u200b\u5927\u5927\u200b\u52a0\u901f\u200b\u4e86\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u7684\u200b\u6536\u655b\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cKrizhevsky\u200b\u7b49\u200b\u4eba\u200b\u4e2d\u200b\u7684\u200b6\u200b\u500d\u200b\u56e0\u5b50\u200b\uff09\u3002\u200b\u4eba\u4eec\u200b\u8ba4\u4e3a\u200b\u8fd9\u662f\u200b\u7531\u4e8e\u200b\u5176\u200b\u7ebf\u6027\u200b\u3001\u200b\u975e\u9971\u548c\u200b\u5f62\u5f0f\u200b\u3002</li> <li>(+) \u200b\u4e0e\u200b\u6d89\u53ca\u200b\u6602\u8d35\u200b\u64cd\u4f5c\u200b\u7684\u200btanh/sigmoid\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u6307\u6570\u200b\u7b49\u200b\uff09\u200b\u76f8\u6bd4\u200b\uff0cReLU\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7b80\u5355\u200b\u5730\u200b\u5728\u200b\u96f6\u5904\u200b\u9608\u503c\u200b\u5316\u200b\u6fc0\u6d3b\u200b\u77e9\u9635\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</li> <li>(-) \u200b\u4e0d\u5e78\u200b\u7684\u200b\u662f\u200b\uff0cReLU\u200b\u5355\u5143\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u8106\u5f31\u200b\u5e76\u4e14\u200b\u53ef\u80fd\u200b\u201c\u200b\u6b7b\u4ea1\u200b\u201d\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6d41\u7ecf\u200bReLU\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u5927\u200b\u68af\u5ea6\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u6743\u91cd\u200b\u66f4\u65b0\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u795e\u7ecf\u5143\u200b\u6c38\u8fdc\u200b\u4e0d\u4f1a\u200b\u5728\u200b\u4efb\u4f55\u200b\u6570\u636e\u200b\u70b9\u4e0a\u200b\u6fc0\u6d3b\u200b\u3002\u200b\u5982\u679c\u200b\u53d1\u751f\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4ece\u90a3\u65f6\u8d77\u200b\uff0c\u200b\u6d41\u7ecf\u200b\u8be5\u200b\u5355\u5143\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c06\u200b\u6c38\u8fdc\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0cReLU\u200b\u5355\u5143\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4e0d\u200b\u53ef\u9006\u200b\u5730\u200b\u6b7b\u4ea1\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u88ab\u200b\u4ece\u200b\u6570\u636e\u6d41\u200b\u4e2d\u200b\u79fb\u9664\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u5f97\u200b\u8fc7\u200b\u9ad8\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u4f60\u200b\u7684\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u591a\u8fbe\u200b40%\u200b\u7684\u200b\u90e8\u5206\u200b\u53ef\u80fd\u200b\u662f\u200b\u201c\u200b\u6b7b\u200b\u7684\u200b\u201d\uff08\u200b\u5373\u200b\uff0c\u200b\u5728\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u4ece\u672a\u200b\u6fc0\u6d3b\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\uff09\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u901a\u8fc7\u200b\u9002\u5f53\u200b\u5730\u200b\u8bbe\u7f6e\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5c31\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u9891\u7e41\u200b\u4e86\u200b\u3002</li> </ul> <p>Leaky ReLU\u3002 Leaky ReLU\u200b\u662f\u200b\u4fee\u590d\u200b\u201c\u200b\u6b7b\u4ea1\u200bReLU\u201d\u200b\u95ee\u9898\u200b\u7684\u200b\u4e00\u79cd\u200b\u5c1d\u8bd5\u200b\u3002\u200b\u5f53\u200bx &lt; 0\u200b\u65f6\u200b\uff0c\u200b\u51fd\u6570\u200b\u4e0d\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u800c\u662f\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u7684\u200b\u6b63\u200b\u659c\u7387\u200b\uff08\u200b\u5982\u200b0.01\uff09\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b \\(f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x)\\)\uff0c\u200b\u5176\u4e2d\u200b\\(\\alpha\\)\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u5e38\u6570\u200b\u3002\u200b\u6709\u4e9b\u200b\u4eba\u200b\u62a5\u544a\u200b\u8bf4\u200b\u8fd9\u79cd\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5f88\u200b\u6210\u529f\u200b\uff0c\u200b\u4f46\u200b\u7ed3\u679c\u200b\u5e76\u200b\u4e0d\u200b\u603b\u662f\u200b\u4e00\u81f4\u200b\u7684\u200b\u3002\u200b\u5728\u200b2015\u200b\u5e74\u200b\u7531\u200bKaiming He\u200b\u7b49\u200b\u4eba\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u6df1\u5165\u7814\u7a76\u200b\u6574\u6d41\u5668\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8d1f\u200b\u533a\u57df\u200b\u7684\u200b\u659c\u7387\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f5c\u4e3a\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200bPReLU\u200b\u795e\u7ecf\u5143\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u597d\u5904\u200b\u5728\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\u76ee\u524d\u200b\u5c1a\u200b\u4e0d\u200b\u6e05\u695a\u200b\u3002</p> <p>Maxout\u3002\u200b\u5df2\u7ecf\u200b\u63d0\u51fa\u200b\u4e86\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u7684\u200b\u5355\u5143\u200b\uff0c\u200b\u5b83\u4eec\u200b\u6ca1\u6709\u200b\u51fd\u6570\u200b\u5f62\u5f0f\u200b \\(f(w^Tx + b)\\)\uff0c\u200b\u5176\u4e2d\u200b\u5728\u200b\u6743\u91cd\u200b\u548c\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u70b9\u79ef\u200b\u4e0a\u200b\u5e94\u7528\u200b\u975e\u7ebf\u6027\u200b\u3002\u200b\u4e00\u4e2a\u200b\u76f8\u5bf9\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u9009\u62e9\u200b\u662f\u200bMaxout\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u6700\u8fd1\u200b\u7531\u200bGoodfellow\u200b\u7b49\u200b\u4eba\u200b\u5f15\u5165\u200b\uff09\uff0c\u200b\u5b83\u200b\u6982\u62ec\u200b\u4e86\u200bReLU\u200b\u53ca\u5176\u200bleaky\u200b\u7248\u672c\u200b\u3002Maxout\u200b\u795e\u7ecf\u5143\u200b\u8ba1\u7b97\u200b\u51fd\u6570\u200b\\(\\max(w_1^Tx+b_1, w_2^Tx + b_2)\\)\u3002\u200b\u6ce8\u610f\u200b\uff0cReLU\u200b\u548c\u200bLeaky ReLU\u200b\u90fd\u200b\u662f\u200b\u8fd9\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u7279\u4f8b\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200bReLU\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\\(w_1, b_1 = 0\\)\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0cMaxout\u200b\u795e\u7ecf\u5143\u200b\u4eab\u6709\u200bReLU\u200b\u5355\u5143\u200b\u7684\u200b\u6240\u6709\u200b\u597d\u5904\u200b\uff08\u200b\u64cd\u4f5c\u200b\u7684\u200b\u7ebf\u6027\u200b\u533a\u57df\u200b\uff0c\u200b\u6ca1\u6709\u200b\u9971\u548c\u200b\uff09\u200b\u5e76\u4e14\u200b\u6ca1\u6709\u200b\u5176\u200b\u7f3a\u70b9\u200b\uff08\u200b\u6b7b\u4ea1\u200bReLU\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e0e\u200bReLU\u200b\u795e\u7ecf\u5143\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5b83\u200b\u4f7f\u200b\u6bcf\u4e2a\u200b\u5355\u4e00\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u7ffb\u500d\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u53c2\u6570\u200b\u603b\u6570\u200b\u5f88\u200b\u9ad8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u7ed3\u675f\u200b\u4e86\u200b\u6211\u4eec\u200b\u5bf9\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u53ca\u5176\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u8ba8\u8bba\u200b\u3002\u200b\u6700\u540e\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5f88\u5c11\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6df7\u5408\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u8fd9\u6837\u200b\u505a\u200b\u6ca1\u6709\u200b\u6839\u672c\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b\uff1a\u201c\u200b\u6211\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u54ea\u200b\u79cd\u200b\u795e\u7ecf\u5143\u200b\u7c7b\u578b\u200b\uff1f\u201d \u200b\u4f7f\u7528\u200bReLU\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u5c0f\u5fc3\u200b\u4f60\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u5e76\u200b\u53ef\u80fd\u200b\u76d1\u63a7\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u201c\u200b\u6b7b\u4ea1\u200b\u201d\u200b\u5355\u5143\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u3002\u200b\u5982\u679c\u200b\u8fd9\u200b\u8ba9\u200b\u4f60\u200b\u62c5\u5fe7\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200bLeaky ReLU\u200b\u6216\u200bMaxout\u3002\u200b\u6c38\u8fdc\u200b\u4e0d\u8981\u200b\u4f7f\u7528\u200bsigmoid\u3002\u200b\u5c1d\u8bd5\u200btanh\uff0c\u200b\u4f46\u200b\u671f\u671b\u200b\u5b83\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6548\u679c\u200b\u6bd4\u200bReLU/Maxout\u200b\u5dee\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_6","title":"\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784","text":""},{"location":"courses/neural_network/neural-networks-1/#_7","title":"\u5206\u5c42\u200b\u7ec4\u7ec7","text":"<p>\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4f5c\u4e3a\u200b\u56fe\u4e2d\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u88ab\u200b\u6a21\u578b\u200b\u5316\u4e3a\u200b\u4ee5\u975e\u200b\u5faa\u73af\u200b\u56fe\u200b\u5f62\u5f0f\u200b\u8fde\u63a5\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u96c6\u5408\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u67d0\u4e9b\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u53ef\u4ee5\u200b\u6210\u4e3a\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u4e0d\u200b\u5141\u8bb8\u200b\u6709\u200b\u5faa\u73af\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u5c06\u200b\u610f\u5473\u7740\u200b\u7f51\u7edc\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u7684\u200b\u65e0\u9650\u200b\u5faa\u73af\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u88ab\u200b\u7ec4\u7ec7\u200b\u6210\u200b\u4e0d\u540c\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5e38\u89c4\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u5c42\u200b\u7c7b\u578b\u200b\u662f\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200bfully-connected layer\uff0c\u200b\u5176\u4e2d\u200b\u4e24\u4e2a\u200b\u76f8\u90bb\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u662f\u200b\u5b8c\u5168\u200b\u6210\u200b\u5bf9\u200b\u8fde\u63a5\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5355\u4e2a\u200b\u5c42\u5185\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u6ca1\u6709\u200b\u5171\u4eab\u200b\u8fde\u63a5\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4f7f\u7528\u200b\u4e00\u5806\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u7684\u200b\u4e24\u4e2a\u200b\u793a\u4f8b\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u62d3\u6251\u200b\u7ed3\u6784\u200b\uff1a</p> <p></p> <p>\u200b\u5de6\u200b\uff1a \u200b\u4e00\u4e2a\u200b2\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b4\u200b\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u6216\u200b\u5355\u5143\u200b\uff09\u200b\u7684\u200b\u9690\u85cf\u200b\u5c42\u200b\u548c\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b2\u200b\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff09\uff0c\u200b\u4ee5\u53ca\u200b\u4e09\u4e2a\u200b\u8f93\u5165\u200b\u3002 \u200b\u53f3\u200b\uff1a \u200b\u4e00\u4e2a\u200b3\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u6709\u200b\u4e09\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u4e24\u4e2a\u200b\u6bcf\u5c42\u200b\u6709\u200b4\u200b\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u9690\u85cf\u200b\u5c42\u200b\u548c\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5c42\u200b\u4e0e\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u6709\u200b\u8fde\u63a5\u200b\uff08\u200b\u7a81\u89e6\u200b\uff09\uff0c\u200b\u4f46\u5c42\u200b\u5185\u90e8\u200b\u6ca1\u6709\u200b\u8fde\u63a5\u200b\u3002</p> <p>\u200b\u547d\u540d\u200b\u7ea6\u5b9a\u200b\u3002 \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u8bf4\u200bN\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\u5c42\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5355\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u63cf\u8ff0\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6ca1\u6709\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u7f51\u7edc\u200b\uff08\u200b\u8f93\u5165\u200b\u76f4\u63a5\u200b\u6620\u5c04\u200b\u5230\u200b\u8f93\u51fa\u200b\uff09\u3002\u200b\u4ece\u200b\u8fd9\u4e2a\u200b\u610f\u4e49\u200b\u4e0a\u200b\u8bf4\u200b\uff0c\u200b\u4f60\u200b\u6709\u65f6\u200b\u4f1a\u200b\u542c\u5230\u200b\u4eba\u4eec\u200b\u8bf4\u200b\u903b\u8f91\u200b\u56de\u5f52\u200b\u6216\u200bSVMs\u200b\u53ea\u662f\u200b\u5355\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7279\u4f8b\u200b\u3002\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u542c\u5230\u200b\u8fd9\u4e9b\u200b\u7f51\u7edc\u200b\u88ab\u200b\u4ea4\u66ff\u200b\u5730\u200b\u79f0\u4e3a\u200b\"\u200b\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u200bArtificial Neural Networks\"\uff08ANN\uff09\u200b\u6216\u200b\"\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200bMulti-Layer Perceptrons\"\uff08MLP\uff09\u3002\u200b\u8bb8\u591a\u200b\u4eba\u200b\u4e0d\u200b\u559c\u6b22\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u771f\u5b9e\u200b\u5927\u8111\u200b\u4e4b\u95f4\u200b\u7684\u200b\u7c7b\u6bd4\u200b\uff0c\u200b\u5e76\u200b\u66f4\u200b\u559c\u6b22\u200b\u5c06\u200b\u795e\u7ecf\u5143\u200b\u79f0\u4e3a\u200b\u5355\u5143\u200b\u3002</p> <p>\u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002 \u200b\u4e0e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u5c42\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5c42\u200b\u795e\u7ecf\u5143\u200b\u901a\u5e38\u200b\u6ca1\u6709\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u6216\u8005\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u5b83\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u8eab\u4efd\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff09\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6700\u540e\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\u901a\u5e38\u200b\u88ab\u200b\u7528\u6765\u200b\u8868\u793a\u200b\u7c7b\u200b\u5f97\u5206\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u5206\u7c7b\u200b\u4e2d\u200b\uff09\uff0c\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\u5b9e\u503c\u200b\u6570\u5b57\u200b\uff0c\u200b\u6216\u200b\u67d0\u79cd\u200b\u5b9e\u503c\u200b\u76ee\u6807\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u56de\u5f52\u200b\u4e2d\u200b\uff09\u3002</p> <p>\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002\u200b\u4eba\u4eec\u200b\u5e38\u7528\u200b\u6765\u200b\u8861\u91cf\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5927\u5c0f\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6307\u6807\u200b\u662f\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u6216\u200b\u66f4\u200b\u5e38\u89c1\u200b\u7684\u200b\u662f\u200b\u53c2\u6570\u200b\u7684\u200b\u6570\u91cf\u200b\u3002\u200b\u4f7f\u7528\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u7684\u200b\u4e24\u4e2a\u200b\u793a\u4f8b\u200b\u7f51\u7edc\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff08\u200b\u5de6\u200b\uff09\u200b\u6709\u200b4 + 2 = 6\u200b\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u4e0d\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\uff09\uff0c[3 x 4] + [4 x 2] = 20\u200b\u4e2a\u200b\u6743\u91cd\u200b\u548c\u200b4 + 2 = 6\u200b\u4e2a\u200b\u504f\u7f6e\u200b\uff0c\u200b\u603b\u5171\u200b\u6709\u200b26\u200b\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</li> <li>\u200b\u7b2c\u4e8c\u4e2a\u200b\u7f51\u7edc\u200b\uff08\u200b\u53f3\u200b\uff09\u200b\u6709\u200b4 + 4 + 1 = 9\u200b\u4e2a\u200b\u795e\u7ecf\u5143\u200b\uff0c[3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32\u200b\u4e2a\u200b\u6743\u91cd\u200b\u548c\u200b4 + 4 + 1 = 9\u200b\u4e2a\u200b\u504f\u5dee\u200b\uff0c\u200b\u603b\u5171\u200b\u6709\u200b41\u200b\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</li> </ul> <p>\u200b\u7ed9\u200b\u4f60\u200b\u4e00\u4e9b\u200b\u80cc\u666f\u200b\uff0c\u200b\u73b0\u4ee3\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\u5305\u542b\u200b\u5927\u7ea6\u200b1\u200b\u4ebf\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u901a\u5e38\u200b\u7531\u200b\u5927\u7ea6\u200b10-20\u200b\u5c42\u200b\u7ec4\u6210\u200b\uff08\u200b\u56e0\u6b64\u200b\u53eb\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u7531\u4e8e\u200b\u53c2\u6570\u200b\u5171\u4eab\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u770b\u5230\u200b\u6709\u6548\u200beffective\u200b\u8fde\u63a5\u200b\u7684\u200b\u6570\u91cf\u200b\u663e\u8457\u200b\u589e\u52a0\u200b\u3002\u200b\u5728\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u5757\u200b\u4e2d\u4f1a\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_8","title":"\u524d\u5411\u200b\u8ba1\u7b97\u200b\u4f8b\u5b50","text":"<p>\u200b\u4ea4\u7ec7\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u91cd\u590d\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200bRepeated matrix multiplications interwoven with activation function\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u88ab\u200b\u7ec4\u7ec7\u200b\u6210\u5c42\u200b\u7684\u200b\u4e3b\u8981\u200b\u539f\u56e0\u200b\u4e4b\u4e00\u200b\u662f\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u7ed3\u6784\u200b\u4f7f\u5f97\u200b\u4f7f\u7528\u200b\u77e9\u9635\u200b\u5411\u91cf\u200b\u64cd\u4f5c\u200b\u8bc4\u4f30\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53d8\u5f97\u200b\u975e\u5e38\u7b80\u5355\u200b\u548c\u200b\u9ad8\u6548\u200b\u3002\u200b\u4f7f\u7528\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e09\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u8f93\u5165\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b[3x1]\u200b\u5411\u91cf\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u7684\u200b\u6240\u6709\u200b\u8fde\u63a5\u200b\u5f3a\u5ea6\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5b58\u50a8\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u77e9\u9635\u200b\u4e2d\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u6743\u91cd\u200b<code>W1</code>\u200b\u5c06\u200b\u662f\u200b[4x3]\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6240\u6709\u200b\u5355\u5143\u200b\u7684\u200b\u504f\u5dee\u200b\u5c06\u200b\u5728\u200b\u5411\u91cf\u200b<code>b1</code>\u200b\u4e2d\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b[4x1]\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5355\u4e00\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u5728\u200b<code>W1</code>\u200b\u7684\u200b\u4e00\u884c\u200b\u4e2d\u6709\u200b\u5176\u200b\u6743\u91cd\u200b\uff0c\u200b\u6240\u4ee5\u200b\u77e9\u9635\u200b\u5411\u91cf\u200b\u4e58\u6cd5\u200b<code>np.dot(W1,x)</code>\u200b\u8bc4\u4f30\u200b\u8be5\u5c42\u200b\u6240\u6709\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u3002\u200b\u540c\u6837\u200b\uff0c<code>W2</code>\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b[4x4]\u200b\u7684\u200b\u77e9\u9635\u200b\uff0c\u200b\u5b58\u50a8\u200b\u7b2c\u4e8c\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u8fde\u63a5\u200b\uff0c<code>W3</code>\u200b\u662f\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\uff08\u200b\u8f93\u51fa\u200b\uff09\u200b\u5c42\u200b\u7684\u200b[1x4]\u200b\u77e9\u9635\u200b\u3002\u200b\u7136\u540e\u200b\uff0c\u200b\u8fd9\u4e2a\u200b3\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5b8c\u6574\u200b\u524d\u5411\u200b\u4f20\u9012\u200b\u53ea\u662f\u200b\u4e09\u4e2a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u4ea4\u7ec7\u7740\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u5e94\u7528\u200b\uff1a</p> <pre><code># forward-pass of a 3-layer neural network:\nf = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\nx = np.random.randn(3, 1) # random input vector of three numbers (3x1)\nh1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\nh2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\nout = np.dot(W3, h2) + b3 # output neuron (1x1)\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c<code>W1\uff0cW2\uff0cW3\uff0cb1\uff0cb2\uff0cb3</code> \u200b\u662f\u200b\u7f51\u7edc\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\u3002\u200b\u8fd8\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4e0e\u5176\u200b\u62e5\u6709\u200b\u5355\u4e2a\u200b\u8f93\u5165\u200b\u5217\u200b\u5411\u91cf\u200b\u4e0d\u540c\u200b\uff0c\u200b\u53d8\u91cf\u200b <code>x</code> \u200b\u53ef\u4ee5\u200b\u5bb9\u7eb3\u200b\u6574\u4e2a\u200b\u6279\u6b21\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff08\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u8f93\u5165\u200b\u793a\u4f8b\u200b\u90fd\u200b\u5c06\u200b\u662f\u200b <code>x</code> \u200b\u7684\u200b\u4e00\u5217\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u5c06\u200b\u4ee5\u200b\u5e76\u884c\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u9ad8\u6548\u200b\u8bc4\u4f30\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6700\u540e\u200b\u7684\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u901a\u5e38\u200b\u6ca1\u6709\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u5206\u7c7b\u200b\u8bbe\u7f6e\u200b\u4e2d\u200b\u5b83\u200b\u4ee3\u8868\u200b\u4e00\u4e2a\u200b\uff08\u200b\u5b9e\u503c\u200b\uff09\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff09\u3002</p> <p>\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u540e\u200b\u8ddf\u200b\u504f\u7f6e\u200b\u504f\u79fb\u200b\u548c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_9","title":"\u8868\u793a\u200b\u80fd\u529b","text":"<p>\u200b\u89c2\u5bdf\u200b\u5177\u6709\u200b\u5b8c\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u662f\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u7ec4\u200b\u7531\u200b\u7f51\u7edc\u200b\u7684\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\u5316\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u4e00\u4e2a\u200b\u81ea\u7136\u800c\u7136\u200b\u7684\u200b\u95ee\u9898\u200b\u662f\u200b\uff1a\u200b\u8fd9\u7ec4\u200b\u51fd\u6570\u200b\u7684\u200b\u8868\u793a\u200b\u80fd\u529b\u200b\u5982\u4f55\u200b\uff1f\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u4e0d\u80fd\u200b\u7528\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5efa\u6a21\u200b\u7684\u200b\u51fd\u6570\u200b\uff1f</p> <p>\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u81f3\u5c11\u200b\u5177\u6709\u200b\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u901a\u7528\u200b\u903c\u8fd1\u200b\u5668\u200buniversal approximators\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8bc1\u660e\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u53c2\u89c1\u200b 1989 \u200b\u5e74\u200b\u7684\u200bSigmoidal Function\u200b\u7684\u200b\u53e0\u52a0\u200b\u903c\u8fd1\u200b\uff08pdf\uff09\uff0c\u200b\u6216\u8005\u200b\u662f\u200b Michael Nielsen \u200b\u7684\u200b\u8fd9\u4e2a\u200b\u76f4\u89c2\u200b\u89e3\u91ca\u200b\uff09\uff0c\u200b\u5bf9\u4e8e\u200b\u4efb\u4f55\u200b\u8fde\u7eed\u51fd\u6570\u200b \\(f(x)\\) \u200b\u548c\u200b\u4e00\u4e9b\u200b \\(\\epsilon &gt; 0\\)\uff0c\u200b\u90fd\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b \\(g(x)\\)\uff08\u200b\u5177\u6709\u200b\u5408\u7406\u200b\u9009\u62e9\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\uff0c\u200b\u4f8b\u5982\u200b sigmoid \u200b\u51fd\u6570\u200b\uff09\uff0c\u200b\u4f7f\u5f97\u200b $ \\forall x, \\mid f(x) - g(x) \\mid &lt; \\epsilon $\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ef\u4ee5\u200b\u903c\u8fd1\u200b\u4efb\u4f55\u200b\u8fde\u7eed\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u8db3\u4ee5\u200b\u903c\u8fd1\u200b\u4efb\u4f55\u200b\u51fd\u6570\u200b\uff0c\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u5c42\u200b\u5e76\u200b\u52a0\u6df1\u200b\u7f51\u7edc\u200b\u5462\u200b\uff1f\u200b\u7b54\u6848\u200b\u662f\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6570\u5b66\u200b\u4e0a\u200b\u53ef\u7231\u200b\uff0c\u200b\u4f46\u200b\u4e24\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u901a\u7528\u200b\u903c\u8fd1\u200b\u5668\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u662f\u200b\u4e00\u4e2a\u200b\u76f8\u5bf9\u200b\u5f31\u200b\u800c\u200b\u65e0\u7528\u200b\u7684\u200b\u8bf4\u6cd5\u200b\u3002\u200b\u5728\u200b\u4e00\u7ef4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u201c\u200b\u6307\u793a\u5668\u200b\u7a81\u8d77\u200b\u7684\u200b\u603b\u548c\u200bsum of indicator bumps\u201d\u200b\u51fd\u6570\u200b \\(g(x) = \\sum_i c_i \\mathbb{1}(a_i &lt; x &lt; b_i)\\)\uff0c\u200b\u5176\u4e2d\u200b \\(a,b,c\\) \u200b\u662f\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u901a\u7528\u200b\u903c\u8fd1\u200b\u5668\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u4eba\u4f1a\u200b\u5efa\u8bae\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u8fd9\u79cd\u200b\u51fd\u6570\u200b\u5f62\u5f0f\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u7d27\u51d1\u200b\u5730\u200b\u8868\u793a\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u9047\u5230\u200b\u7684\u200b\u6570\u636e\u200b\u7684\u200b\u7edf\u8ba1\u200b\u5c5e\u6027\u200b\u76f8\u7b26\u5408\u200b\u7684\u200b\u6f02\u4eae\u200b\u3001\u200b\u5e73\u6ed1\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u5bb9\u6613\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\uff08\u200b\u4f8b\u5982\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\u3002\u200b\u540c\u6837\u200b\uff0c\u200b\u6df1\u5c42\u200b\u7f51\u7edc\u200b\uff08\u200b\u5177\u6709\u200b\u591a\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\uff09\u200b\u53ef\u4ee5\u200b\u6bd4\u5355\u200b\u9690\u85cf\u200b\u5c42\u200b\u7f51\u7edc\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u5b83\u4eec\u200b\u7684\u200b\u8868\u793a\u200b\u80fd\u529b\u200b\u76f8\u7b49\u200b\u3002</p> <p>\u200b\u503c\u5f97\u4e00\u63d0\u7684\u662f\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c3\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4f1a\u200b\u4f18\u4e8e\u200b2\u200b\u5c42\u200b\u7f51\u7edc\u200b\uff0c\u200b\u4f46\u200b\u66f4\u200b\u6df1\u5c42\u6b21\u200b\uff084\u30015\u30016\u200b\u5c42\u200b\uff09\u200b\u5f88\u5c11\u200b\u6709\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u5e2e\u52a9\u200b\u3002\u200b\u8fd9\u200b\u4e0e\u200b\u5377\u79ef\u200b\u7f51\u7edc\u200b\u5f62\u6210\u200b\u4e86\u200b\u9c9c\u660e\u5bf9\u6bd4\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5728\u200b\u5176\u4e2d\u200b\u88ab\u200b\u53d1\u73b0\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6781\u5176\u91cd\u8981\u200b\u7684\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b\u51fa\u8272\u200b\u7684\u200b\u8bc6\u522b\u7cfb\u7edf\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7ea6\u200b\u6709\u200b10\u200b\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5c42\u6b21\u200b\uff09\u3002\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e00\u200b\u89c2\u5bdf\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8bba\u70b9\u200b\u662f\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5305\u542b\u200b\u5c42\u6b21\u7ed3\u6784\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8138\u200b\u7531\u200b\u773c\u775b\u200b\u7ec4\u6210\u200b\uff0c\u200b\u773c\u775b\u200b\u7531\u200b\u8fb9\u7f18\u200b\u7ec4\u6210\u200b\u7b49\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u9886\u57df\u200b\uff0c\u200b\u591a\u5c42\u6b21\u200b\u5904\u7406\u200b\u662f\u200b\u76f4\u89c2\u200b\u7684\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u5b8c\u6574\u200b\u7684\u200b\u6545\u4e8b\u200b\u8fdc\u6bd4\u200b\u8fd9\u200b\u590d\u6742\u200b\u5f97\u200b\u591a\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u6700\u8fd1\u200b\u7814\u7a76\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u4e3b\u9898\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u8bdd\u9898\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5efa\u8bae\u200b\u8fdb\u4e00\u6b65\u200b\u9605\u8bfb\u200b\uff1a</p> <ul> <li>Bengio\u3001Goodfellow\u3001Courville \u200b\u7f16\u5199\u200b\u7684\u200bDeep Learning\u200b\u4e66\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u7b2c\u200b6.4\u200b\u7ae0\u200b\u3002</li> <li>\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u771f\u7684\u200b\u9700\u8981\u200b\u6df1\u200b\u5417\u200b\uff1f</li> <li>FitNets: Thin Deep Nets\u200b\u7684\u200b\u63d0\u793a\u200b</li> </ul>"},{"location":"courses/neural_network/neural-networks-1/#_10","title":"\u8bbe\u7f6e\u200b\u5c42\u6570\u200b\u548c\u200b\u5b83\u4eec\u200b\u7684\u200b\u5927\u5c0f","text":"<p>\u200b\u5f53\u200b\u9762\u4e34\u200b\u5b9e\u9645\u200b\u95ee\u9898\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u51b3\u5b9a\u200b\u4f7f\u7528\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u4f53\u7cfb\u7ed3\u6784\u200b\uff1f\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u9690\u85cf\u200b\u5c42\u200b\u5417\u200b\uff1f\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\uff1f\u200b\u4e24\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\uff1f\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u5e94\u8be5\u200b\u6709\u200b\u591a\u200b\u5927\u200b\uff1f\u200b\u9996\u5148\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u968f\u7740\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u5c42\u200b\u7684\u200b\u5927\u5c0f\u200b\u548c\u200b\u6570\u91cf\u200b\u7684\u200b\u589e\u52a0\u200b\uff0c\u200b\u7f51\u7edc\u200b\u7684\u200b\u5bb9\u91cf\u200bcapacity\u200b\u4e5f\u200b\u4f1a\u200b\u589e\u52a0\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u53ef\u200b\u8868\u793a\u200b\u51fd\u6570\u200b\u7684\u200b\u7a7a\u95f4\u200b\u4f1a\u200b\u589e\u5927\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u795e\u7ecf\u5143\u200b\u53ef\u4ee5\u200b\u534f\u4f5c\u200b\u6765\u200b\u8868\u793a\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u5728\u200b\u4e8c\u7ef4\u200b\u4e0a\u200b\u6709\u200b\u4e00\u4e2a\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8bad\u7ec3\u200b\u4e09\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u67d0\u4e2a\u200b\u5927\u5c0f\u200b\uff0c\u200b\u5e76\u200b\u5f97\u5230\u200b\u4ee5\u4e0b\u200b\u5206\u7c7b\u5668\u200b\uff1a</p> <p></p> <p>\u200b\u66f4\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u6570\u636e\u200b\u663e\u793a\u200b\u4e3a\u200b\u6309\u200b\u5176\u200b\u7c7b\u522b\u200b\u7740\u8272\u200b\u7684\u200b\u5706\u5708\u200b\uff0c\u200b\u800c\u200b\u7ecf\u8fc7\u8bad\u7ec3\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u51b3\u7b56\u200b\u533a\u57df\u200b\u663e\u793a\u200b\u5728\u200b\u4e0b\u65b9\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u4e2a\u200bConvNetsJS\u200b\u6f14\u793a\u200b\u94fe\u63a5\u200b\u4e2d\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u8fdb\u884c\u200b\u4e92\u52a8\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u56fe\u8868\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5177\u6709\u200b\u66f4\u200b\u591a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u200b\u65e2\u200b\u662f\u200b\u4e00\u79cd\u200b\u597d\u5904\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5b66\u4f1a\u200b\u5206\u7c7b\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u4e5f\u200b\u662f\u200b\u4e00\u79cd\u200b\u8bc5\u5492\u200b\uff08\u200b\u56e0\u4e3a\u200b\u66f4\u200b\u5bb9\u6613\u200b\u8fc7\u200b\u62df\u5408\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff09\u3002\u200b\u8fc7\u200b\u62df\u5408\u200bOverfitting\u200b\u53d1\u751f\u200b\u5728\u200b\u9ad8\u5bb9\u91cf\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u566a\u58f0\u200b\u800c\u200b\u4e0d\u662f\u200b\uff08\u200b\u5047\u5b9a\u200b\u7684\u200b\uff09\u200b\u6f5c\u5728\u200b\u5173\u7cfb\u200b\u65f6\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5177\u6709\u200b20\u200b\u4e2a\u200b\u9690\u85cf\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u4e86\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u4f46\u200b\u4ee5\u200b\u5c06\u200b\u7a7a\u95f4\u200b\u5206\u5272\u200b\u6210\u200b\u8bb8\u591a\u200b\u4e0d\u200b\u76f8\u4ea4\u200b\u7684\u200b\u7ea2\u8272\u200b\u548c\u200b\u7eff\u8272\u200b\u51b3\u7b56\u200b\u533a\u57df\u200b\u4e3a\u200b\u4ee3\u4ef7\u200b\u3002\u200b\u5177\u6709\u200b3\u200b\u4e2a\u200b\u9690\u85cf\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u6709\u200b\u5728\u200b\u5b8f\u89c2\u200b\u4e0a\u200b\u5177\u6709\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u7684\u200b\u8868\u73b0\u200b\u80fd\u529b\u200b\u3002\u200b\u5b83\u200b\u5c06\u200b\u6570\u636e\u200b\u5efa\u6a21\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u6591\u70b9\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u7eff\u8272\u200b\u96c6\u7fa4\u200b\u5185\u200b\u7684\u200b\u5c11\u6570\u200b\u7ea2\u200b\u70b9\u200b\u89e3\u91ca\u200b\u4e3a\u200b\u5f02\u5e38\u200b\u503c\u200boutliers\uff08\u200b\u566a\u58f0\u200b\uff09\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u66f4\u597d\u200b\u7684\u200b\u6cdb\u5316\u200bgeneralization\u3002</p> <p>\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u7684\u200b\u8ba8\u8bba\u200b\uff0c\u200b\u4f3c\u4e4e\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u4e0d\u591f\u200b\u590d\u6742\u200b\u4ee5\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u90a3\u4e48\u200b\u53ef\u4ee5\u200b\u9996\u9009\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b \u2014\u2014\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u9996\u9009\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u8ba8\u8bba\u200b\uff08\u200b\u4f8b\u5982\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u3001\u200b\u4e22\u5f03\u200bdropout\u3001\u200b\u8f93\u5165\u200b\u566a\u58f0\u200b\u7b49\u200b\uff09\u3002\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u59cb\u7ec8\u200b\u6700\u597d\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u6765\u200b\u63a7\u5236\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u8c03\u6574\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6570\u91cf\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u80cc\u540e\u200b\u7684\u200b\u5fae\u5999\u200b\u539f\u56e0\u200b\u662f\u200b\uff0c\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u7f51\u7edc\u200b\u66f4\u96be\u200b\u4f7f\u7528\u200b\u8bf8\u5982\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u5c40\u90e8\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff1a\u200b\u5f88\u200b\u660e\u663e\u200b\uff0c\u200b\u5b83\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5177\u6709\u200b\u76f8\u5bf9\u200b\u8f83\u5c11\u200b\u7684\u200b\u5c40\u90e8\u200b\u6700\u5c0f\u503c\u200b\uff0c\u200b\u4f46\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bb8\u591a\u200b\u6700\u5c0f\u503c\u200b\u66f4\u200b\u5bb9\u6613\u200b\u6536\u655b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5b83\u4eec\u200b\u662f\u200b\u7cdf\u7cd5\u200b\u7684\u200b\uff08\u200b\u5373\u200b\u5177\u6709\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u635f\u5931\u200b\uff09\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u66f4\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5305\u542b\u200b\u660e\u663e\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5c40\u90e8\u200b\u6700\u5c0f\u503c\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u6700\u5c0f\u503c\u200b\u5728\u200b\u5b9e\u9645\u200b\u635f\u5931\u200b\u65b9\u9762\u200b\u8981\u200b\u597d\u5f97\u591a\u200b\u3002\u200b\u7531\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u975e\u200b\u51f8\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u6570\u5b66\u200b\u4e0a\u200b\u7814\u7a76\u200b\u8fd9\u4e9b\u200b\u5c5e\u6027\u200b\u5f88\u200b\u56f0\u96be\u200b\uff0c\u200b\u4f46\u200b\u4e00\u4e9b\u200b\u5c1d\u8bd5\u200b\u7406\u89e3\u200b\u8fd9\u4e9b\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u7684\u200b\u5de5\u4f5c\u200b\u5df2\u7ecf\u200b\u8fdb\u884c\u200b\uff0c\u200b\u4f8b\u5982\u200b\u6700\u8fd1\u200b\u7684\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\u591a\u5c42\u200b\u7f51\u7edc\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u9762\u200bThe Loss Surfaces of Multilayer Networks\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u60a8\u200b\u4f1a\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5982\u679c\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5c0f\u578b\u200b\u7f51\u7edc\u200b\uff0c\u200b\u6700\u7ec8\u200b\u7684\u200b\u635f\u5931\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u663e\u793a\u200b\u51fa\u200b\u76f8\u5f53\u200b\u5927\u200b\u7684\u200b\u65b9\u5dee\u200b\u2014\u2014\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u60a8\u200b\u4f1a\u200b\u5e78\u8fd0\u5730\u200b\u6536\u655b\u200b\u5230\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u67d0\u4e9b\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u60a8\u200b\u4f1a\u200b\u9677\u5165\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u4e0d\u597d\u200b\u7684\u200b\u5c40\u90e8\u200b\u6700\u5c0f\u503c\u200b\u3002\u200b\u53e6\u4e00\u65b9\u9762\u200b\uff0c\u200b\u5982\u679c\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5927\u578b\u200b\u7f51\u7edc\u200b\uff0c\u200b\u60a8\u200b\u5c06\u200b\u5f00\u59cb\u200b\u627e\u5230\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff0c\u200b\u4f46\u200b\u6700\u7ec8\u200b\u5b9e\u73b0\u200b\u7684\u200b\u635f\u5931\u200b\u65b9\u5dee\u200b\u4f1a\u200b\u5c0f\u5f97\u591a\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6240\u6709\u200b\u89e3\u51b3\u65b9\u6848\u200b\u90fd\u200b\u5dee\u4e0d\u591a\u200b\u4e00\u6837\u200b\u597d\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u592a\u200b\u4f9d\u8d56\u4e8e\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u8fd0\u6c14\u200b\u3002</p> <p>\u200b\u518d\u6b21\u200b\u5f3a\u8c03\u200b\uff0c\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200bregularization strength\u200b\u662f\u200b\u63a7\u5236\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u9996\u9009\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u4e09\u79cd\u200b\u4e0d\u540c\u200b\u8bbe\u7f6e\u200b\u6240\u200b\u8fbe\u5230\u200b\u7684\u200b\u7ed3\u679c\u200b\uff1a</p> <p> </p> <p>\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u7684\u200b\u5f71\u54cd\u200b\uff1a\u200b\u4e0a\u9762\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u90fd\u200b\u6709\u200b20\u200b\u4e2a\u200b\u9690\u85cf\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u4f46\u200b\u6539\u53d8\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u4f1a\u200b\u4f7f\u200b\u5176\u200b\u6700\u7ec8\u200b\u7684\u200b\u51b3\u7b56\u200b\u533a\u57df\u200b\u66f4\u52a0\u200b\u5e73\u6ed1\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u4e2a\u200bConvNetsJS\u200b\u6f14\u793a\u200b\u94fe\u63a5\u200b\u4e2d\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u8fdb\u884c\u200b\u4e92\u52a8\u200b\u3002</p> <p>\u200b\u7ed3\u8bba\u200b\u662f\u200b\uff0c\u200b\u60a8\u200b\u4e0d\u200b\u5e94\u8be5\u200b\u56e0\u4e3a\u200b\u62c5\u5fc3\u200b\u8fc7\u200b\u62df\u5408\u200b\u800c\u200b\u4f7f\u7528\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u5c3d\u53ef\u80fd\u200b\u4f7f\u7528\u200b\u8f83\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u53ea\u8981\u200b\u60a8\u200b\u7684\u200b\u8ba1\u7b97\u200b\u9884\u7b97\u200b\u5141\u8bb8\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200b\u6b63\u5219\u200b\u5316\u200b\u6280\u672f\u200b\u6765\u200b\u63a7\u5236\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-1/#_11","title":"\u603b\u7ed3","text":"<p>\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\uff0c</p> <ul> <li>\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u4e86\u200b\u751f\u7269\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u7c97\u7565\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u5b9e\u8df5\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u51e0\u79cd\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u5176\u4e2d\u200bReLU\u200b\u662f\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u9009\u62e9\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u4e86\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5176\u4e2d\u200b\u795e\u7ecf\u5143\u200b\u901a\u8fc7\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u76f8\u8fde\u200b\uff0c\u200b\u76f8\u90bb\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u5177\u6709\u200b\u5168\u5bf9\u200b\u5168\u200b\u7684\u200b\u8fde\u63a5\u200b\uff0c\u200b\u4f46\u540c\u200b\u4e00\u5c42\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u4e4b\u95f4\u200b\u6ca1\u6709\u200b\u8fde\u63a5\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u8fd9\u79cd\u200b\u5206\u5c42\u200b\u4f53\u7cfb\u7ed3\u6784\u200b\u4f7f\u5f97\u200b\u57fa\u4e8e\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4e0e\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u5e94\u7528\u200b\u4e4b\u95f4\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u975e\u5e38\u200b\u9ad8\u6548\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u901a\u7528\u200b\u51fd\u6570\u200b\u903c\u8fd1\u200b\u5668\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u4e5f\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u8fd9\u200b\u4e00\u200b\u5c5e\u6027\u200b\u4e0e\u200b\u5b83\u4eec\u200b\u7684\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u8f83\u200b\u5c0f\u200b\u3002\u200b\u5b83\u4eec\u200b\u4e4b\u6240\u4ee5\u200b\u4f7f\u7528\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u5bf9\u200b\u5b9e\u8df5\u200b\u4e2d\u200b\u51fa\u73b0\u200b\u7684\u200b\u51fd\u6570\u200b\u7684\u200b\u529f\u80fd\u200b\u5f62\u5f0f\u200b\u505a\u51fa\u200b\u4e86\u200b\u67d0\u4e9b\u200b\u201c\u200b\u6b63\u786e\u200b\u201d\u200b\u7684\u200b\u5047\u8bbe\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u8f83\u5927\u200b\u7684\u200b\u7f51\u7edc\u200b\u5c06\u200b\u59cb\u7ec8\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\u7684\u200b\u7f51\u7edc\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\uff0c\u200b\u4f46\u200b\u5fc5\u987b\u200b\u9002\u5f53\u200b\u5730\u200b\u91c7\u7528\u200b\u66f4\u5f3a\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\uff08\u200b\u4f8b\u5982\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u6743\u91cd\u200b\u8870\u51cf\u200b\uff09\uff0c\u200b\u5426\u5219\u200b\u53ef\u80fd\u200b\u4f1a\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u770b\u5230\u200b\u66f4\u200b\u591a\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\uff08\u200b\u7279\u522b\u200b\u662f\u200bdropout\uff09\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-1/#_12","title":"\u5176\u4ed6\u200b\u53c2\u8003\u8d44\u6599","text":"<ul> <li>deeplearning.net\u200b\u6559\u7a0b\u200b\uff08\u200b\u4f7f\u7528\u200bTheano\uff09</li> <li>\u200b\u7528\u4e8e\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\u7684\u200bConvNetJS\u200b\u6f14\u793a\u200b</li> <li>Michael Nielsen\u200b\u7684\u200b\u6559\u7a0b\u200b</li> </ul>"},{"location":"courses/neural_network/neural-networks-2/","title":"\u521b\u5efa\u200b\u6570\u636e\u200b\u548c\u200b\u635f\u5931","text":""},{"location":"courses/neural_network/neural-networks-2/#_1","title":"\u8bbe\u7f6e\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b","text":"<p>\u200b\u5728\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u5728\u200b\u8ba1\u7b97\u200b\u5185\u79ef\u200b\u540e\u200b\u8fdb\u884c\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u795e\u7ecf\u5143\u200b\u7ec4\u7ec7\u200b\u6210\u200b\u5404\u4e2a\u200b\u5c42\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u505a\u6cd5\u200b\u5171\u540c\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u8bc4\u5206\u200b\u51fd\u6570\u200b\uff08score function\uff09\u200b\u7684\u200b\u65b0\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u8be5\u200b\u5f62\u5f0f\u200b\u662f\u4ece\u200b\u524d\u9762\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7684\u200b\u7b80\u5355\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u53d1\u5c55\u200b\u800c\u6765\u200b\u7684\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c31\u662f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u4e0e\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u4ea4\u7ec7\u200b\u7684\u200b\u8fd0\u7b97\u200b\u3002\u200b\u672c\u8282\u200b\u5c06\u200b\u8ba8\u8bba\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7b97\u6cd5\u200b\u8bbe\u8ba1\u200b\u9009\u9879\u200b\uff0c\u200b\u6bd4\u5982\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-2/#_2","title":"\u6570\u636e\u200b\u9884\u5904\u7406","text":"<p>\u200b\u5173\u4e8e\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u6211\u4eec\u200b\u6709\u200b3\u200b\u4e2a\u200b\u5e38\u7528\u200b\u7684\u200b\u7b26\u53f7\u200b\uff0c\u200b\u6570\u636e\u200b\u77e9\u9635\u200b<code>X</code>\uff0c\u200b\u5047\u8bbe\u200b\u5176\u200b\u5c3a\u5bf8\u200b\u662f\u200b<code>[N x D]</code>\uff08<code>N</code>\u200b\u662f\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c<code>D</code>\u200b\u662f\u200b\u6570\u636e\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff09\u3002</p> <p>\u200b\u5747\u503c\u200b\u51cf\u6cd5\u200b\uff08Mean subtraction\uff09\u200b\u662f\u200b\u9884\u5904\u7406\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u5f62\u5f0f\u200b\u3002\u200b\u5b83\u200b\u5bf9\u200b\u6570\u636e\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u72ec\u7acb\u200b\u7279\u5f81\u200b\u51cf\u53bb\u200b\u5e73\u5747\u503c\u200b\uff0c\u200b\u4ece\u200b\u51e0\u4f55\u200b\u4e0a\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u90fd\u200b\u5c06\u200b\u6570\u636e\u200b\u4e91\u200b\u7684\u200b\u4e2d\u5fc3\u200b\u90fd\u200b\u8fc1\u79fb\u200b\u5230\u200b\u539f\u70b9\u200b\u3002\u200b\u5728\u200bnumpy\u200b\u4e2d\u200b\uff0c\u200b\u8be5\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee3\u7801\u200b<code>X -= np.mean(X, axis = 0)</code>\u200b\u5b9e\u73b0\u200b\u3002\u200b\u800c\u200b\u5bf9\u4e8e\u200b\u56fe\u50cf\u200b\uff0c\u200b\u66f4\u200b\u5e38\u7528\u200b\u7684\u200b\u662f\u200b\u5bf9\u200b\u6240\u6709\u200b\u50cf\u7d20\u200b\u90fd\u200b\u51cf\u53bb\u200b\u4e00\u4e2a\u200b\u503c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u200b<code>X -= np.mean(X)</code>\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b3\u200b\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u4e0a\u200b\u5206\u522b\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u5f52\u4e00\u5316\u200b\uff08Normalization\uff09\u200b\u662f\u200b\u6307\u200b\u5c06\u200b\u6570\u636e\u200b\u7684\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u90fd\u200b\u8fd1\u4f3c\u200b\u76f8\u7b49\u200b\u3002\u200b\u6709\u200b\u4e24\u79cd\u200b\u5e38\u7528\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u5f52\u4e00\u5316\u200b\u3002\u200b\u7b2c\u4e00\u79cd\u200b\u662f\u200b\u5148\u200b\u5bf9\u200b\u6570\u636e\u200b\u505a\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\uff08zero-centered\uff09\u200b\u5904\u7406\u200b\uff0c\u200b\u7136\u540e\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u9664\u4ee5\u200b\u5176\u200b\u6807\u51c6\u5dee\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u4e3a\u200b<code>X /= np.std(X, axis = 0)</code>\u3002\u200b\u7b2c\u4e8c\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u505a\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6700\u5927\u200b\u548c\u200b\u6700\u5c0f\u503c\u200b\u662f\u200b1\u200b\u548c\u200b-1\u3002\u200b\u8fd9\u4e2a\u200b\u9884\u5904\u7406\u200b\u64cd\u4f5c\u200b\u53ea\u6709\u200b\u5728\u200b\u786e\u4fe1\u200b\u4e0d\u540c\u200b\u7684\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff08\u200b\u6216\u200b\u8ba1\u91cf\u5355\u4f4d\u200b\uff09\u200b\u65f6\u624d\u200b\u6709\u200b\u610f\u4e49\u200b\uff0c\u200b\u4f46\u200b\u8981\u200b\u6ce8\u610f\u200b\u9884\u5904\u7406\u200b\u64cd\u4f5c\u200b\u7684\u200b\u91cd\u8981\u6027\u200b\u51e0\u4e4e\u200b\u7b49\u540c\u4e8e\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u672c\u8eab\u200b\u3002\u200b\u5728\u200b\u56fe\u50cf\u5904\u7406\u200b\u4e2d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u50cf\u7d20\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u51e0\u4e4e\u200b\u662f\u200b\u4e00\u81f4\u200b\u7684\u200b\uff08\u200b\u90fd\u200b\u5728\u200b0-255\u200b\u4e4b\u95f4\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u8fdb\u884c\u200b\u8fd9\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u6b65\u9aa4\u200b\u5e76\u200b\u4e0d\u662f\u200b\u5f88\u200b\u5fc5\u8981\u200b\u3002</p> <p> </p> <p>\u200b\u4e00\u822c\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u6d41\u7a0b\u200b\uff1a\u200b\u5de6\u8fb9\u200b\uff1a\u200b\u539f\u59cb\u200b\u7684\u200b2\u200b\u7ef4\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u3002\u200b\u4e2d\u95f4\u200b\uff1a\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u90fd\u200b\u51cf\u53bb\u200b\u5e73\u5747\u503c\u200b\u540e\u200b\u5f97\u5230\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\u6570\u636e\u200b\uff0c\u200b\u73b0\u5728\u200b\u6570\u636e\u200b\u4e91\u662f\u200b\u4ee5\u200b\u539f\u70b9\u200b\u4e3a\u200b\u4e2d\u5fc3\u200b\u7684\u200b\u3002\u200b\u53f3\u8fb9\u200b\uff1a\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u9664\u4ee5\u200b\u5176\u200b\u6807\u51c6\u5dee\u200b\u6765\u200b\u8c03\u6574\u200b\u5176\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u3002\u200b\u7ea2\u8272\u200b\u7684\u200b\u7ebf\u200b\u6307\u51fa\u200b\u4e86\u200b\u6570\u636e\u200b\u5404\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff0c\u200b\u5728\u200b\u4e2d\u95f4\u200b\u7684\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\u6570\u636e\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u4e0d\u540c\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u53f3\u8fb9\u200b\u5f52\u4e00\u5316\u200b\u6570\u636e\u200b\u4e2d\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u4e3b\u200b\u6210\u5206\u200b\u5206\u6790\u200bPCA\u200b\u548c\u200b\u767d\u5316\u200bWhitening \u200b\u662f\u200b\u53e6\u200b\u4e00\u79cd\u200b\u9884\u5904\u7406\u200b\u5f62\u5f0f\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u5904\u7406\u200b\u4e2d\u200b\uff0c\u200b\u5148\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\u5904\u7406\u200b\uff0c\u200b\u7136\u540e\u200b\u8ba1\u7b97\u200b\u534f\u65b9\u5dee\u200bcovariance\u200b\u77e9\u9635\u200b\uff0c\u200b\u5b83\u200b\u5c55\u793a\u200b\u4e86\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u76f8\u5173\u6027\u200bcorrelation\u200b\u7ed3\u6784\u200b\u3002</p> <pre><code># \u200b\u5047\u8bbe\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u77e9\u9635\u200bX\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u4e3a\u200b[N x D]\nX -= np.mean(X, axis = 0) # \u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b(\u200b\u91cd\u8981\u200b)\ncov = np.dot(X.T, X) / X.shape[0] # \u200b\u5f97\u5230\u200b\u6570\u636e\u200b\u7684\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\n</code></pre> <p>\u200b\u6570\u636e\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u7684\u200b\u7b2c\u200b(i, j)\u200b\u4e2a\u200b\u5143\u7d20\u200b\u662f\u200b\u6570\u636e\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u548c\u200b\u7b2c\u200bj\u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u534f\u65b9\u5dee\u200bcovariance\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u8be5\u200b\u77e9\u9635\u200b\u7684\u200b\u5bf9\u89d2\u7ebf\u200b\u4e0a\u200b\u7684\u200b\u5143\u7d20\u200b\u662f\u200b\u65b9\u5dee\u200bvariances\u3002\u200b\u8fd8\u6709\u200b\uff0c\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u662f\u200b\u5bf9\u79f0\u200b\u548c\u200b\u534a\u200b\u6b63\u5b9a\u200bpositive semi-definite\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6570\u636e\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u8fdb\u884c\u200bSVD\uff08\u200b\u5947\u5f02\u200b\u503c\u200b\u5206\u89e3\u200bfactorization\uff09\u200b\u8fd0\u7b97\u200b\u3002</p> <pre><code>U,S,V = np.linalg.svd(cov)\n</code></pre> <p>\u200b\u5176\u4e2d\u200b <code>U</code>\u200b\u7684\u200b\u5217\u200b\u662f\u200b\u7279\u5f81\u5411\u91cf\u200b\uff0c<code>S</code> \u200b\u662f\u200b\u88c5\u6709\u200b\u5947\u5f02\u200b\u503c\u200bsingular values\u200b\u7684\u200b1\u200b\u7ef4\u200b\u6570\u7ec4\u200b\uff08\u200b\u56e0\u4e3a\u200bcov\u200b\u662f\u200b\u5bf9\u79f0\u200b\u4e14\u200b\u534a\u200b\u6b63\u5b9a\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200bS\u200b\u4e2d\u200b\u5143\u7d20\u200b\u662f\u200b\u7279\u5f81\u503c\u200b\u7684\u200b\u5e73\u65b9\u200b\uff09\u3002\u200b\u4e3a\u4e86\u200b\u53bb\u9664\u200b\u6570\u636e\u200b\u76f8\u5173\u6027\u200bdecorrelate\uff0c\u200b\u5c06\u200b\u5df2\u7ecf\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\u5904\u7406\u200b\u8fc7\u200b\u7684\u200b\u539f\u59cb\u6570\u636e\u200b\u6295\u5f71\u200b\u5230\u200b\u7279\u5f81\u200b\u57fa\u51c6\u200beigenbasis\u200b\u4e0a\u200b\uff1a</p> <pre><code>Xrot = np.dot(X, U) # decorrelate the data\n</code></pre> <p>\u200b\u6ce8\u610f\u200b<code>U</code>\u200b\u7684\u200b\u5217\u200b\u662f\u200b\u6807\u51c6\u200b\u6b63\u4ea4\u200b\u5411\u91cf\u200b\u7684\u200b\u96c6\u5408\u200b\uff08\u200b\u8303\u5f0f\u200b\u4e3a\u200b1\uff0c\u200b\u5217\u200b\u4e4b\u95f4\u200b\u6807\u51c6\u200b\u6b63\u4ea4\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u53ef\u4ee5\u200b\u628a\u200b\u5b83\u4eec\u200b\u770b\u505a\u200b\u6807\u51c6\u200b\u6b63\u4ea4\u200b\u57fa\u200b\u5411\u91cf\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6295\u5f71\u200b\u5bf9\u5e94\u200b<code>X</code>\u200b\u4e2d\u200b\u7684\u200b\u6570\u636e\u200b\u7684\u200b\u4e00\u4e2a\u200b\u65cb\u8f6c\u200b\uff0c\u200b\u65cb\u8f6c\u200b\u4ea7\u751f\u200b\u7684\u200b\u7ed3\u679c\u200b\u5c31\u662f\u200b\u65b0\u200b\u7684\u200b\u7279\u5f81\u5411\u91cf\u200beigenvectors\u3002\u200b\u5982\u679c\u200b\u8ba1\u7b97\u200b<code>Xrot</code>\u200b\u7684\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\uff0c\u200b\u5c06\u4f1a\u200b\u770b\u5230\u200b\u5b83\u200b\u662f\u200b\u5bf9\u89d2\u200b\u5bf9\u79f0\u200b\u7684\u200b\u3002<code>np.linalg.svd</code>\u200b\u7684\u200b\u4e00\u4e2a\u200b\u826f\u597d\u200b\u6027\u8d28\u200b\u662f\u200b\u5728\u200b\u5b83\u200b\u7684\u200b\u8fd4\u56de\u503c\u200b<code>U</code>\u200b\u4e2d\u200b\uff0c\u200b\u7279\u5f81\u5411\u91cf\u200b\u662f\u200b\u6309\u7167\u200b\u7279\u5f81\u503c\u200b\u7684\u200b\u5927\u5c0f\u200b\u6392\u5217\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u8fd9\u4e2a\u200b\u6027\u8d28\u200b\u6765\u200b\u5bf9\u200b\u6570\u636e\u200b\u964d\u7ef4\u200b\uff0c\u200b\u53ea\u8981\u200b\u4f7f\u7528\u200b\u524d\u9762\u200b\u7684\u200b\u5c0f\u200b\u90e8\u5206\u200b\u7279\u5f81\u5411\u91cf\u200b\uff0c\u200b\u4e22\u5f03\u200b\u6389\u200b\u90a3\u4e9b\u200b\u5305\u542b\u200b\u7684\u200b\u6570\u636e\u200b\u6ca1\u6709\u200b\u65b9\u5dee\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002 \u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u200b\u4e3a\u4e3b\u200b\u6210\u5206\u200b\u5206\u6790\u200b\uff08 Principal Component Analysis \u200b\u7b80\u79f0\u200bPCA\uff09\u200b\u964d\u7ef4\u200b\uff1a</p> <pre><code>Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]\n</code></pre> <p>\u200b\u7ecf\u8fc7\u200b\u4e0a\u9762\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5c06\u200b\u539f\u59cb\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5927\u5c0f\u200b\u7531\u200b[N x D]\u200b\u964d\u5230\u200b\u4e86\u200b[N x 100]\uff0c\u200b\u7559\u4e0b\u200b\u4e86\u200b\u6570\u636e\u200b\u4e2d\u200b\u5305\u542b\u200b\u6700\u5927\u200b\u65b9\u5dee\u200b\u7684\u200b100\u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200bPCA\u200b\u964d\u7ef4\u8fc7\u200b\u7684\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u548c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4f1a\u200b\u8fbe\u5230\u200b\u975e\u5e38\u200b\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\u6548\u679c\u200b\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u200b\u80fd\u200b\u8282\u7701\u65f6\u95f4\u200b\u548c\u200b\u5b58\u50a8\u5668\u200b\u7a7a\u95f4\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u4f1a\u200b\u770b\u89c1\u200b\u7684\u200b\u53d8\u6362\u200b\u662f\u200b\u767d\u5316\u200b\uff08whitening\uff09\u3002\u200b\u767d\u5316\u200b\u64cd\u4f5c\u200b\u7684\u200b\u8f93\u5165\u200b\u662f\u200b\u7279\u5f81\u200b\u57fa\u51c6\u200b\u4e0a\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u9664\u4ee5\u200b\u5176\u200b\u7279\u5f81\u503c\u200b\u6765\u200b\u5bf9\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u3002\u200b\u8be5\u200b\u53d8\u6362\u200b\u7684\u200b\u51e0\u4f55\u200b\u89e3\u91ca\u200b\u662f\u200b\uff1a\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u670d\u4ece\u200b\u591a\u200b\u53d8\u91cf\u200b\u7684\u200b\u9ad8\u65af\u5206\u5e03\u200b\uff0c\u200b\u90a3\u4e48\u200b\u7ecf\u8fc7\u200b\u767d\u5316\u200b\u540e\u200b\uff0c\u200b\u6570\u636e\u200b\u7684\u200b\u5206\u5e03\u200b\u5c06\u4f1a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5747\u503c\u200b\u4e3a\u200b\u96f6\u200b\uff0c\u200b\u4e14\u200b\u534f\u65b9\u5dee\u200b\u76f8\u7b49\u200b\u7684\u200b\u77e9\u9635\u200b\u3002\u200b\u8be5\u200b\u64cd\u4f5c\u200b\u7684\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code># \u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u767d\u5316\u200b\u64cd\u4f5c\u200b:\n# \u200b\u9664\u4ee5\u200b\u7279\u5f81\u503c\u200b \uff08\u200b\u5176\u4e3a\u200b\u5947\u5f02\u200b\u503c\u200b\u7684\u200b\u5e73\u65b9\u6839\u200b\uff09\nXwhite = Xrot / np.sqrt(S + 1e-5)\n</code></pre> <p>\u200b\u8b66\u544a\u200b\uff1a\u200b\u5938\u5927\u200b\u7684\u200b\u566a\u58f0\u200bExaggerating noise\u3002\u200b\u6ce8\u610f\u200b\u5206\u6bcd\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u4e86\u200b1e-5\uff08\u200b\u6216\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u5e38\u91cf\u200b\uff09\u200b\u6765\u200b\u9632\u6b62\u200b\u5206\u6bcd\u200b\u4e3a\u200b0\u3002\u200b\u8be5\u200b\u53d8\u6362\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7f3a\u9677\u200b\u662f\u200b\u5728\u200b\u53d8\u6362\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5938\u5927\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u566a\u58f0\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u200b\u5c06\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u62c9\u4f38\u200b\u5230\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\u4e5f\u200b\u5305\u542b\u200b\u4e86\u200b\u90a3\u4e9b\u200b\u53ea\u6709\u200b\u6781\u5c11\u200b\u5dee\u5f02\u6027\u200b(\u200b\u65b9\u5dee\u200b\u5c0f\u200b)\u200b\u800c\u200b\u5927\u591a\u200b\u662f\u200b\u566a\u58f0\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u53ef\u4ee5\u200b\u7528\u200b\u66f4\u200b\u5f3a\u200b\u7684\u200b\u5e73\u6ed1\u200b\u6765\u200b\u89e3\u51b3\u200b\uff08\u200b\u4f8b\u5982\u200b\uff1a\u200b\u91c7\u7528\u200b\u6bd4\u200b1e-5\u200b\u66f4\u5927\u200b\u7684\u200b\u503c\u200b\uff09\u3002</p> <p> </p> <p>PCA/\u200b\u767d\u5316\u200b\u3002\u200b\u5de6\u8fb9\u200b\u662f\u200b\u4e8c\u7ef4\u200b\u7684\u200b\u539f\u59cb\u6570\u636e\u200b\u3002\u200b\u4e2d\u95f4\u200b\uff1a\u200b\u7ecf\u8fc7\u200bPCA\u200b\u64cd\u4f5c\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\u6570\u636e\u200b\u9996\u5148\u200b\u662f\u200b\u96f6\u200b\u4e2d\u5fc3\u200b\u7684\u200b\uff0c\u200b\u7136\u540e\u200b\u53d8\u6362\u200b\u5230\u200b\u4e86\u200b\u6570\u636e\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u7684\u200b\u57fa\u51c6\u200b\u8f74\u4e0a\u200b\u3002\u200b\u8fd9\u6837\u200b\u5c31\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u4e86\u89e3\u200b\u76f8\u5173\u200b\uff08\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u53d8\u6210\u200b\u5bf9\u89d2\u200b\u9635\u200b\uff09\u3002\u200b\u53f3\u8fb9\u200b\uff1a\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u90fd\u200b\u88ab\u200b\u7279\u5f81\u503c\u200b\u8c03\u6574\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff0c\u200b\u5c06\u200b\u6570\u636e\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u53d8\u4e3a\u200b\u5355\u4f4d\u77e9\u9635\u200b\u3002\u200b\u4ece\u200b\u51e0\u4f55\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u5c31\u662f\u200b\u5bf9\u200b\u6570\u636e\u200b\u5728\u200b\u5404\u4e2a\u200b\u65b9\u5411\u200b\u4e0a\u200b\u62c9\u4f38\u200b\u538b\u7f29\u200b\uff0c\u200b\u4f7f\u200b\u4e4b\u200b\u53d8\u6210\u200b\u670d\u4ece\u200b\u9ad8\u65af\u5206\u5e03\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u70b9\u200b\u5206\u5e03\u200b(isotropic gaussian blob)\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bCIFAR-10\u200b\u6570\u636e\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u53d8\u5316\u200b\u53ef\u89c6\u5316\u200b\u51fa\u6765\u200b\u3002CIFAR-10\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u5927\u5c0f\u200b\u662f\u200b50000x3072\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u62c9\u4f38\u200b\u4e3a\u200b3072\u200b\u7ef4\u200b\u7684\u200b\u884c\u5411\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b[3072 x 3072]\u200b\u7684\u200b\u534f\u65b9\u5dee\u200b\u77e9\u9635\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b\u5947\u5f02\u200b\u503c\u200b\u5206\u89e3\u200b\uff08\u200b\u6bd4\u8f83\u200b\u8017\u8d39\u200b\u8ba1\u7b97\u200b\u6027\u80fd\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u7ecf\u8fc7\u200b\u8ba1\u7b97\u200b\u7684\u200b\u7279\u5f81\u5411\u91cf\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u5462\u200b\uff1f\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5e2e\u52a9\u200b\u7406\u89e3\u200b\uff1a </p> <p> </p> <p>\u200b\u6700\u200b\u5de6\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u6f14\u793a\u200b\u7684\u200b\u96c6\u5408\u200b\uff0c\u200b\u542b\u200b49\u200b\u5f20\u200b\u56fe\u7247\u200b\u3002\u200b\u5de6\u4e8c\u200b\uff1a3072\u200b\u4e2a\u200b\u7279\u5f81\u503c\u200b\u5411\u91cf\u200b\u4e2d\u200b\u7684\u200b\u524d\u200b144\u200b\u4e2a\u200b\u3002\u200b\u9760\u524d\u9762\u200b\u7684\u200b\u7279\u5f81\u5411\u91cf\u200b\u89e3\u91ca\u200b\u4e86\u200b\u6570\u636e\u200b\u4e2d\u200b\u5927\u90e8\u5206\u200b\u7684\u200b\u65b9\u5dee\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u89c1\u200b\u5b83\u4eec\u200b\u4e0e\u200b\u56fe\u50cf\u200b\u4e2d\u8f83\u200b\u4f4e\u200b\u7684\u200b\u9891\u7387\u200b\u76f8\u5173\u200b\u3002\u200b\u7b2c\u4e09\u5f20\u200b\u662f\u200b49\u200b\u5f20\u200b\u7ecf\u8fc7\u200b\u4e86\u200bPCA\u200b\u964d\u7ef4\u200b\u5904\u7406\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u5c55\u793a\u200b\u4e86\u200b144\u200b\u4e2a\u200b\u7279\u5f81\u5411\u91cf\u200b\u3002\u200b\u8fd9\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5c55\u793a\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u662f\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u7528\u200b3072\u200b\u7ef4\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u5411\u91cf\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\u662f\u200b\u56fe\u7247\u200b\u4e0a\u200b\u67d0\u4e2a\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u50cf\u7d20\u200b\u5728\u200b\u67d0\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u4e2d\u200b\u7684\u200b\u4eae\u5ea6\u200b\u503c\u200b\u3002\u200b\u800c\u200b\u73b0\u5728\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4e00\u4e2a\u200b144\u200b\u7ef4\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u8868\u793a\u200b\u4e86\u200b\u7279\u5f81\u5411\u91cf\u200b\u5bf9\u4e8e\u200b\u7ec4\u6210\u200b\u8fd9\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u8d21\u732e\u5ea6\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u56fe\u7247\u200b\u80fd\u591f\u200b\u6b63\u5e38\u200b\u663e\u793a\u200b\uff0c\u200b\u9700\u8981\u200b\u5c06\u200b144\u200b\u7ef4\u5ea6\u200b\u91cd\u65b0\u200b\u53d8\u6210\u200b\u57fa\u4e8e\u200b\u50cf\u7d20\u200b\u57fa\u51c6\u200b\u7684\u200b3072\u200b\u4e2a\u200b\u6570\u503c\u200b\u3002\u200b\u56e0\u4e3a\u200bU\u200b\u662f\u200b\u4e00\u4e2a\u200b\u65cb\u8f6c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e58\u4ee5\u200bU.transpose()[:144,:]\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5f97\u5230\u200b\u7684\u200b3072\u200b\u4e2a\u200b\u6570\u503c\u200b\u53ef\u89c6\u5316\u200b\u3002\u200b\u53ef\u4ee5\u200b\u770b\u89c1\u200b\u56fe\u50cf\u200b\u53d8\u5f97\u200b\u6709\u70b9\u200b\u6a21\u7cca\u200b\u4e86\u200b\uff0c\u200b\u8fd9\u200b\u6b63\u597d\u200b\u8bf4\u660e\u200b\u524d\u9762\u200b\u7684\u200b\u7279\u5f81\u5411\u91cf\u200b\u83b7\u53d6\u200b\u4e86\u200b\u8f83\u200b\u4f4e\u200b\u7684\u200b\u9891\u7387\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5927\u591a\u6570\u200b\u4fe1\u606f\u200b\u8fd8\u662f\u200b\u4fdd\u7559\u200b\u4e86\u200b\u4e0b\u6765\u200b\u3002\u200b\u6700\u53f3\u200b\uff1a\u200b\u5c06\u200b\u201c\u200b\u767d\u5316\u200b\u201d\u200b\u540e\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u663e\u793a\u200b\u3002\u200b\u5176\u4e2d\u200b144\u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\u7684\u200b\u65b9\u5dee\u200b\u90fd\u200b\u88ab\u200b\u538b\u7f29\u200b\u5230\u200b\u4e86\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u3002\u200b\u7136\u540e\u200b144\u200b\u4e2a\u200b\u767d\u5316\u200b\u540e\u200b\u7684\u200b\u6570\u503c\u200b\u901a\u8fc7\u200b\u4e58\u4ee5\u200bU.transpose()[:144,:]\u200b\u8f6c\u6362\u200b\u5230\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u57fa\u51c6\u200b\u4e0a\u200b\u3002\u200b\u73b0\u5728\u200b\u8f83\u200b\u4f4e\u200b\u7684\u200b\u9891\u7387\u200b\uff08\u200b\u4ee3\u8868\u200b\u4e86\u200b\u5927\u591a\u6570\u200b\u65b9\u5dee\u200b\uff09\u200b\u53ef\u4ee5\u200b\u5ffd\u7565\u4e0d\u8ba1\u200b\u4e86\u200b\uff0c\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u9891\u7387\u200b\uff08\u200b\u4ee3\u8868\u200b\u76f8\u5bf9\u200b\u5c11\u200b\u7684\u200b\u65b9\u5dee\u200b\uff09\u200b\u5c31\u200b\u88ab\u200b\u5938\u5927\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5b9e\u8df5\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u200b\u4e2d\u200b\u63d0\u5230\u200bPCA\u200b\u548c\u200b\u767d\u5316\u200b\u4e3b\u8981\u200b\u662f\u200b\u4e3a\u4e86\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u5b8c\u6574\u6027\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\u5728\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5e76\u200b\u4e0d\u4f1a\u200b\u91c7\u7528\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\u3002\u200b\u7136\u800c\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\u64cd\u4f5c\u200b\u8fd8\u662f\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\uff0c\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u50cf\u7d20\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u4e5f\u200b\u5f88\u200b\u5e38\u89c1\u200b\u3002</p> <p>\u200b\u5e38\u89c1\u200b\u9519\u8bef\u200b\u3002\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\u5f88\u200b\u91cd\u8981\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\uff1a\u200b\u4efb\u4f55\u200b\u9884\u5904\u7406\u200b\u7b56\u7565\u200b\uff08\u200b\u6bd4\u5982\u200b\u6570\u636e\u200b\u5747\u503c\u200b\uff09\u200b\u90fd\u200b\u53ea\u80fd\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u8bad\u7ec3\u200b\u5b8c\u6bd5\u200b\u540e\u200b\u518d\u200b\u5e94\u7528\u200b\u5230\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u6216\u8005\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u5148\u200b\u8ba1\u7b97\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u56fe\u50cf\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u7136\u540e\u200b\u6bcf\u5f20\u200b\u56fe\u7247\u200b\u90fd\u200b\u51cf\u53bb\u200b\u5e73\u5747\u503c\u200b\uff0c\u200b\u6700\u540e\u200b\u5c06\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u5206\u6210\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8fd9\u4e2a\u200b\u505a\u6cd5\u200b\u662f\u200b\u9519\u8bef\u200b\u7684\u200b\u3002\u200b\u5e94\u8be5\u200b\u600e\u4e48\u200b\u505a\u200b\u5462\u200b\uff1f\u200b\u5e94\u8be5\u200b\u5148\u200b\u5206\u6210\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u53ea\u662f\u200b\u4ece\u200b\u8bad\u7ec3\u200b\u96c6\u4e2d\u200b\u6c42\u200b\u56fe\u7247\u200b\u5e73\u5747\u503c\u200b\uff0c\u200b\u7136\u540e\u200b\u5404\u4e2a\u200b\u96c6\u200b\uff08\u200b\u8bad\u7ec3\u200b/\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff09\u200b\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\u518d\u200b\u51cf\u53bb\u200b\u8fd9\u4e2a\u200b\u5e73\u5747\u503c\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-2/#_3","title":"\u6743\u91cd\u200b\u521d\u59cb\u5316","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7ed3\u6784\u200b\u5e76\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u7f51\u7edc\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8fd8\u200b\u9700\u8981\u200b\u521d\u59cb\u5316\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u9519\u8bef\u200b\uff1a\u200b\u5168\u96f6\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u5e94\u8be5\u200b\u907f\u514d\u200b\u7684\u200b\u9519\u8bef\u200b\u5f00\u59cb\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5b8c\u6bd5\u200b\u540e\u200b\uff0c\u200b\u867d\u7136\u200b\u4e0d\u200b\u77e5\u9053\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u6743\u91cd\u200b\u7684\u200b\u6700\u7ec8\u200b\u503c\u200b\u5e94\u8be5\u200b\u662f\u200b\u591a\u5c11\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u7ecf\u8fc7\u200b\u4e86\u200b\u6070\u5f53\u200b\u7684\u200b\u5f52\u4e00\u5316\u200b\u7684\u8bdd\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5047\u8bbe\u200b\u6240\u6709\u6743\u200b\u91cd\u200b\u6570\u503c\u200b\u4e2d\u200b\u5927\u7ea6\u200b\u4e00\u534a\u200b\u4e3a\u200b\u6b63\u6570\u200b\uff0c\u200b\u4e00\u534a\u200b\u4e3a\u200b\u8d1f\u6570\u200b\u3002\u200b\u8fd9\u6837\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u542c\u200b\u8d77\u6765\u200b\u86ee\u200b\u5408\u7406\u200b\u7684\u200b\u60f3\u6cd5\u200b\u5c31\u662f\u200b\u628a\u200b\u8fd9\u4e9b\u200b\u6743\u91cd\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\u90fd\u200b\u8bbe\u4e3a\u200b0\u200b\u5427\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u671f\u671b\u200b\u4e0a\u200b\u6765\u8bf4\u200b0\u200b\u662f\u200b\u6700\u200b\u5408\u7406\u200b\u7684\u200b\u731c\u6d4b\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u505a\u6cd5\u200b\u9519\u8bef\u200b\u7684\u200b\uff01\u200b\u56e0\u4e3a\u200b\u5982\u679c\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u540c\u6837\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u7136\u540e\u200b\u5b83\u4eec\u200b\u5c31\u200b\u4f1a\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u540c\u6837\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u4ece\u800c\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u6743\u91cd\u200b\u88ab\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b\u540c\u6837\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u795e\u7ecf\u5143\u200b\u4e4b\u95f4\u200b\u5c31\u200b\u5931\u53bb\u200b\u4e86\u200b\u4e0d\u5bf9\u79f0\u6027\u200basymmetry\u200b\u7684\u200b\u6e90\u5934\u200b\u3002</p> <p>\u200b\u5c0f\u200b\u968f\u673a\u6570\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6743\u91cd\u200b\u521d\u59cb\u503c\u200b\u8981\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b0\u200b\u53c8\u200b\u4e0d\u80fd\u200b\u7b49\u4e8e\u200b0\u3002\u200b\u89e3\u51b3\u200b\u65b9\u6cd5\u200b\u5c31\u662f\u200b\u5c06\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b\u5f88\u5c0f\u200b\u7684\u200b\u6570\u503c\u200b\uff0c\u200b\u4ee5\u6b64\u200b\u6765\u200b\u6253\u7834\u200b\u5bf9\u79f0\u6027\u200bsymmetry breaking\u3002\u200b\u5176\u200b\u601d\u8def\u200b\u662f\u200b\uff1a\u200b\u5982\u679c\u200b\u795e\u7ecf\u5143\u200b\u521a\u200b\u5f00\u59cb\u200b\u7684\u200b\u65f6\u5019\u200b\u662f\u200b\u968f\u673a\u200b\u4e14\u200b\u4e0d\u200b\u76f8\u7b49\u200b\u7684\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u4eec\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u4e0d\u540c\u200b\u7684\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u81ea\u8eab\u200b\u53d8\u6210\u200b\u6574\u4e2a\u200b\u7f51\u7edc\u200b\u7684\u200b\u4e0d\u540c\u200b\u90e8\u5206\u200b\u3002\u200b\u5c0f\u200b\u968f\u673a\u6570\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u6cd5\u200b\u662f\u200b\uff1a<code>W = 0.01* np.random.randn(D,H)</code>\uff0c\u200b\u5176\u4e2d\u200b<code>randn</code>\u200b\u51fd\u6570\u200b\u662f\u200b\u57fa\u4e8e\u200b\u96f6\u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u5dee\u200b\u7684\u200b\u4e00\u4e2a\u200b\u9ad8\u65af\u5206\u5e03\u200b\u6765\u200b\u751f\u6210\u200b\u968f\u673a\u6570\u200b\u7684\u200b\u3002\u200b\u6839\u636e\u200b\u8fd9\u4e2a\u200b\u5f0f\u5b50\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u90fd\u200b\u88ab\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u5411\u91cf\u200b\uff0c\u200b\u800c\u200b\u8fd9\u4e9b\u200b\u968f\u673a\u200b\u5411\u91cf\u200b\u53c8\u200b\u670d\u4ece\u200b\u4e00\u4e2a\u591a\u200b\u53d8\u91cf\u200b\u9ad8\u65af\u5206\u5e03\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5728\u200b\u8f93\u5165\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\uff0c\u200b\u6240\u6709\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6307\u5411\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\u3002\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5747\u5300\u5206\u5e03\u200b\u751f\u6210\u200b\u7684\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u4f46\u662f\u200b\u4ece\u200b\u5b9e\u8df5\u200b\u7ed3\u679c\u200b\u6765\u770b\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u7b97\u6cd5\u200b\u7684\u200b\u7ed3\u679c\u200b\u5f71\u54cd\u200b\u6781\u5c0f\u200b\u3002</p> <p>\u200b\u8b66\u544a\u200b\u3002\u200b\u5e76\u200b\u4e0d\u662f\u200b\u5c0f\u200b\u6570\u503c\u200b\u4e00\u5b9a\u200b\u4f1a\u200b\u5f97\u5230\u200b\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5c42\u200b\u4e2d\u200b\u7684\u200b\u6743\u91cd\u200b\u503c\u200b\u5f88\u5c0f\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u65f6\u5019\u200b\u5c31\u200b\u4f1a\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u56e0\u4e3a\u200b\u68af\u5ea6\u200b\u4e0e\u200b\u6743\u91cd\u200b\u503c\u200b\u662f\u200b\u6210\u200b\u6bd4\u4f8b\u200b\u7684\u200b\uff09\u3002\u200b\u8fd9\u200b\u5c31\u200b\u4f1a\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u51cf\u5c0f\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u7684\u200b\u201c\u200b\u68af\u5ea6\u200b\u4fe1\u53f7\u200b\u201d\uff0c\u200b\u5728\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u5c31\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b1/sqrt(n)\u200b\u6821\u51c6\u200b\u65b9\u5dee\u200b\u3002\u200b\u4e0a\u9762\u200b\u505a\u6cd5\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u968f\u7740\u200b\u8f93\u5165\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u589e\u957f\u200b\uff0c\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u6570\u636e\u200b\u7684\u200b\u5206\u5e03\u200b\u4e2d\u200b\u7684\u200b\u65b9\u5dee\u200b\u4e5f\u200b\u5728\u200b\u589e\u5927\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9664\u4ee5\u200b\u8f93\u5165\u200b\u6570\u636e\u91cf\u200b\u7684\u200b\u5e73\u65b9\u6839\u200b\u6765\u200b\u8c03\u6574\u200b\u5176\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff0c\u200b\u8fd9\u6837\u200b\u795e\u7ecf\u5143\u200b\u8f93\u51fa\u200b\u7684\u200b\u65b9\u5dee\u200b\u5c31\u200b\u5f52\u4e00\u5316\u200b\u5230\u200b1\u200b\u4e86\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5efa\u8bae\u200b\u5c06\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b\uff1a<code>w = np.random.randn(n) / sqrt(n)</code>\uff0c\u200b\u5176\u4e2d\u200b<code>n</code>\u200b\u662f\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u6570\u91cf\u200b\u3002\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4fdd\u8bc1\u200b\u4e86\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6240\u6709\u200b\u795e\u7ecf\u5143\u200b\u8d77\u59cb\u200b\u65f6\u6709\u200b\u8fd1\u4f3c\u200b\u540c\u6837\u200b\u7684\u200b\u8f93\u51fa\u200b\u5206\u5e03\u200b\u3002\u200b\u5b9e\u8df5\u7ecf\u9a8c\u200b\u8bc1\u660e\u200b\uff0c\u200b\u8fd9\u6837\u200b\u505a\u200b\u53ef\u4ee5\u200b\u63d0\u9ad8\u200b\u6536\u655b\u200b\u7684\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u4e0a\u8ff0\u200b\u7ed3\u8bba\u200b\u7684\u200b\u63a8\u5bfc\u200b\u8fc7\u7a0b\u200b\u5982\u4e0b\u200b\uff1a\u200b\u5047\u8bbe\u200b\u6743\u91cd\u200b\\(w\\)\u200b\u548c\u200b\u8f93\u5165\u200b\\(x\\)\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5185\u79ef\u200b\u4e3a\u200b\\(s = \\sum_i^n w_i x_i\\)\uff0c\u200b\u8fd9\u200b\u662f\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u8fdb\u884c\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8fd0\u7b97\u200b\u4e4b\u524d\u200b\u7684\u200b\u539f\u59cb\u200b\u6570\u503c\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u68c0\u67e5\u200b\\(s\\)\u200b\u7684\u200b\u65b9\u5dee\u200b\uff1a</p> \\[ \\begin{align} \\text{Var}(s) &amp;= \\text{Var}(\\sum_i^n w_ix_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(w_ix_i) \\\\\\\\ &amp;= \\sum_i^n [E(w_i)]^2\\text{Var}(x_i) + [E(x_i)]^2\\text{Var}(w_i) + \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\sum_i^n \\text{Var}(x_i)\\text{Var}(w_i) \\\\\\\\ &amp;= \\left( n \\text{Var}(w) \\right) \\text{Var}(x) \\end{align} \\] <p>\u200b\u5728\u200b\u524d\u200b\u4e24\u6b65\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b\u65b9\u5dee\u200b\u7684\u200b\u6027\u8d28\u200b\u3002\u200b\u5728\u200b\u7b2c\u4e09\u6b65\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5047\u8bbe\u200b\u8f93\u5165\u200b\u548c\u200b\u6743\u91cd\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u90fd\u200b\u662f\u200b0\uff0c\u200b\u6240\u4ee5\u200b\\(E[x_i] = E[w_i] = 0\\)\u3002\u200b\u6ce8\u610f\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u662f\u200b\u4e00\u822c\u5316\u200b\u60c5\u51b5\u200b\uff0c\u200b\u6bd4\u5982\u200b\u5728\u200bReLU\u200b\u5355\u5143\u200b\u4e2d\u200b\u5747\u503c\u200b\u5c31\u200b\u4e3a\u200b\u6b63\u200b\u3002\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u5047\u8bbe\u200b\u6240\u6709\u200b\u7684\u200b\\(w_i, x_i\\)\u200b\u90fd\u200b\u670d\u4ece\u200b\u540c\u6837\u200b\u7684\u200b\u5206\u5e03\u200b\u3002\u200b\u4ece\u200b\u8fd9\u4e2a\u200b\u63a8\u5bfc\u200b\u8fc7\u7a0b\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u89c1\u200b\uff0c\u200b\u5982\u679c\u200b\u60f3\u8981\u200b\\(s\\)\u200b\u6709\u200b\u548c\u200b\u8f93\u5165\u200b\\(x\\)\u200b\u4e00\u6837\u200b\u7684\u200b\u65b9\u5dee\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5728\u200b\u521d\u59cb\u5316\u200b\u7684\u200b\u65f6\u5019\u200b\u5fc5\u987b\u200b\u4fdd\u8bc1\u200b\u6bcf\u4e2a\u200b\u6743\u91cd\u200b\\(w\\)\u200b\u7684\u200b\u65b9\u5dee\u200b\u662f\u200b\\(1/n\\)\u3002\u200b\u53c8\u200b\u56e0\u4e3a\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u968f\u673a\u53d8\u91cf\u200b\\(X\\)\u200b\u548c\u200b\u6807\u91cf\u200b\\(a\\)\uff0c\u200b\u6709\u200b\\(\\text{Var}(aX) = a^2\\text{Var}(X)\\)\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u8bf4\u660e\u200b\u53ef\u4ee5\u200b\u57fa\u4e8e\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u200b\u9ad8\u65af\u5206\u5e03\u200b\uff0c\u200b\u7136\u540e\u200b\u4e58\u4ee5\u200b\\(a = \\sqrt{1/n}\\)\uff0c\u200b\u4f7f\u200b\u5176\u200b\u65b9\u5dee\u200b\u4e3a\u200b\\(1/n\\)\uff0c\u200b\u4e8e\u662f\u200b\u5f97\u51fa\u200b\uff1a<code>w = np.random.randn(n) / sqrt(n)</code>\u3002</p> <p>Glorot\u200b\u7b49\u200b\u5728\u200b\u8bba\u6587\u200bUnderstanding the difficulty of training deep feedforward neural networks\u200b\u4e2d\u200b\u4f5c\u51fa\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u5206\u6790\u200b\u3002\u200b\u5728\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u63a8\u8350\u200b\u521d\u59cb\u5316\u200b\u516c\u5f0f\u200b\u4e3a\u200b\\(\\text{Var}(w) = 2/(n_{in} + n_{out})\\)\uff0c\u200b\u5176\u4e2d\u200b\\(n_{in}, n_{out}\\)\u200b\u662f\u200b\u5728\u200b\u524d\u200b\u4e00\u5c42\u200b\u548c\u200b\u540e\u200b\u4e00\u5c42\u200b\u4e2d\u200b\u5355\u5143\u200b\u7684\u200b\u4e2a\u6570\u200b\u3002\u200b\u8fd9\u662f\u200b\u57fa\u4e8e\u200b\u59a5\u534f\u200b\u548c\u200b\u5bf9\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u68af\u5ea6\u200b\u7684\u200b\u5206\u6790\u200b\u5f97\u51fa\u200b\u7684\u200b\u7ed3\u8bba\u200b\u3002\u200b\u8be5\u200b\u4e3b\u9898\u200b\u4e0b\u200b\u6700\u65b0\u200b\u7684\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\u662f\u200b\uff1aDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\uff0c\u200b\u4f5c\u8005\u200b\u662f\u200bHe\u200b\u7b49\u200b\u4eba\u200b\u3002\u200b\u6587\u4e2d\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u4e00\u79cd\u200b\u9488\u5bf9\u200bReLU\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u7279\u6b8a\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u5e76\u200b\u7ed9\u51fa\u200b\u7ed3\u8bba\u200b\uff1a\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u65b9\u5dee\u200b\u5e94\u8be5\u200b\u662f\u200b\\(2.0/n\\)\u3002\u200b\u4ee3\u7801\u200b\u4e3a\u200b<code>w = np.random.randn(n) * sqrt(2.0/n)</code>\u3002\u200b\u8fd9\u4e2a\u200b\u5f62\u5f0f\u200b\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7b97\u6cd5\u200b\u4f7f\u7528\u200bReLU\u200b\u795e\u7ecf\u5143\u200b\u65f6\u200b\u7684\u200b\u5f53\u524d\u200b\u6700\u4f73\u200b\u63a8\u8350\u200b\u3002</p> <p>\u200b\u7a00\u758f\u200b\u521d\u59cb\u5316\u200b\uff08Sparse initialization\uff09\u3002\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5904\u7406\u200b\u975e\u200b\u6807\u5b9a\u200b\u65b9\u5dee\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5c06\u200b\u6240\u6709\u6743\u200b\u91cd\u200b\u77e9\u9635\u200b\u8bbe\u200b\u4e3a\u200b0\uff0c\u200b\u4f46\u662f\u200b\u4e3a\u4e86\u200b\u6253\u7834\u200b\u5bf9\u79f0\u6027\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u540c\u4e0b\u200b\u4e00\u5c42\u200b\u56fa\u5b9a\u200b\u6570\u76ee\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u968f\u673a\u200b\u8fde\u63a5\u200b\uff08\u200b\u5176\u200b\u6743\u91cd\u200b\u6570\u503c\u200b\u7531\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u7684\u200b\u9ad8\u65af\u5206\u5e03\u200b\u751f\u6210\u200b\uff09\u3002\u200b\u4e00\u4e2a\u200b\u6bd4\u8f83\u200b\u5178\u578b\u200b\u7684\u200b\u8fde\u63a5\u200b\u6570\u76ee\u200b\u662f\u200b10\u200b\u4e2a\u200b\u3002</p> <p>\u200b\u504f\u7f6e\u200b\uff08biases\uff09\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u901a\u5e38\u200b\u5c06\u200b\u504f\u7f6e\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b0\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u968f\u673a\u200b\u5c0f\u200b\u6570\u503c\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u5df2\u7ecf\u200b\u6253\u7834\u200b\u4e86\u200b\u5bf9\u79f0\u6027\u200b\u3002\u200b\u5bf9\u4e8e\u200bReLU\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u6709\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u559c\u6b22\u200b\u4f7f\u7528\u200b\u5982\u200b0.01\u200b\u8fd9\u6837\u200b\u7684\u200b\u5c0f\u200b\u6570\u503c\u200b\u5e38\u91cf\u200b\u4f5c\u4e3a\u200b\u6240\u6709\u200b\u504f\u7f6e\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u4ed6\u4eec\u200b\u8ba4\u4e3a\u200b\u8fd9\u6837\u200b\u505a\u80fd\u200b\u8ba9\u200b\u6240\u6709\u200b\u7684\u200bReLU\u200b\u5355\u5143\u200b\u4e00\u200b\u5f00\u59cb\u200b\u5c31\u200b\u6fc0\u6d3b\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u80fd\u200b\u4fdd\u5b58\u200b\u5e76\u200b\u4f20\u64ad\u200b\u4e00\u4e9b\u200b\u68af\u5ea6\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u6837\u200b\u505a\u200b\u662f\u4e0d\u662f\u200b\u603b\u662f\u200b\u80fd\u200b\u63d0\u9ad8\u200b\u7b97\u6cd5\u200b\u6027\u80fd\u200b\u5e76\u200b\u4e0d\u200b\u6e05\u695a\u200b\uff08\u200b\u6709\u65f6\u5019\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u53cd\u800c\u200b\u663e\u793a\u200b\u6027\u80fd\u200b\u66f4\u5dee\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u901a\u5e38\u200b\u8fd8\u662f\u200b\u4f7f\u7528\u200b0\u200b\u6765\u200b\u521d\u59cb\u5316\u200b\u504f\u7f6e\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u5b9e\u8df5\u200b\u3002\u200b\u5f53\u524d\u200b\u7684\u200b\u63a8\u8350\u200b\u662f\u200b\u4f7f\u7528\u200bReLU\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f7f\u7528\u200b<code>w = np.random.randn(n) * sqrt(2.0/n)</code>\u200b\u6765\u200b\u8fdb\u884c\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u5173\u4e8e\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8fd9\u200b\u7bc7\u6587\u7ae0\u200b\u6709\u200b\u8ba8\u8bba\u200b\u3002</p> <p>\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\uff08Batch Normalization\uff09\u3002\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\u662f\u200bloffe\u200b\u548c\u200bSzegedy\u200b\u6700\u8fd1\u200b\u624d\u200b\u63d0\u51fa\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u51cf\u8f7b\u200b\u4e86\u200b\u5982\u4f55\u200b\u5408\u7406\u200b\u521d\u59cb\u5316\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8fd9\u4e2a\u200b\u68d8\u624b\u200b\u95ee\u9898\u200b\u5e26\u6765\u200b\u7684\u200b\u5934\u75db\u200b\uff1a\uff09\uff0c\u200b\u5176\u200b\u505a\u6cd5\u200b\u662f\u200b\u8ba9\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5f00\u59cb\u200b\u524d\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u7f51\u7edc\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u4f7f\u200b\u5176\u200b\u670d\u4ece\u200b\u6807\u51c6\u200b\u9ad8\u65af\u5206\u5e03\u200b\u3002\u200b\u56e0\u4e3a\u200b\u5f52\u4e00\u5316\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u53ef\u200b\u6c42\u5bfc\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e0a\u8ff0\u200b\u601d\u8def\u200b\u662f\u200b\u53ef\u884c\u200b\u7684\u200b\u3002\u200b\u5728\u200b\u5b9e\u73b0\u200b\u5c42\u9762\u200b\uff0c\u200b\u5e94\u7528\u200b\u8fd9\u4e2a\u200b\u6280\u5de7\u200b\u901a\u5e38\u200b\u610f\u5473\u7740\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff08\u200b\u6216\u8005\u200b\u662f\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u540e\u7eed\u200b\u4f1a\u200b\u8bb2\u200b\uff09\u200b\u4e0e\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u4e4b\u95f4\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200bBatchNorm\u200b\u5c42\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e2a\u200b\u6280\u5de7\u200b\u672c\u200b\u8282\u200b\u4e0d\u4f1a\u200b\u5c55\u5f00\u200b\u8bb2\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4e0a\u9762\u200b\u7684\u200b\u53c2\u8003\u6587\u732e\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u8bb2\u5f97\u200b\u5f88\u200b\u6e05\u695a\u200b\u4e86\u200b\uff0c\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u7684\u200b\u662f\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\u5df2\u7ecf\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u5e38\u89c1\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\u7f51\u7edc\u200b\u5bf9\u4e8e\u200b\u4e0d\u597d\u200b\u7684\u200b\u521d\u59cb\u503c\u200b\u6709\u200b\u66f4\u200b\u5f3a\u200b\u7684\u200b\u9c81\u68d2\u6027\u200b\u3002\u200b\u6700\u540e\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u603b\u7ed3\u200b\uff1a\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\u5728\u200b\u7f51\u7edc\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u4e4b\u524d\u200b\u90fd\u200b\u505a\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u53ea\u662f\u200b\u8fd9\u79cd\u200b\u64cd\u4f5c\u200b\u4ee5\u200b\u53e6\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u7f51\u7edc\u200b\u96c6\u6210\u200b\u5728\u200b\u4e86\u200b\u4e00\u8d77\u200b\u3002\u200b\u641e\u5b9a\u200b\uff01</p>"},{"location":"courses/neural_network/neural-networks-2/#regularization","title":"\u6b63\u5219\u200b\u5316\u200b Regularization","text":"<p>\u200b\u6709\u200b\u4e0d\u5c11\u200b\u65b9\u6cd5\u200b\u662f\u200b\u901a\u8fc7\u200b\u63a7\u5236\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5bb9\u91cf\u200b\u6765\u200b\u9632\u6b62\u200b\u5176\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\uff1a</p> <p>L2\u200b\u6b63\u5219\u200b\u5316\u200b\u53ef\u80fd\u200b\u662f\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u65b9\u6cd5\u200b\u4e86\u200b\u3002\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u60e9\u7f5a\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e2d\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u7684\u200b\u5e73\u65b9\u200b\u5c06\u200b\u5176\u200b\u5b9e\u73b0\u200b\u3002\u200b\u5373\u200b\u5bf9\u4e8e\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u6743\u91cd\u200b\\(w\\)\uff0c\u200b\u5411\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e2d\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b \\(\\frac{1}{2} \\lambda w^2\\)\uff0c\u200b\u5176\u4e2d\u200b\\(\\lambda\\)\u200b\u662f\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u3002\u200b\u524d\u9762\u200b\u8fd9\u4e2a\u200b\\(\\frac{1}{2}\\)\u200b\u5f88\u200b\u5e38\u89c1\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u52a0\u4e0a\u200b\\(\\frac{1}{2}\\)\u200b\u540e\u200b\uff0c\u200b\u8be5\u200b\u5f0f\u5b50\u200b\u5173\u4e8e\u200b\ufffd\u200b\u68af\u5ea6\u200b\u5c31\u662f\u200b\\(\\lambda w\\)\u200b\u800c\u200b\u4e0d\u662f\u200b\\(2 \\lambda w\\)\u200b\u4e86\u200b\u3002L2\u200b\u6b63\u5219\u200b\u5316\u200b\u53ef\u4ee5\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\u4e3a\u200b\u5b83\u200b\u5bf9\u4e8e\u200b\u5927\u200b\u6570\u503c\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u8fdb\u884c\u200b\u4e25\u5389\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u503e\u5411\u200b\u4e8e\u200b\u66f4\u52a0\u200b\u5206\u6563\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u3002\u200b\u5728\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u8fc7\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8f93\u5165\u200b\u548c\u200b\u6743\u91cd\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e58\u6cd5\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4f18\u826f\u200b\u7684\u200b\u7279\u6027\u200b\uff1a\u200b\u4f7f\u200b\u7f51\u7edc\u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4e25\u91cd\u200b\u4f9d\u8d56\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u4e2d\u200b\u67d0\u4e9b\u200b\u5c0f\u200b\u90e8\u5206\u200b\u7279\u5f81\u200b\u3002\u200b\u6700\u540e\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u5728\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u548c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f7f\u7528\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u610f\u5473\u7740\u200b\u6240\u6709\u200b\u7684\u200b\u6743\u91cd\u200b\u90fd\u200b\u4ee5\u200b\u5411\u200b<code>W += -lambda * W</code> \u200b\u7740\u200b0\u200b\u7ebf\u6027\u200b\u4e0b\u964d\u200b\u3002</p> <p>L1\u200b\u6b63\u5219\u200b\u5316\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u76f8\u5bf9\u200b\u5e38\u7528\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u65b9\u6cd5\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200b \\(w\\)\u200b\u6211\u4eec\u200b\u90fd\u200b\u5411\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\\(\\lambda  \\mid w \\mid\\)\u3002L1\u200b\u548c\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u7ec4\u5408\u200b\uff1a\\(\\lambda_1 \\mid w \\mid + \\lambda_2 w^2\\)\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u88ab\u79f0\u4f5c\u200bElastic net regularizaton\u3002L1\u200b\u6b63\u5219\u200b\u5316\u6709\u200b\u4e00\u4e2a\u200b\u6709\u8da3\u200b\u7684\u200b\u6027\u8d28\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u8ba9\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u5728\u200b\u6700\u4f18\u5316\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u53d8\u5f97\u200b\u7a00\u758f\u200b\uff08\u200b\u5373\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b0\uff09\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u4f7f\u7528\u200bL1\u200b\u6b63\u5219\u200b\u5316\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u6700\u540e\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u5b83\u4eec\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u7a00\u758f\u200b\u5b50\u96c6\u200b\uff0c\u200b\u540c\u65f6\u200b\u5bf9\u4e8e\u200b\u566a\u97f3\u200b\u8f93\u5165\u200b\u5219\u200b\u51e0\u4e4e\u200b\u662f\u200b\u4e0d\u53d8\u200b\u7684\u200b\u4e86\u200b\u3002\u200b\u76f8\u200b\u8f83\u200bL1\u200b\u6b63\u5219\u200b\u5316\u200b\uff0cL2\u200b\u6b63\u5219\u200b\u5316\u4e2d\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u5927\u591a\u200b\u662f\u200b\u5206\u6563\u200b\u7684\u200b\u5c0f\u200b\u6570\u5b57\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0d\u662f\u200b\u7279\u522b\u200b\u5173\u6ce8\u200b\u67d0\u4e9b\u200b\u660e\u786e\u200b\u7684\u200b\u7279\u5f81\u9009\u62e9\u200b\uff0c\u200b\u4e00\u822c\u8bf4\u6765\u200bL2\u200b\u6b63\u5219\u200b\u5316\u90fd\u4f1a\u200b\u6bd4\u200bL1\u200b\u6b63\u5219\u200b\u5316\u200b\u6548\u679c\u200b\u597d\u200b\u3002</p> <p>\u200b\u6700\u5927\u200b\u8303\u5f0f\u200b\u7ea6\u675f\u200b\uff08Max norm constraints\uff09\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u662f\u200b\u7ed9\u200b\u6bcf\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u4e2d\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u7684\u200b\u91cf\u7ea7\u200b\u8bbe\u5b9a\u200b\u4e0a\u9650\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u6295\u5f71\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200bprojected gradient descent\u200b\u6765\u200b\u786e\u4fdd\u200b\u8fd9\u4e00\u200b\u7ea6\u675f\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4e0e\u200b\u4e4b\u200b\u5bf9\u5e94\u200b\u7684\u200b\u662f\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u65b9\u5f0f\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u7136\u540e\u200b\u8981\u6c42\u200b\u795e\u7ecf\u5143\u200b\u4e2d\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\\(\\vec{w}\\) \u200b\u5fc5\u987b\u200b\u6ee1\u8db3\u200b\\(\\Vert \\vec{w} \\Vert_2 &lt; c\\)\u200b\u8fd9\u4e00\u200b\u6761\u4ef6\u200b\uff0c\u200b\u4e00\u822c\u200b\\(c\\)\u200b\u503c\u4e3a\u200b3\u200b\u6216\u8005\u200b4\u3002\u200b\u6709\u200b\u7814\u7a76\u8005\u200b\u53d1\u6587\u200b\u79f0\u200b\u5728\u200b\u4f7f\u7528\u200b\u8fd9\u79cd\u200b\u6b63\u5219\u200b\u5316\u200b\u65b9\u6cd5\u200b\u65f6\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\u3002\u200b\u8fd9\u79cd\u200b\u6b63\u5219\u200b\u5316\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u826f\u597d\u200b\u7684\u200b\u6027\u8d28\u200b\uff0c\u200b\u5373\u4f7f\u200b\u5728\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u8fc7\u9ad8\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u4e5f\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u6570\u503c\u200b\u201c\u200b\u7206\u70b8\u200b\u201d\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u200b\u7684\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u59cb\u7ec8\u200b\u662f\u200b\u88ab\u200b\u9650\u5236\u200b\u7740\u200b\u7684\u200b\u3002</p> <p>\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff08Dropout\uff09\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u53c8\u200b\u6781\u5176\u200b\u6709\u6548\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u65b9\u6cd5\u200b\u3002\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u7531\u200bSrivastava\u200b\u5728\u200b\u8bba\u6587\u200bDropout: A Simple Way to Prevent Neural Networks from Overfitting\u200b\u4e2d\u200b\u63d0\u51fa\u200b\u7684\u200b\uff0c\u200b\u4e0e\u200bL1\u200b\u6b63\u5219\u200b\u5316\u200b\uff0cL2\u200b\u6b63\u5219\u200b\u5316\u200b\u548c\u200b\u6700\u5927\u200b\u8303\u5f0f\u200b\u7ea6\u675f\u200b\u7b49\u200b\u65b9\u6cd5\u200b\u4e92\u4e3a\u8865\u5145\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u5b9e\u73b0\u200b\u65b9\u6cd5\u200b\u662f\u200b\u8ba9\u200b\u795e\u7ecf\u5143\u200b\u4ee5\u8d85\u200b\u53c2\u6570\u200b\\(p\\)\u200b\u7684\u200b\u6982\u7387\u200b\u88ab\u200b\u6fc0\u6d3b\u200b\u6216\u8005\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\u3002</p> <p> </p> <p>\u200b\u4ece\u200bDropout\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53d6\u5f97\u200b\u7684\u200b\u56fe\u8868\u8bf4\u660e\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\u6982\u5ff5\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u5bf9\u200b\u5b8c\u6574\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u62bd\u6837\u200b\u51fa\u200b\u4e00\u4e9b\u200b\u5b50\u96c6\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u57fa\u4e8e\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u53ea\u200b\u66f4\u65b0\u200b\u5b50\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\uff08\u200b\u7136\u800c\u200b\uff0c\u200b\u6570\u91cf\u200b\u5de8\u5927\u200b\u7684\u200b\u5b50\u200b\u7f51\u7edc\u200b\u4eec\u200b\u5e76\u200b\u4e0d\u662f\u200b\u76f8\u4e92\u200b\u72ec\u7acb\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u5171\u4eab\u200b\u53c2\u6570\u200b\uff09\u3002\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\u662f\u200b\u5bf9\u200b\u6570\u91cf\u200b\u5de8\u5927\u200b\u7684\u200b\u5b50\u200b\u7f51\u7edc\u200b\u4eec\u200b\u505a\u200b\u4e86\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\uff08model ensemble\uff09\uff0c\u200b\u4ee5\u6b64\u200b\u6765\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u4e00\u4e2a\u200b\u5e73\u5747\u200b\u7684\u200b\u9884\u6d4b\u200b\uff08\u200b\u5728\u200b\u4e0b\u200b\u4e00\u90e8\u5206\u200b\u4e2d\u6709\u200b\u5173\u4e8e\u200b\u96c6\u5408\u200bensembles\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\uff09\u3002</p> <p>\u200b\u4e00\u4e2a\u200b3\u200b\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u666e\u901a\u200b\u7248\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e0b\u9762\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\uff1a</p> <pre><code>\"\"\" \u200b\u666e\u901a\u200b\u7248\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b: \u200b\u4e0d\u200b\u63a8\u8350\u200b\u5b9e\u73b0\u200b (\u200b\u770b\u200b\u4e0b\u9762\u200b\u7b14\u8bb0\u200b) \"\"\"\n\np = 0.5 # \u200b\u6fc0\u6d3b\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6982\u7387\u200b. p\u200b\u503c\u200b\u66f4\u200b\u9ad8\u200b = \u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u66f4\u5f31\u200b\n\ndef train_step(X):\n\"\"\" X\u200b\u4e2d\u662f\u200b\u8f93\u5165\u200b\u6570\u636e\u200b \"\"\"\n\n  # 3\u200b\u5c42\u200bneural network\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = np.random.rand(*H1.shape) &lt; p # \u200b\u7b2c\u4e00\u4e2a\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u906e\u7f69\u200b\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = np.random.rand(*H2.shape) &lt; p # \u200b\u7b2c\u4e8c\u4e2a\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u906e\u7f69\u200b\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b:\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b... (\u200b\u7565\u200b)\n  # \u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b... (\u200b\u7565\u200b)\n\ndef predict(X):\n  # \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\n  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # \u200b\u6ce8\u610f\u200b\uff1a\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u8981\u200b\u4e58\u4ee5\u200bp\n  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # \u200b\u6ce8\u610f\u200b\uff1a\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u8981\u200b\u4e58\u4ee5\u200bp\n  out = np.dot(W3, H2) + b3\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0ctrain_step\u200b\u51fd\u6570\u200b\u5728\u200b\u7b2c\u4e00\u4e2a\u200b\u9690\u5c42\u200b\u548c\u200b\u7b2c\u4e8c\u4e2a\u200b\u9690\u5c42\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e24\u6b21\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u3002\u200b\u5728\u200b\u8f93\u5165\u200b\u5c42\u200b\u4e0a\u9762\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u4e5f\u200b\u662f\u200b\u53ef\u4ee5\u200b\u7684\u200b\uff0c\u200b\u4e3a\u6b64\u200b\u9700\u8981\u200b\u4e3a\u200b\u8f93\u5165\u200b\u6570\u636e\u200bX\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e8c\u503c\u200b\u7684\u200b\u906e\u7f69\u200b\u3002\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u4f46\u662f\u200b\u80af\u5b9a\u200b\u9700\u8981\u200b\u5c06\u200b\u906e\u7f69\u200bU1\u200b\u548c\u200bU2\u200b\u52a0\u5165\u200b\u8fdb\u53bb\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5728\u200b<code>predict</code>\u200b\u51fd\u6570\u200b\u4e2d\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff0c\u200b\u4f46\u662f\u200b\u5bf9\u4e8e\u200b\u4e24\u4e2a\u200b\u9690\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u90fd\u200b\u8981\u200b\u4e58\u4ee5\u200b\\(p\\)\uff0c\u200b\u8c03\u6574\u200b\u5176\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u3002\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u6240\u6709\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u80fd\u200b\u770b\u89c1\u200b\u5b83\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u9884\u671f\u200b\u8f93\u51fa\u200b\u662f\u200b\u4e00\u81f4\u200b\u7684\u200b\u3002\u200b\u4ee5\u200b\\(p = 0.5\\)\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u795e\u7ecf\u5143\u200b\u5fc5\u987b\u200b\u628a\u200b\u5b83\u4eec\u200b\u7684\u200b\u8f93\u51fa\u200b\u51cf\u534a\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\u5b83\u4eec\u200b\u7684\u200b\u8f93\u51fa\u200b\u53ea\u6709\u200b\u4e00\u534a\u200b\u3002\u200b\u4e3a\u4e86\u200b\u7406\u89e3\u200b\u8fd9\u70b9\u200b\uff0c\u200b\u5148\u200b\u5047\u8bbe\u200b\u6709\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\\(x\\)\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u5728\u200bdropout\u200b\u4e4b\u524d\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u8be5\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c31\u662f\u200b\\(px + (1-p)0\\)\uff0c\u200b\u8fd9\u200b\u662f\u200b\u6709\u200b\\(1-p\\)\u200b\u7684\u200b\u6982\u7387\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e3a\u200b0\u3002\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u795e\u7ecf\u5143\u200b\u603b\u662f\u200b\u6fc0\u6d3b\u200b\u7684\u200b\uff0c\u200b\u5c31\u200b\u5fc5\u987b\u200b\u8c03\u6574\u200b\\(x \\rightarrow px\\)\u200b\u6765\u200b\u4fdd\u6301\u200b\u540c\u6837\u200b\u7684\u200b\u9884\u671f\u200b\u8f93\u51fa\u200b\u3002\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u4f1a\u200b\u5728\u200b\u6240\u6709\u200b\u53ef\u80fd\u200b\u7684\u200b\u4e8c\u503c\u200b\u906e\u7f69\u200b\uff08\u200b\u4e5f\u200b\u5c31\u662f\u200b\u6570\u91cf\u200b\u5e9e\u5927\u200b\u7684\u200b\u6240\u6709\u200b\u5b50\u200b\u7f51\u7edc\u200b\uff09\u200b\u4e2d\u200b\u8fed\u4ee3\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u5b83\u4eec\u200b\u7684\u200b\u534f\u4f5c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u8fdb\u884c\u200b\u8fd9\u79cd\u200b\u51cf\u5f31\u200b\u7684\u200b\u64cd\u4f5c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u4e0e\u200b\u4e4b\u200b\u76f8\u5173\u200b\u7684\u200b\u3002</p> <p>\u200b\u5173\u952e\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b<code>predict</code>\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u518d\u200b\u6267\u884c\u200bdropout\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5bf9\u200b\u4e24\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u8fdb\u884c\u200b\u4e86\u200b\\(p\\)\u200b\u7684\u200b\u7f29\u653e\u200b\u3002\u200b\u8fd9\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\uff0c\u200b\u6240\u6709\u200b\u795e\u7ecf\u5143\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u5b83\u4eec\u200b\u7684\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u8f93\u51fa\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u9884\u671f\u200b\u8f93\u51fa\u200b\u76f8\u540c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5f53\u200b\\(p = 0.5\\)\u200b\u65f6\u200b\uff0c\u200b\u795e\u7ecf\u5143\u200b\u5fc5\u987b\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u5c06\u200b\u5176\u200b\u8f93\u51fa\u200b\u51cf\u534a\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u83b7\u5f97\u200b\u4e0e\u200b\u671f\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\u76f8\u540c\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002\u200b\u4e3a\u4e86\u200b\u7406\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8003\u8651\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\\(x\\)\uff08\u200b\u5728\u200bdropout\u200b\u4e4b\u524d\u200b\uff09\u3002\u200b\u6709\u200b\u4e86\u200bdropout\uff0c\u200b\u8fd9\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u9884\u671f\u200b\u8f93\u51fa\u200b\u5c06\u200b\u53d8\u4e3a\u200b\\(px + (1-p)0\\)\uff0c\u200b\u56e0\u4e3a\u200b\u8be5\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u6709\u200b\\(1-p\\)\u200b\u7684\u200b\u6982\u7387\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u603b\u662f\u200b\u4fdd\u6301\u200b\u795e\u7ecf\u5143\u200b\u6d3b\u8dc3\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u8c03\u6574\u200b\\(x \\rightarrow px\\)\u200b\u4ee5\u200b\u4fdd\u6301\u200b\u76f8\u540c\u200b\u7684\u200b\u9884\u671f\u200b\u8f93\u51fa\u200b\u3002\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8bc1\u660e\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u6267\u884c\u200b\u8fd9\u79cd\u200b\u8870\u51cf\u200b\u4e0e\u200b\u904d\u5386\u200b\u6240\u6709\u200b\u53ef\u80fd\u200b\u7684\u200b\u4e8c\u8fdb\u5236\u200b\u63a9\u7801\u200b\uff08\u200b\u56e0\u6b64\u200b\u6240\u6709\u200b\u6307\u6570\u200b\u591a\u200b\u7684\u200b\u5b50\u200b\u7f51\u7edc\u200b\uff09\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u5b83\u4eec\u200b\u7684\u200b\u6574\u4f53\u200b\u9884\u6d4b\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u6709\u5173\u200b\u3002</p> <p>\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\u4e0d\u597d\u200b\u7684\u200b\u6027\u8d28\u200b\u662f\u200b\u5fc5\u987b\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u5bf9\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u8981\u200b\u6309\u7167\u200b\\(p\\)\u200b\u8fdb\u884c\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u8c03\u6574\u200b\u3002\u200b\u65e2\u7136\u200b\u6d4b\u8bd5\u200b\u6027\u80fd\u200b\u5982\u6b64\u200b\u5173\u952e\u200b\uff0c\u200b\u5b9e\u9645\u200b\u66f4\u200b\u503e\u5411\u200b\u4f7f\u7528\u200b\u53cd\u5411\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff08inverted dropout\uff09\uff0c\u200b\u5b83\u200b\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u5c31\u200b\u8fdb\u884c\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u8c03\u6574\u200b\uff0c\u200b\u4ece\u800c\u200b\u8ba9\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002\u200b\u8fd9\u6837\u200b\u505a\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u597d\u5904\u200b\uff0c\u200b\u65e0\u8bba\u200b\u4f60\u200b\u51b3\u5b9a\u200b\u662f\u5426\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff0c\u200b\u9884\u6d4b\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4ee3\u7801\u200b\u53ef\u4ee5\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002\u200b\u53cd\u5411\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>\"\"\" \n\u200b\u53cd\u5411\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b: \u200b\u63a8\u8350\u200b\u5b9e\u73b0\u200b\u65b9\u5f0f\u200b.\n\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200bdrop\u200b\u548c\u200b\u8c03\u6574\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u4e0d\u200b\u505a\u200b\u4efb\u4f55\u200b\u4e8b\u200b.\n\"\"\"\n\np = 0.5 # \u200b\u6fc0\u6d3b\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u6982\u7387\u200b. p\u200b\u503c\u200b\u66f4\u200b\u9ad8\u200b = \u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u66f4\u5f31\u200b\n\ndef train_step(X):\n  # 3\u200b\u5c42\u200bneural network\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\n  H1 = np.maximum(0, np.dot(W1, X) + b1)\n  U1 = (np.random.rand(*H1.shape) &lt; p) / p # \u200b\u7b2c\u4e00\u4e2a\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u906e\u7f69\u200b. \u200b\u6ce8\u610f\u200b/p!\n  H1 *= U1 # drop!\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  U2 = (np.random.rand(*H2.shape) &lt; p) / p # \u200b\u7b2c\u4e8c\u4e2a\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u906e\u7f69\u200b. \u200b\u6ce8\u610f\u200b/p!\n  H2 *= U2 # drop!\n  out = np.dot(W3, H2) + b3\n\n  # \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b:\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b... (\u200b\u7565\u200b)\n  # \u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b... (\u200b\u7565\u200b)\n\ndef predict(X):\n  # \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\n  H1 = np.maximum(0, np.dot(W1, X) + b1) # \u200b\u4e0d\u7528\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u8c03\u6574\u200b\u4e86\u200b\n  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n  out = np.dot(W3, H2) + b3\n</code></pre> <p>\u200b\u5728\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u53d1\u5e03\u200b\u540e\u200b\uff0c\u200b\u5f88\u5feb\u200b\u6709\u200b\u5927\u91cf\u200b\u7814\u7a76\u200b\u4e3a\u4ec0\u4e48\u200b\u5b83\u200b\u7684\u200b\u5b9e\u8df5\u200b\u6548\u679c\u200b\u5982\u6b64\u200b\u4e4b\u597d\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5b83\u200b\u548c\u200b\u5176\u4ed6\u200b\u6b63\u5219\u200b\u5316\u200b\u65b9\u6cd5\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u611f\u5174\u8da3\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u770b\u200b\u8fd9\u4e9b\u200b\u6587\u732e\u200b\uff1a</p> <ul> <li>Dropout paper by Srivastava et al. 2014.</li> <li>Dropout Training as Adaptive Regularization\uff1a\u201c\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\uff1a\u200b\u5728\u200b\u4f7f\u7528\u200b\u8d39\u5e0c\u5c14\u200b\u4fe1\u606f\u200b\u77e9\u9635\u200b\uff08fisher information matrix\uff09\u200b\u7684\u200b\u5bf9\u89d2\u200b\u9006\u200b\u77e9\u9635\u200b\u7684\u200b\u671f\u671b\u200b\u5bf9\u200b\u7279\u5f81\u200b\u8fdb\u884c\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u8c03\u6574\u200b\u540e\u200b\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u8fd9\u200b\u4e00\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4e0e\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u6b63\u5219\u200b\u5316\u662f\u200b\u4e00\u9636\u200b\u76f8\u7b49\u200b\u7684\u200b\u3002\u201d</li> </ul> <p>\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u7684\u200b\u566a\u97f3\u200b\u3002\u200b\u5728\u200b\u66f4\u200b\u4e00\u822c\u5316\u200b\u7684\u200b\u5206\u7c7b\u200b\u4e0a\u200b\uff0c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u5c5e\u4e8e\u200b\u7f51\u7edc\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u6709\u200b\u968f\u673a\u200b\u884c\u4e3a\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5206\u6790\u6cd5\u200b\uff08\u200b\u5728\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u672c\u4f8b\u200b\u4e2d\u200b\u5c31\u662f\u200b\u4e58\u4ee5\u200b\\(p\\)\uff09\u200b\u6216\u200b\u6570\u503c\u200b\u6cd5\u200b\uff08\u200b\u4f8b\u5982\u200b\u901a\u8fc7\u200b\u62bd\u6837\u200b\u51fa\u200b\u5f88\u591a\u200b\u5b50\u200b\u7f51\u7edc\u200b\uff0c\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e0d\u540c\u200b\u5b50\u200b\u7f51\u7edc\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u6700\u540e\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u53d6\u200b\u5e73\u5747\u200b\uff09\u200b\u5c06\u200b\u566a\u97f3\u200b\u8fb9\u7f18\u5316\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u65b9\u5411\u200b\u4e0a\u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7814\u7a76\u200b\u662f\u200bDropConnect\uff0c\u200b\u5b83\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4e00\u7cfb\u5217\u200b\u6743\u91cd\u200b\u88ab\u200b\u968f\u673a\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b0\u3002\u200b\u63d0\u524d\u200b\u8bf4\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u540c\u6837\u200b\u4f1a\u200b\u5438\u53d6\u200b\u8fd9\u200b\u7c7b\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4f18\u70b9\u200b\uff0c\u200b\u6bd4\u5982\u200b\u968f\u673a\u200b\u6c60\u5316\u200b\uff08stochastic pooling\uff09\uff0c\u200b\u5206\u7ea7\u200b\u6c60\u5316\u200b\uff08fractional pooling\uff09\uff0c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff08data augmentation\uff09\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u540e\u9762\u200b\u4f1a\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u3002</p> <p>\u200b\u504f\u7f6e\u200b\u6b63\u5219\u200b\u5316\u200b\u3002\u200b\u5728\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u8fc7\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u504f\u7f6e\u200b\u53c2\u6570\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u5e76\u200b\u4e0d\u200b\u5e38\u89c1\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u5728\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4e2d\u200b\u548c\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u5e76\u200b\u4e0d\u200b\u4ea7\u751f\u200b\u4e92\u52a8\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5e76\u4e0d\u9700\u8981\u200b\u63a7\u5236\u200b\u5176\u200b\u5728\u200b\u6570\u636e\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u7136\u800c\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff08\u200b\u4f7f\u7528\u200b\u4e86\u200b\u5408\u7406\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff09\uff0c\u200b\u5bf9\u200b\u504f\u7f6e\u200b\u8fdb\u884c\u200b\u6b63\u5219\u200b\u5316\u200b\u4e5f\u200b\u5f88\u5c11\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u7b97\u6cd5\u200b\u6027\u80fd\u200b\u53d8\u5dee\u200b\u3002\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u56e0\u4e3a\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u6743\u91cd\u200b\u53c2\u6570\u200b\uff0c\u200b\u504f\u7f6e\u200b\u53c2\u6570\u200b\u5b9e\u5728\u200b\u592a\u200b\u5c11\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5206\u7c7b\u5668\u200b\u9700\u8981\u200b\u5b83\u4eec\u200b\u6765\u200b\u83b7\u5f97\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u6570\u636e\u200b\u635f\u5931\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8fd8\u662f\u200b\u80fd\u591f\u200b\u627f\u53d7\u200b\u7684\u200b\u3002</p> <p>\u200b\u6bcf\u5c42\u200b\u6b63\u5219\u200b\u5316\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u5c42\u200b\u8fdb\u884c\u200b\u4e0d\u540c\u200b\u5f3a\u5ea6\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u5f88\u5c11\u200b\u89c1\u200b\uff08\u200b\u53ef\u80fd\u200b\u9664\u4e86\u200b\u8f93\u51fa\u200b\u5c42\u200b\u4ee5\u5916\u200b\uff09\uff0c\u200b\u5173\u4e8e\u200b\u8fd9\u4e2a\u200b\u601d\u8def\u200b\u7684\u200b\u76f8\u5173\u200b\u6587\u732e\u200b\u4e5f\u200b\u5f88\u5c11\u200b\u3002</p> <p>\u200b\u5b9e\u8df5\u200b\uff1a\u200b\u901a\u8fc7\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u83b7\u5f97\u200b\u4e00\u4e2a\u200b\u5168\u5c40\u200b\u4f7f\u7528\u200b\u7684\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u662f\u200b\u6bd4\u8f83\u200b\u5e38\u89c1\u200b\u7684\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u7684\u200b\u540c\u65f6\u200b\u5728\u200b\u6240\u6709\u200b\u5c42\u200b\u540e\u9762\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u4e5f\u200b\u5f88\u200b\u5e38\u89c1\u200b\u3002\\(p\\)\u200b\u503c\u200b\u4e00\u822c\u200b\u9ed8\u8ba4\u200b\u8bbe\u200b\u4e3a\u200b0.5\uff0c\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u5728\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u4e0a\u8c03\u200b\u53c2\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-2/#_4","title":"\u635f\u5931\u200b\u51fd\u6570","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8ba8\u8bba\u200b\u8fc7\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u90e8\u5206\u200b\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u770b\u505a\u200b\u662f\u200b\u5bf9\u6a21\u578b\u200b\u590d\u6742\u7a0b\u5ea6\u200b\u7684\u200b\u67d0\u79cd\u200b\u60e9\u7f5a\u200b\u3002\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b\u90e8\u5206\u200b\u662f\u200b\u6570\u636e\u200b\u635f\u5931\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6709\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8861\u91cf\u200b\u5206\u7c7b\u200b\u7b97\u6cd5\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u5373\u200b\u5206\u7c7b\u200b\u8bc4\u5206\u200b\uff09\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u7ed3\u679c\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\u3002\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u662f\u200b\u5bf9\u200b\u6240\u6709\u200b\u6837\u672c\u200b\u7684\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u6c42\u200b\u5e73\u5747\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\\(L = \\frac{1}{N} \\sum_i L_i\\) \u200b\u4e2d\u200b\uff0c\\(N\\)\u200b\u662f\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6570\u636e\u200b\u7684\u200b\u6837\u672c\u6570\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u8f93\u51fa\u200b\u5c42\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7b80\u5199\u200b\u4e3a\u200b\\(f = f(x_i; W)\\)\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u4e2d\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u89e3\u51b3\u200b\u4ee5\u4e0b\u200b\u51e0\u7c7b\u200b\u95ee\u9898\u200b\uff1a</p> <p>\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u662f\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u8ba8\u8bba\u200b\u7684\u200b\u3002\u200b\u5728\u200b\u8be5\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6709\u200b\u4e00\u4e2a\u200b\u88c5\u6ee1\u200b\u6837\u672c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u552f\u4e00\u200b\u7684\u200b\u6b63\u786e\u200b\u6807\u7b7e\u200b\uff08\u200b\u662f\u200b\u56fa\u5b9a\u200b\u5206\u7c7b\u200b\u6807\u7b7e\u200b\u4e4b\u4e00\u200b\uff09\u3002\u200b\u5728\u200b\u8fd9\u7c7b\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5c31\u662f\u200bSVM\uff08\u200b\u662f\u200bWeston Watkins \u200b\u516c\u5f0f\u200b\uff09\uff1a</p> \\[ L_i = \\sum_{j\\neq y_i} \\max(0, f_j - f_{y_i} + 1) \\] <p>\u200b\u4e4b\u524d\u200b\u7b80\u8981\u200b\u63d0\u8d77\u8fc7\u200b\uff0c\u200b\u6709\u4e9b\u200b\u5b66\u8005\u200b\u7684\u200b\u8bba\u6587\u200b\u4e2d\u200b\u6307\u51fa\u200b\u5e73\u65b9\u200b\u6298\u53f6\u200b\u635f\u5931\u200b\uff08\u200b\u5373\u200b\u4f7f\u7528\u200b\\(\\max(0, f_j - f_{y_i} + 1)^2\\)\uff09\u200b\u7b97\u6cd5\u200b\u7684\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u66f4\u597d\u200b\u3002\u200b\u7b2c\u4e8c\u4e2a\u200b\u5e38\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u662f\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u5b83\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\uff1a</p> \\[ L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\] <p>\u200b\u95ee\u9898\u200b\uff1a\u200b\u7c7b\u522b\u200b\u6570\u76ee\u200b\u5de8\u5927\u200b\u3002\u200b\u5f53\u200b\u6807\u7b7e\u96c6\u200b\u975e\u5e38\u200b\u5e9e\u5927\u200b\uff08\u200b\u4f8b\u5982\u200b\u5b57\u5178\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u82f1\u8bed\u5355\u8bcd\u200b\uff0c\u200b\u6216\u8005\u200bImageNet\u200b\u4e2d\u200b\u7684\u200b22000\u200b\u79cd\u200b\u5206\u7c7b\u200b\uff09\uff0c\u200b\u8ba1\u7b97\u200b\u5b8c\u6574\u200b\u7684\u200bsoftmax\u200b\u6982\u7387\u200b\u53d8\u5f97\u200b\u6602\u8d35\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u67d0\u4e9b\u200b\u5e94\u7528\u200b\uff0c\u200b\u8fd1\u4f3c\u200b\u7248\u672c\u200b\u5f88\u200b\u53d7\u6b22\u8fce\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5206\u5c42\u200bHierarchical Softmax\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\uff08\u200b\u8bf7\u53c2\u9605\u200b\u6b64\u5904\u200b\u4e00\u4e2a\u200b\u89e3\u91ca\u200b\uff08pdf\uff09\uff09\u3002\u200b\u5206\u5c42\u200bsoftmax\u200b\u5c06\u200b\u6807\u7b7e\u200b\u5206\u89e3\u6210\u200b\u4e00\u4e2a\u200b\u6811\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u6807\u7b7e\u200b\u90fd\u200b\u8868\u793a\u200b\u6210\u200b\u8fd9\u4e2a\u200b\u6811\u4e0a\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8def\u5f84\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6811\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u8282\u70b9\u200b\u5904\u200b\u90fd\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\u6765\u200b\u5728\u200b\u5de6\u200b\u548c\u200b\u53f3\u200b\u5206\u679d\u200b\u4e4b\u95f4\u200b\u505a\u200b\u51b3\u7b56\u200b\u3002\u200b\u6811\u200b\u7684\u200b\u7ed3\u6784\u200b\u5bf9\u4e8e\u200b\u7b97\u6cd5\u200b\u7684\u200b\u6700\u7ec8\u200b\u7ed3\u679c\u200b\u5f71\u54cd\u200b\u5f88\u5927\u200b\uff0c\u200b\u800c\u4e14\u200b\u4e00\u822c\u200b\u9700\u8981\u200b\u5177\u4f53\u200b\u95ee\u9898\u200b\u5177\u4f53\u5206\u6790\u200b\u3002</p> <p>\u200b\u5c5e\u6027\u200b\uff08Attribute\uff09\u200b\u5206\u7c7b\u200b\u3002\u200b\u4e0a\u9762\u200b\u4e24\u4e2a\u200b\u635f\u5931\u200b\u516c\u5f0f\u200b\u7684\u200b\u524d\u63d0\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u5047\u8bbe\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u53ea\u6709\u200b\u4e00\u4e2a\u200b\u6b63\u786e\u200b\u7684\u200b\u6807\u7b7e\u200b\\(y_i\\)\u3002\u200b\u4f46\u662f\u200b\u5982\u679c\u200b\\(y_i\\)\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e8c\u503c\u200b\u5411\u91cf\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u53ef\u80fd\u200b\u6709\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u6ca1\u6709\u200b\u67d0\u4e2a\u200b\u5c5e\u6027\u200b\uff0c\u200b\u800c\u4e14\u200b\u5c5e\u6027\u200b\u4e4b\u95f4\u200b\u5e76\u200b\u4e0d\u200b\u76f8\u4e92\u200b\u6392\u65a5\u200b\u5462\u200b\uff1f\u200b\u6bd4\u5982\u200b\u5728\u200bInstagram\u200b\u4e0a\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u770b\u6210\u200b\u662f\u200b\u88ab\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u6807\u7b7e\u200b\u96c6\u5408\u200b\u4e2d\u200b\u7684\u200b\u67d0\u4e2a\u200b\u5b50\u96c6\u200b\u6253\u200b\u4e0a\u200b\u6807\u7b7e\u200b\uff0c\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\u4e0a\u200b\u53ef\u80fd\u200b\u6709\u200b\u591a\u4e2a\u200b\u6807\u7b7e\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u660e\u667a\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u5c5e\u6027\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u9488\u5bf9\u200b\u6bcf\u4e2a\u200b\u5206\u7c7b\u200b\u7684\u200b\u4e8c\u200b\u5206\u7c7b\u5668\u200b\u4f1a\u200b\u91c7\u7528\u200b\u4e0b\u9762\u200b\u7684\u200b\u516c\u5f0f\u200b\uff1a</p> \\[ L_i = \\sum_j \\max(0, 1 - y_{ij} f_j) \\] <p>\u200b\u4e0a\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u6c42\u548c\u200b\u662f\u200b\u5bf9\u200b\u6240\u6709\u200b\u5206\u7c7b\u200b\\(j\\)\uff0c\\(y_{ij}\\)\u200b\u7684\u200b\u503c\u200b\u4e3a\u200b1\u200b\u6216\u8005\u200b-1\uff0c\u200b\u5177\u4f53\u200b\u6839\u636e\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u6837\u672c\u200b\u662f\u5426\u200b\u88ab\u200b\u7b2c\u200bj\u200b\u4e2a\u200b\u5c5e\u6027\u200b\u6253\u200b\u6807\u7b7e\u200b\u800c\u5b9a\u200b\uff0c\u200b\u5f53\u8be5\u200b\u7c7b\u522b\u200b\u88ab\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u5e76\u200b\u5c55\u793a\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5206\u503c\u200b\u5411\u91cf\u200b\\(f_j\\)\u200b\u4e3a\u200b\u6b63\u200b\uff0c\u200b\u5176\u4f59\u200b\u60c5\u51b5\u200b\u4e3a\u200b\u8d1f\u200b\u3002\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5f53\u200b\u4e00\u4e2a\u200b\u6b63\u200b\u6837\u672c\u200b\u7684\u200b\u5f97\u5206\u200b\u5c0f\u4e8e\u200b+1\uff0c\u200b\u6216\u8005\u200b\u4e00\u4e2a\u200b\u8d1f\u200b\u6837\u672c\u200b\u5f97\u5206\u200b\u5927\u4e8e\u200b-1\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u5c31\u200b\u4f1a\u200b\u7d2f\u8ba1\u200b\u635f\u5931\u200b\u503c\u200b\u3002</p> <p>\u200b\u53e6\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5bf9\u200b\u6bcf\u79cd\u200b\u5c5e\u6027\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u903b\u8f91\u200b\u56de\u5f52\u200b\u5206\u7c7b\u5668\u200b\u3002\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u903b\u8f91\u200b\u56de\u5f52\u200b\u5206\u7c7b\u5668\u200b\u53ea\u6709\u200b\u4e24\u4e2a\u200b\u5206\u7c7b\u200b\uff080\uff0c1\uff09\uff0c\u200b\u5176\u4e2d\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b1\u200b\u7684\u200b\u6982\u7387\u200b\u8ba1\u7b97\u200b\u4e3a\u200b\uff1a</p> \\[ P(y = 1 \\mid x; w, b) = \\frac{1}{1 + e^{-(w^Tx +b)}} = \\sigma (w^Tx + b) \\] <p>\u200b\u56e0\u4e3a\u200b\u7c7b\u522b\u200b0\u200b\u548c\u200b\u7c7b\u522b\u200b1\u200b\u7684\u200b\u6982\u7387\u200b\u548c\u200b\u4e3a\u200b1\uff0c\u200b\u6240\u4ee5\u200b\u7c7b\u522b\u200b0\u200b\u7684\u200b\u6982\u7387\u200b\u4e3a\u200b\uff1a\\(P(y = 0 \\mid x; w, b) = 1 - P(y = 1 \\mid x; w,b)\\)\u3002\u200b\u8fd9\u6837\u200b\uff0c\u200b\u5982\u679c\u200b\\(\\sigma (w^Tx + b) &gt; 0.5\\)\u200b\u6216\u8005\u200b\\(w^Tx +b &gt; 0\\)\uff0c\u200b\u90a3\u4e48\u200b\u6837\u672c\u200b\u5c31\u8981\u200b\u88ab\u200b\u5206\u7c7b\u200b\u6210\u4e3a\u200b\u6b63\u200b\u6837\u672c\u200b\uff08y=1\uff09\u3002\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u968f\u540e\u200b\u6700\u5927\u5316\u200b\u8fd9\u4e00\u200b\u6982\u7387\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u786e\u4fe1\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u7b80\u5316\u200b\u4e3a\u200b\u6700\u5c0f\u5316\u200b\u8d1f\u200b\u5bf9\u6570\u200b\u4f3c\u7136\u200b\uff1a</p> \\[ L_i = -\\sum_j y_{ij} \\log(\\sigma(f_j)) + (1 - y_{ij}) \\log(1 - \\sigma(f_j)) \\] <p>\u200b\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6807\u7b7e\u200b\\(y_{ij}\\)\u200b\u975e\u200b1(\u200b\u6b63\u200bpositive)\u200b\u5373\u200b0 (\u200b\u8d1f\u200bnegative)\uff0c\\(\\sigma(\\cdot)\\)\u200b\u5c31\u662f\u200bsigmoid\u200b\u51fd\u6570\u200b\u3002\u200b\u4e0a\u9762\u200b\u7684\u200b\u516c\u5f0f\u200b\u770b\u8d77\u6765\u200b\u5413\u4eba\u200b\uff0c\u200b\u4f46\u662f\u200b\\(f\\)\u200b\u7684\u200b\u68af\u5ea6\u200b\u5b9e\u9645\u4e0a\u200b\u975e\u5e38\u7b80\u5355\u200b\uff1a\\(\\partial{L_i} / \\partial{f_j} = \\sigma(f_j) - y_{ij}\\) \uff08\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u6c42\u5bfc\u200b\u6765\u200b\u9a8c\u8bc1\u200b\uff09\u3002</p> <p>\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\u662f\u200b\u9884\u6d4b\u200b\u5b9e\u6570\u200b\u7684\u200b\u503c\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u6bd4\u5982\u200b\u9884\u6d4b\u200b\u623f\u4ef7\u200b\uff0c\u200b\u9884\u6d4b\u200b\u56fe\u7247\u200b\u4e2d\u200b\u67d0\u4e2a\u200b\u4e1c\u897f\u200b\u7684\u200b\u957f\u5ea6\u200b\u7b49\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u8fd9\u79cd\u200b\u95ee\u9898\u200b\uff0c\u200b\u901a\u5e38\u200b\u662f\u200b\u8ba1\u7b97\u200b\u9884\u6d4b\u503c\u200b\u548c\u200b\u771f\u5b9e\u200b\u503c\u200b\u4e4b\u95f4\u200b\u7684\u200b\u635f\u5931\u200b\u3002\u200b\u7136\u540e\u200b\u7528\u200bL2\u200b\u5e73\u65b9\u200b\u8303\u5f0f\u200b\u6216\u200bL1\u200b\u8303\u5f0f\u200b\u5ea6\u91cf\u200b\u5dee\u5f02\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u67d0\u4e2a\u200b\u6837\u672c\u200b\uff0c</p> \\[ L_i = \\Vert f - y_i \\Vert_2^2 \\] <p>\u200b\u4e4b\u6240\u4ee5\u200bL2\u200b\u8303\u6570\u200b\u5728\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e2d\u8981\u200b\u8fdb\u884c\u200b\u5e73\u65b9\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u68af\u5ea6\u200b\u7b97\u200b\u8d77\u6765\u200b\u66f4\u52a0\u200b\u7b80\u5355\u200b\u3002\u200b\u56e0\u4e3a\u200b\u5e73\u65b9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u8c03\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e0d\u7528\u200b\u6539\u53d8\u200b\u6700\u4f18\u200b\u53c2\u6570\u200b\u3002L1\u200b\u8303\u5f0f\u200b\u5219\u200b\u662f\u200b\u8981\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u7edd\u5bf9\u503c\u200b\u52a0\u200b\u8d77\u6765\u200b\uff1a</p> \\[ L_i = \\Vert f - y_i \\Vert_1 = \\sum_j \\mid f_j - (y_i)_j \\mid \\] <p>\u200b\u5728\u200b\u4e0a\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u591a\u4e2a\u200b\u6570\u91cf\u200b\u88ab\u200b\u9884\u6d4b\u200b\u4e86\u200b\uff0c\u200b\u5c31\u8981\u200b\u5bf9\u200b\u9884\u6d4b\u200b\u7684\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u9884\u6d4b\u200b\u6c42\u548c\u200b\uff0c\u200b\u5373\u200b\\(\\sum_j\\)\u3002\u200b\u89c2\u5bdf\u200b\u7b2c\u200bi\u200b\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u7b2c\u200bj\u200b\u7ef4\u200b\uff0c\u200b\u7528\u200b\\(\\delta_{ij}\\)\u200b\u8868\u793a\u200b\u9884\u6d4b\u503c\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u503c\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002\u200b\u5173\u4e8e\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u4e5f\u200b\u5c31\u662f\u200b\\(\\partial{L_i} / \\partial{f_j}\\)\uff09\u200b\u80fd\u591f\u200b\u8f7b\u677e\u200b\u5730\u200b\u901a\u8fc7\u200b\u88ab\u200b\u6c42\u5bfc\u200b\u4e3a\u200bL2\u200b\u8303\u5f0f\u200b\u7684\u200b\\(\\delta_{ij}\\)\u200b\u6216\u200b\\(sign(\\delta_{ij})\\)\u3002\u200b\u8fd9\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u8bc4\u5206\u200b\u503c\u200b\u7684\u200b\u68af\u5ea6\u200b\u8981\u4e48\u200b\u4e0e\u200b\u8bef\u5dee\u200b\u4e2d\u200b\u7684\u200b\u5dee\u503c\u200b\u76f4\u63a5\u200b\u6210\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u8981\u4e48\u200b\u662f\u200b\u56fa\u5b9a\u200b\u7684\u200b\u5e76\u200b\u4ece\u200b\u5dee\u503c\u200b\u4e2d\u200b\u7ee7\u627f\u200bsign\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1aL2\u200b\u635f\u5931\u200b\u6bd4\u8d77\u200b\u8f83\u4e3a\u200b\u7a33\u5b9a\u200b\u7684\u200bSoftmax\u200b\u635f\u5931\u200b\u6765\u200b\uff0c\u200b\u5176\u200b\u6700\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u8981\u200b\u56f0\u96be\u200b\u5f88\u591a\u200b\u3002\u200b\u76f4\u89c2\u200b\u800c\u8a00\u200b\uff0c\u200b\u5b83\u200b\u9700\u8981\u200b\u7f51\u7edc\u200b\u5177\u5907\u200b\u4e00\u4e2a\u200b\u7279\u522b\u200b\u7684\u200b\u6027\u8d28\u200b\uff0c\u200b\u5373\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200b\u8f93\u5165\u200b\uff08\u200b\u548c\u200b\u589e\u91cf\u200b\uff09\u200b\u90fd\u200b\u8981\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200b\u786e\u5207\u200b\u7684\u200b\u6b63\u786e\u200b\u503c\u200b\u3002\u200b\u800c\u200b\u5728\u200bSoftmax\u200b\u4e2d\u200b\u5c31\u200b\u4e0d\u662f\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u8bc4\u5206\u200b\u7684\u200b\u51c6\u786e\u200b\u503c\u200b\u5e76\u200b\u4e0d\u662f\u200b\u90a3\u4e48\u200b\u91cd\u8981\u200b\uff1a\u200b\u53ea\u6709\u200b\u5f53\u200b\u5b83\u4eec\u200b\u91cf\u7ea7\u200b\u9002\u5f53\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u624d\u200b\u6709\u200b\u610f\u4e49\u200b\u3002\u200b\u8fd8\u6709\u200b\uff0cL2\u200b\u635f\u5931\u200b\u9c81\u68d2\u6027\u200b\u4e0d\u597d\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5f02\u5e38\u200b\u503c\u200b\u53ef\u4ee5\u200b\u5bfc\u81f4\u200b\u5f88\u5927\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u6240\u4ee5\u200b\u5728\u200b\u9762\u5bf9\u200b\u4e00\u4e2a\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\u65f6\u200b\uff0c\u200b\u5148\u200b\u8003\u8651\u200b\u5c06\u200b\u8f93\u51fa\u200b\u53d8\u6210\u200b\u4e8c\u503c\u5316\u200b\u662f\u5426\u200b\u771f\u7684\u200b\u4e0d\u591f\u200b\u7528\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u5bf9\u200b\u4e00\u4e2a\u200b\u4ea7\u54c1\u200b\u7684\u200b\u661f\u7ea7\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4f7f\u7528\u200b5\u200b\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u6765\u200b\u5bf9\u200b1-5\u200b\u661f\u200b\u8fdb\u884c\u200b\u6253\u5206\u200b\u7684\u200b\u6548\u679c\u200b\u4e00\u822c\u200b\u6bd4\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u56de\u5f52\u200b\u635f\u5931\u200b\u8981\u200b\u597d\u200b\u5f88\u591a\u200b\u3002\u200b\u5206\u7c7b\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u4f18\u70b9\u200b\uff0c\u200b\u5c31\u662f\u200b\u80fd\u200b\u7ed9\u51fa\u200b\u5173\u4e8e\u200b\u56de\u5f52\u200b\u7684\u200b\u8f93\u51fa\u200b\u7684\u200b\u5206\u5e03\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u6beb\u65e0\u628a\u63e1\u200b\u7684\u200b\u8f93\u51fa\u200b\u503c\u200b\u3002\u200b\u5982\u679c\u200b\u786e\u4fe1\u200b\u5206\u7c7b\u200b\u4e0d\u200b\u9002\u7528\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4f7f\u7528\u200bL2\u200b\u635f\u5931\u200b\u5427\u200b\uff0c\u200b\u4f46\u662f\u200b\u4e00\u5b9a\u200b\u8981\u200b\u8c28\u614e\u200b\uff1aL2\u200b\u975e\u5e38\u200b\u8106\u5f31\u200b\uff0c\u200b\u5728\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff08\u200b\u5c24\u5176\u200b\u662f\u200b\u5728\u200bL2\u200b\u635f\u5931\u200b\u5c42\u200b\u7684\u200b\u4e0a\u200b\u4e00\u5c42\u200b\uff09\u200b\u4e0d\u662f\u200b\u597d\u200b\u4e3b\u610f\u200b\u3002</p> <p>\u200b\u5f53\u200b\u9762\u5bf9\u200b\u4e00\u4e2a\u200b\u56de\u5f52\u200b\u4efb\u52a1\u200b\uff0c\u200b\u9996\u5148\u200b\u8003\u8651\u200b\u662f\u4e0d\u662f\u200b\u5fc5\u987b\u200b\u8fd9\u6837\u200b\u3002\u200b\u4e00\u822c\u800c\u8a00\u200b\uff0c\u200b\u5c3d\u91cf\u200b\u628a\u200b\u4f60\u200b\u7684\u200b\u8f93\u51fa\u200b\u53d8\u6210\u200b\u4e8c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u4ece\u800c\u200b\u53d8\u6210\u200b\u4e00\u4e2a\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u7ed3\u6784\u5316\u200b\u9884\u6d4b\u200b\uff08structured prediction\uff09\u200b\u7ed3\u6784\u5316\u200b\u635f\u5931\u200b\u662f\u200b\u6307\u200b\u6807\u7b7e\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\u7ed3\u6784\u200b\uff0c\u200b\u4f8b\u5982\u200b\u56fe\u8868\u200b\u3001\u200b\u6811\u200b\u6216\u8005\u200b\u5176\u4ed6\u200b\u590d\u6742\u200b\u7269\u4f53\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002\u200b\u901a\u5e38\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u8fd8\u4f1a\u200b\u5047\u8bbe\u200b\u7ed3\u6784\u200b\u7a7a\u95f4\u200b\u975e\u5e38\u200b\u5de8\u5927\u200b\uff0c\u200b\u4e0d\u200b\u5bb9\u6613\u200b\u8fdb\u884c\u200b\u904d\u5386\u200b\u3002\u200b\u7ed3\u6784\u5316\u200bSVM\u200b\u80cc\u540e\u200b\u7684\u200b\u57fa\u672c\u200b\u601d\u60f3\u200b\u5c31\u662f\u200b\u5728\u200b\u6b63\u786e\u200b\u7684\u200b\u7ed3\u6784\u200b\\(y_i\\)\u200b\u548c\u200b\u5f97\u5206\u200b\u6700\u9ad8\u200b\u7684\u200b\u975e\u200b\u6b63\u786e\u200b\u7ed3\u6784\u200b\u4e4b\u95f4\u200b\u753b\u51fa\u200b\u4e00\u4e2a\u200b\u8fb9\u754c\u200b\u3002\u200b\u89e3\u51b3\u200b\u8fd9\u200b\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u5e76\u200b\u4e0d\u662f\u200b\u50cf\u200b\u89e3\u51b3\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u65e0\u200b\u9650\u5236\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u95ee\u9898\u200b\u90a3\u6837\u200b\u4f7f\u7528\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e86\u200b\uff0c\u200b\u800c\u662f\u200b\u9700\u8981\u200b\u8bbe\u8ba1\u200b\u4e00\u4e9b\u200b\u7279\u6b8a\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u6709\u6548\u200b\u5229\u7528\u200b\u5bf9\u4e8e\u200b\u7ed3\u6784\u200b\u7a7a\u95f4\u200b\u7684\u200b\u7279\u6b8a\u200b\u7b80\u5316\u200b\u5047\u8bbe\u200b\u3002\u200b\u6211\u4eec\u200b\u7b80\u8981\u5730\u200b\u63d0\u200b\u4e00\u4e0b\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f46\u662f\u200b\u8be6\u7ec6\u200b\u5185\u5bb9\u200b\u5c31\u200b\u8d85\u51fa\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u8303\u56f4\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-2/#_5","title":"\u5c0f\u7ed3","text":"<p>\u200b\u5c0f\u7ed3\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u63a8\u8350\u200b\u7684\u200b\u9884\u5904\u7406\u200b\u64cd\u4f5c\u200b\u662f\u200b\u5bf9\u200b\u6570\u636e\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7279\u5f81\u200b\u90fd\u200b\u8fdb\u884c\u200b\u96f6\u200b\u4e2d\u5fc3\u5316\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5176\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u90fd\u200b\u5f52\u4e00\u5316\u200b\u5230\u200b[-1,1]\u200b\u8303\u56f4\u200b\u4e4b\u5185\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6807\u51c6\u5dee\u200b\u4e3a\u200b\\(\\sqrt{2/n}\\)\u200b\u7684\u200b\u9ad8\u65af\u5206\u5e03\u200b\u6765\u200b\u521d\u59cb\u5316\u200b\u6743\u91cd\u200b\uff0c\u200b\u5176\u4e2d\u200b\\(n\\)\u200b\u662f\u200b\u8f93\u5165\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u6570\u200b\u3002\u200b\u4f8b\u5982\u200b\u7528\u200bnumpy\u200b\u53ef\u4ee5\u200b\u5199\u4f5c\u200b\uff1a\u3002<code>w = np.random.randn(n) * sqrt(2.0/n)</code></li> <li>\u200b\u4f7f\u7528\u200bL2\u200b\u6b63\u5219\u200b\u5316\u200b\u548c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u5012\u7f6e\u200b\u7248\u672c\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6279\u91cf\u200b\u5f52\u4e00\u5316\u200b\u3002</li> <li>\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u53ef\u80fd\u200b\u8981\u200b\u9762\u5bf9\u200b\u7684\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6bcf\u4e2a\u200b\u4efb\u52a1\u200b\u5bf9\u5e94\u200b\u7684\u200b\u5e38\u7528\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</li> </ul> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u9884\u5904\u7406\u200b\u4e86\u200b\u6570\u636e\u200b\uff0c\u200b\u521d\u59cb\u5316\u200b\u4e86\u200b\u6a21\u578b\u200b\u3002\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8ba8\u8bba\u200b\u7b97\u6cd5\u200b\u7684\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\u53ca\u5176\u200b\u8fd0\u4f5c\u200b\u7279\u6027\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/","title":"\u5b66\u4e60\u200b\u548c\u200b\u8bc4\u4f30","text":""},{"location":"courses/neural_network/neural-networks-3/#_1","title":"\u5b66\u4e60\u200b\u8fc7\u7a0b","text":"<p>\u200b\u5728\u200b\u524d\u9762\u200b\u7ae0\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u9759\u6001\u200b\u90e8\u5206\u200b\uff1a\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u7f51\u7edc\u200b\u7684\u200b\u8fde\u63a5\u200b\u3001\u200b\u6570\u636e\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u672c\u8282\u200b\u5c06\u200b\u81f4\u529b\u4e8e\u200b\u8bb2\u89e3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u52a8\u6001\u200b\u90e8\u5206\u200b\uff0c\u200b\u5373\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\u548c\u200b\u641c\u7d22\u200b\u6700\u4f18\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_2","title":"\u68af\u5ea6\u200b\u68c0\u67e5","text":"<p>\u200b\u7406\u8bba\u200b\u4e0a\u200b\u8bb2\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u5f88\u200b\u7b80\u5355\u200b\uff0c\u200b\u5c31\u662f\u200b\u7b80\u5355\u200b\u5730\u200b\u628a\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u548c\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002\u200b\u7136\u800c\u200b\u4ece\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u5c42\u9762\u200b\u4e0a\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u66f4\u52a0\u200b\u590d\u6742\u200b\u4e14\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e9b\u200b\u63d0\u793a\u200b\u3001\u200b\u6280\u5de7\u200b\u548c\u200b\u9700\u8981\u200b\u4ed4\u7ec6\u200b\u6ce8\u610f\u200b\u7684\u200b\u4e8b\u60c5\u200b\uff1a</p> <p>\u200b\u4f7f\u7528\u200b\u4e2d\u5fc3\u5316\u200b\u516c\u5f0f\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u6709\u9650\u200b\u5dee\u503c\u200b\u8fd1\u4f3c\u200b\u6765\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u516c\u5f0f\u200b\u662f\u200b\uff1a</p> \\[ \\frac{df(x)}{dx} = \\frac{f(x + h) - f(x)}{h} \\hspace{0.1in} \\text{(bad, do not use)} \\] <p>\u200b\u5176\u4e2d\u200b\\(h\\)\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u5c0f\u200b\u7684\u200b\u6570\u5b57\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u8fd1\u4f3c\u200b\u4e3a\u200b1e-5\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u8bc1\u660e\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e2d\u5fc3\u5316\u200b\u516c\u5f0f\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\uff1a</p> \\[ \\frac{df(x)}{dx} = \\frac{f(x + h) - f(x - h)}{2h} \\hspace{0.1in} \\text{(use instead)} \\] <p>\u200b\u8be5\u200b\u516c\u5f0f\u200b\u5728\u200b\u68c0\u67e5\u200b\u68af\u5ea6\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f1a\u200b\u8981\u6c42\u200b\u8ba1\u7b97\u200b\u4e24\u6b21\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u6240\u4ee5\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u7684\u200b\u8017\u8d39\u200b\u4e5f\u200b\u662f\u200b\u4e24\u500d\u200b\uff09\uff0c\u200b\u4f46\u662f\u200b\u68af\u5ea6\u200b\u7684\u200b\u8fd1\u4f3c\u503c\u200b\u4f1a\u200b\u51c6\u786e\u200b\u5f88\u591a\u200b\u3002\u200b\u8981\u200b\u7406\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5bf9\u200b\\(f(x+h)\\)\u200b\u548c\u200b\\(f(x-h)\\)\u200b\u4f7f\u7528\u200b\u6cf0\u52d2\u200b\u5c55\u5f00\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u7b2c\u4e00\u4e2a\u200b\u516c\u5f0f\u200b\u7684\u200b\u8bef\u5dee\u200b\u8fd1\u4f3c\u200b\\(O(h)\\)\uff0c\u200b\u7b2c\u4e8c\u4e2a\u200b\u516c\u5f0f\u200b\u7684\u200b\u8bef\u5dee\u200b\u8fd1\u4f3c\u200b\\(O(h^2)\\)\uff08\u200b\u662f\u200b\u4e2a\u200b\u4e8c\u9636\u200b\u8fd1\u4f3c\u200b\uff09\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u6765\u200b\u6bd4\u8f83\u200b\u3002\u200b\u6bd4\u8f83\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\\(f'_n\\)\u200b\u548c\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\\(f'_a\\)\u200b\u7684\u200b\u7ec6\u8282\u200b\u6709\u200b\u54ea\u4e9b\u200b\uff1f\u200b\u5982\u4f55\u200b\u5f97\u77e5\u200b\u6b64\u200b\u4e24\u8005\u200b\u4e0d\u200b\u5339\u914d\u200b\uff1f\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u503e\u5411\u200b\u4e8e\u200b\u76d1\u6d4b\u200b\u5b83\u4eec\u200b\u7684\u200b\u5dee\u200b\u7684\u200b\u7edd\u5bf9\u503c\u200b\\(\\mid f'_a - f'_n \\mid\\)\u200b\u6216\u8005\u200b\u5dee\u200b\u7684\u200b\u5e73\u65b9\u200b\u503c\u200b\uff0c\u200b\u7136\u540e\u200b\u5b9a\u4e49\u200b\u8be5\u503c\u200b\u5982\u679c\u200b\u8d85\u8fc7\u200b\u67d0\u4e2a\u200b\u89c4\u5b9a\u200b\u9608\u503c\u200b\uff0c\u200b\u5c31\u200b\u5224\u65ad\u200b\u68af\u5ea6\u200b\u5b9e\u73b0\u200b\u5931\u8d25\u200b\u3002\u200b\u7136\u800c\u200b\u8be5\u200b\u601d\u8def\u200b\u662f\u200b\u6709\u200b\u95ee\u9898\u200b\u7684\u200b\u3002\u200b\u60f3\u60f3\u200b\uff0c\u200b\u5047\u8bbe\u200b\u8fd9\u4e2a\u200b\u5dee\u503c\u200b\u662f\u200b1e-4\uff0c\u200b\u5982\u679c\u200b\u4e24\u4e2a\u200b\u68af\u5ea6\u200b\u503c\u200b\u5728\u200b1.0\u200b\u5de6\u53f3\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5dee\u503c\u200b\u770b\u8d77\u6765\u200b\u5c31\u200b\u5f88\u200b\u5408\u9002\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u4e24\u4e2a\u200b\u68af\u5ea6\u200b\u662f\u200b\u5339\u914d\u200b\u7684\u200b\u3002\u200b\u7136\u800c\u200b\u5982\u679c\u200b\u68af\u5ea6\u200b\u503c\u200b\u662f\u200b1e-5\u200b\u6216\u8005\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u90a3\u4e48\u200b1e-4\u200b\u5c31\u662f\u200b\u975e\u5e38\u200b\u5927\u200b\u7684\u200b\u5dee\u8ddd\u200b\uff0c\u200b\u68af\u5ea6\u200b\u5b9e\u73b0\u200b\u80af\u5b9a\u200b\u5c31\u662f\u200b\u5931\u8d25\u200b\u7684\u200b\u4e86\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4f7f\u7528\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u603b\u662f\u200b\u66f4\u200b\u5408\u9002\u200b\u4e00\u4e9b\u200b\uff1a</p> \\[ \\frac{\\mid f'_a - f'_n \\mid}{\\max(\\mid f'_a \\mid, \\mid f'_n \\mid)} \\] <p>\u200b\u4e0a\u200b\u5f0f\u200b\u8003\u8651\u200b\u4e86\u200b\u5dee\u503c\u200b\u5360\u200b\u4e24\u4e2a\u200b\u68af\u5ea6\u200b\u7edd\u5bf9\u503c\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u3002\u200b\u6ce8\u610f\u200b\u901a\u5e38\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u516c\u5f0f\u200b\u53ea\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u5f0f\u5b50\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\uff08\u200b\u4efb\u610f\u200b\u4e00\u4e2a\u200b\u5747\u200b\u53ef\u200b\uff09\uff0c\u200b\u4f46\u662f\u200b\u6211\u200b\u66f4\u200b\u503e\u5411\u200b\u53d6\u200b\u4e24\u4e2a\u200b\u5f0f\u5b50\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u6216\u8005\u200b\u53d6\u200b\u4e24\u4e2a\u200b\u5f0f\u5b50\u200b\u7684\u200b\u548c\u200b\u3002\u200b\u8fd9\u6837\u200b\u505a\u200b\u662f\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u5728\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u5f0f\u5b50\u200b\u4e3a\u200b0\u200b\u65f6\u200b\uff0c\u200b\u516c\u5f0f\u200b\u5206\u6bcd\u200b\u4e3a\u200b0\uff08\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5728\u200bReLU\u200b\u4e2d\u662f\u200b\u7ecf\u5e38\u200b\u53d1\u751f\u200b\u7684\u200b\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd8\u200b\u5fc5\u987b\u200b\u6ce8\u610f\u200b\u4e24\u4e2a\u200b\u5f0f\u5b50\u200b\u90fd\u200b\u4e3a\u200b\u96f6\u4e14\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff1a</p> <ul> <li>\u200b\u76f8\u5bf9\u8bef\u5dee\u200b&gt;1e-2\uff1a\u200b\u901a\u5e38\u200b\u5c31\u200b\u610f\u5473\u7740\u200b\u68af\u5ea6\u200b\u53ef\u80fd\u200b\u51fa\u9519\u200b\u3002</li> <li>1e-2&gt;\u200b\u76f8\u5bf9\u8bef\u5dee\u200b&gt;1e-4\uff1a\u200b\u8981\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u611f\u5230\u200b\u4e0d\u200b\u8212\u670d\u200b\u624d\u884c\u200b\u3002</li> <li>1e-4&gt;\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\uff1a\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u7684\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u5bf9\u4e8e\u200b\u6709\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u7684\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u662f\u200bOK\u200b\u7684\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e2d\u200b\u6ca1\u6709\u200bkink\uff08\u200b\u4f7f\u7528\u200btanh\u200b\u548c\u200bsoftmax\uff09\uff0c\u200b\u90a3\u4e48\u200b\u76f8\u5bf9\u200b\u8bef\u5dee\u503c\u200b1e-4\u200b\u8fd8\u662f\u200b\u592a\u9ad8\u200b\u3002</li> <li>1e-7\u200b\u6216\u8005\u200b\u66f4\u200b\u5c0f\u200b\uff1a\u200b\u597d\u200b\u7ed3\u679c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u9ad8\u5174\u200b\u4e00\u628a\u200b\u4e86\u200b\u3002</li> </ul> <p>\u200b\u8981\u200b\u77e5\u9053\u200b\u7684\u200b\u662f\u200b\u7f51\u7edc\u200b\u7684\u200b\u6df1\u5ea6\u200b\u8d8a\u6df1\u200b\uff0c\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u5c31\u200b\u8d8a\u200b\u9ad8\u200b\u3002\u200b\u6240\u4ee5\u200b\u5982\u679c\u200b\u4f60\u200b\u662f\u200b\u5728\u200b\u5bf9\u200b\u4e00\u4e2a\u200b10\u200b\u5c42\u200b\u7f51\u7edc\u200b\u7684\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u505a\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\uff0c\u200b\u90a3\u4e48\u200b1e-2\u200b\u7684\u200b\u76f8\u5bf9\u200b\u8bef\u5dee\u503c\u200b\u53ef\u80fd\u200b\u5c31\u200bOK\u200b\u4e86\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8bef\u5dee\u200b\u4e00\u76f4\u200b\u5728\u200b\u7d2f\u79ef\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u53ef\u5fae\u200b\u51fd\u6570\u200b\u7684\u200b\u76f8\u5bf9\u200b\u8bef\u5dee\u503c\u200b\u662f\u200b1e-2\uff0c\u200b\u90a3\u4e48\u200b\u901a\u5e38\u200b\u8bf4\u660e\u200b\u68af\u5ea6\u200b\u5b9e\u73b0\u200b\u4e0d\u200b\u6b63\u786e\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u53cc\u200b\u7cbe\u5ea6\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u9519\u8bef\u200b\u662f\u200b\u4f7f\u7528\u200b\u5355\u7cbe\u5ea6\u200b\u6d6e\u70b9\u6570\u200b\u6765\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u3002\u200b\u8fd9\u6837\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u5373\u4f7f\u200b\u68af\u5ea6\u200b\u5b9e\u73b0\u200b\u6b63\u786e\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u8bef\u5dee\u503c\u200b\u4e5f\u200b\u4f1a\u200b\u5f88\u200b\u9ad8\u200b\uff08\u200b\u6bd4\u5982\u200b1e-2\uff09\u3002\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u7ecf\u9a8c\u200b\u800c\u8a00\u200b\uff0c\u200b\u51fa\u73b0\u200b\u8fc7\u200b\u4f7f\u7528\u200b\u5355\u7cbe\u5ea6\u200b\u6d6e\u70b9\u6570\u200b\u65f6\u200b\u76f8\u5bf9\u8bef\u5dee\u200b\u4e3a\u200b1e-2\uff0c\u200b\u6362\u6210\u200b\u53cc\u200b\u7cbe\u5ea6\u200b\u6d6e\u70b9\u6570\u200b\u65f6\u200b\u5c31\u200b\u964d\u4f4e\u200b\u4e3a\u200b1e-8\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u4fdd\u6301\u200b\u5728\u200b\u6d6e\u70b9\u6570\u200b\u7684\u200b\u6709\u6548\u200b\u8303\u56f4\u200b\u3002\u200b\u5efa\u8bae\u200b\u901a\u8bfb\u200b\u300a\u200b\u6bcf\u200b\u4e00\u4f4d\u200b\u8ba1\u7b97\u673a\u200b\u79d1\u5b66\u5bb6\u200b\u90fd\u200b\u5e94\u8be5\u200b\u4e86\u89e3\u200b\u6d6e\u70b9\u8fd0\u7b97\u200b\u300b\u200b\u4e00\u6587\u200b\uff0c\u200b\u8be5\u6587\u200b\u5c06\u200b\u9610\u660e\u200b\u4f60\u200b\u53ef\u80fd\u200b\u72af\u200b\u7684\u200b\u9519\u8bef\u200b\uff0c\u200b\u4fc3\u4f7f\u200b\u4f60\u200b\u5199\u4e0b\u200b\u66f4\u52a0\u200b\u7ec6\u5fc3\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u5728\u200b\u4e00\u4e2a\u200b\u6279\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u5bf9\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u662f\u200b\u5f88\u200b\u5e38\u89c1\u200b\u7684\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u70b9\u200b\u7684\u200b\u68af\u5ea6\u200b\u5f88\u5c0f\u200b\uff0c\u200b\u7136\u540e\u200b\u53c8\u200b\u7528\u200b\u6570\u636e\u200b\u70b9\u200b\u7684\u200b\u6570\u91cf\u200b\u53bb\u9664\u200b\uff0c\u200b\u5c31\u200b\u4f7f\u5f97\u200b\u6570\u503c\u200b\u66f4\u200b\u5c0f\u200b\uff0c\u200b\u8fd9\u200b\u53cd\u8fc7\u6765\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u503c\u200b\u95ee\u9898\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u200b\u4e3a\u4ec0\u4e48\u200b\u603b\u662f\u200b\u4f1a\u200b\u628a\u200b\u539f\u59cb\u200b\u7684\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u548c\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u6570\u636e\u200b\u6253\u5370\u200b\u51fa\u6765\u200b\uff0c\u200b\u786e\u4fdd\u200b\u7528\u6765\u200b\u6bd4\u8f83\u200b\u7684\u200b\u6570\u5b57\u200b\u7684\u200b\u503c\u200b\u4e0d\u662f\u200b\u8fc7\u200b\u5c0f\u200b\uff08\u200b\u901a\u5e38\u200b\u7edd\u5bf9\u503c\u200b\u5c0f\u4e8e\u200b1e-10\u200b\u5c31\u200b\u7edd\u5bf9\u200b\u8ba9\u200b\u4eba\u200b\u62c5\u5fc3\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u786e\u5b9e\u200b\u8fc7\u200b\u5c0f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u5e38\u6570\u200b\u6682\u65f6\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u6570\u503c\u200b\u8303\u56f4\u200b\u6269\u5c55\u200b\u5230\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u201c\u200b\u597d\u200b\u201d\u200b\u7684\u200b\u8303\u56f4\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u8303\u56f4\u200b\u4e2d\u200b\u6d6e\u70b9\u6570\u200b\u53d8\u5f97\u200b\u66f4\u52a0\u200b\u81f4\u5bc6\u200b\u3002\u200b\u6bd4\u8f83\u200b\u7406\u60f3\u200b\u7684\u200b\u662f\u200b1.0\u200b\u7684\u200b\u6570\u91cf\u7ea7\u200b\u4e0a\u200b\uff0c\u200b\u5373\u5f53\u200b\u6d6e\u70b9\u6570\u200b\u6307\u6570\u200b\u4e3a\u200b0\u200b\u65f6\u200b\u3002</p> <p>\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u7684\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\uff08kinks\uff09\u3002\u200b\u5728\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u65f6\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5bfc\u81f4\u200b\u4e0d\u200b\u51c6\u786e\u200b\u7684\u200b\u539f\u56e0\u200b\u662f\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u95ee\u9898\u200b\u3002\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u662f\u200b\u6307\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e0d\u53ef\u200b\u5bfc\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u7531\u200bReLU\uff08\\(max(0,x)\\)\uff09\u200b\u7b49\u200b\u51fd\u6570\u200b\uff0c\u200b\u6216\u200bSVM\u200b\u635f\u5931\u200b\uff0cMaxout\u200b\u795e\u7ecf\u5143\u200b\u7b49\u200b\u5f15\u5165\u200b\u3002\u200b\u8003\u8651\u200b\u5f53\u200b\\(x = -1e6\\)\u200b\u7684\u200b\u65f6\u200b\uff0c\u200b\u5bf9\u200bReLU\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u3002\u200b\u56e0\u4e3a\u200b\\(x &lt; 0\\)\uff0c\u200b\u6240\u4ee5\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u5728\u200b\u8be5\u200b\u70b9\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e3a\u200b0\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u4f1a\u200b\u7a81\u7136\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u4e00\u4e2a\u200b\u975e\u96f6\u200b\u7684\u200b\u68af\u5ea6\u200b\u503c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\\(f(x+h)\\)\u200b\u53ef\u80fd\u200b\u8d8a\u8fc7\u200b\u4e86\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b(\u200b\u4f8b\u5982\u200b\uff1a\u200b\u5982\u679c\u200b\\(h &gt; 1e-6\\))\uff0c\u200b\u5bfc\u81f4\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u975e\u96f6\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba4\u4e3a\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u6781\u7aef\u200b\u7684\u200b\u6848\u4f8b\u200b\uff0c\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u5f88\u200b\u5e38\u89c1\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u7528\u200bCIFAR-10\u200b\u8bad\u7ec3\u200b\u7684\u200bSVM\u200b\u4e2d\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6709\u200b50,000\u200b\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u4e14\u200b\u6839\u636e\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u4ea7\u751f\u200b9\u200b\u4e2a\u200b\u5f0f\u5b50\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5305\u542b\u200b\u6709\u200b450,000\u200b\u4e2a\u200b\\(max(0,x)\\)\u200b\u5f0f\u5b50\u200b\u3002\u200b\u800c\u200b\u4e00\u4e2a\u200b\u7528\u200bSVM\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u56e0\u4e3a\u200b\u91c7\u7528\u200b\u4e86\u200bReLU\uff0c\u200b\u8fd8\u4f1a\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u662f\u200b\u53ef\u4ee5\u200b\u77e5\u9053\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u6709\u6ca1\u6709\u200b\u88ab\u200b\u8d8a\u8fc7\u200b\u7684\u200b\u3002\u200b\u5728\u200b\u5177\u6709\u200b\\(max(x,y)\\);\u200b\u5f62\u5f0f\u200b\u7684\u200b\u51fd\u6570\u200b\u4e2d\u200b\u6301\u7eed\u200b\u8ddf\u8e2a\u200b\u6240\u6709\u200b\u201c\u200b\u8d62\u5bb6\u200b\u201d\u200b\u7684\u200b\u8eab\u4efd\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002\u200b\u5176\u5b9e\u200b\u5c31\u662f\u200b\u770b\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\uff0c\u200b\u5230\u5e95\u200bx\u200b\u548c\u200by\u200b\u8c01\u200b\u66f4\u200b\u5927\u200b\u3002\u200b\u5982\u679c\u200b\u5728\u200b\u8ba1\u7b97\u200b\\(f(x+h)\\)\u200b\u548c\u200b\\(f(x-h)\\)\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u81f3\u5c11\u200b\u6709\u200b\u4e00\u4e2a\u200b\u201c\u200b\u8d62\u5bb6\u200b\u201d\u200b\u7684\u200b\u8eab\u4efd\u200b\u53d8\u200b\u4e86\u200b\uff0c\u200b\u90a3\u200b\u5c31\u200b\u8bf4\u660e\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u88ab\u200b\u8d8a\u8fc7\u200b\u4e86\u200b\uff0c\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u4f1a\u200b\u4e0d\u200b\u51c6\u786e\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u70b9\u200b\u3002\u200b\u89e3\u51b3\u200b\u4e0a\u9762\u200b\u7684\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u95ee\u9898\u200b\u7684\u200b\u4e00\u4e2a\u200b\u529e\u6cd5\u200b\u662f\u200b\u4f7f\u7528\u200b\u66f4\u5c11\u200b\u7684\u200b\u6570\u636e\u200b\u70b9\u200b\u3002\u200b\u56e0\u4e3a\u200b\u542b\u6709\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b(\u200b\u4f8b\u5982\u200b\uff1a\u200b\u56e0\u4e3a\u200b\u4f7f\u7528\u200b\u4e86\u200bReLU\u200b\u6216\u8005\u200b\u8fb9\u7f18\u200b\u635f\u5931\u200b\u7b49\u200b\u51fd\u6570\u200b)\u200b\u7684\u200b\u6570\u636e\u200b\u70b9\u200b\u8d8a\u5c11\u200b\uff0c\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u5c31\u200b\u8d8a\u5c11\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b\u8ba1\u7b97\u200b\u6709\u9650\u200b\u5dee\u503c\u200b\u8fd1\u4f3c\u200b\u65f6\u200b\u8d8a\u8fc7\u200b\u4e0d\u53ef\u200b\u5bfc\u70b9\u200b\u7684\u200b\u51e0\u7387\u200b\u5c31\u200b\u8d8a\u200b\u5c0f\u200b\u3002\u200b\u8fd8\u6709\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u5bf9\u200b2-3\u200b\u4e2a\u200b\u6570\u636e\u200b\u70b9\u200b\u90fd\u200b\u6709\u6548\u200b\uff0c\u200b\u90a3\u4e48\u200b\u57fa\u672c\u4e0a\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u6279\u91cf\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u4e5f\u200b\u662f\u200b\u6ca1\u200b\u95ee\u9898\u200b\u7684\u200b\u3002\u200b\u6240\u4ee5\u200b\u4f7f\u7528\u200b\u5f88\u200b\u5c11\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\u70b9\u200b\uff0c\u200b\u80fd\u200b\u8ba9\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u66f4\u200b\u8fc5\u901f\u200b\u9ad8\u6548\u200b\u3002</p> <p>\u200b\u8c28\u614e\u200b\u8bbe\u7f6e\u200b\u6b65\u957f\u200bh\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200bh\u200b\u5e76\u200b\u4e0d\u662f\u200b\u8d8a\u5c0f\u8d8a\u200b\u597d\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5f53\u200b\u210e\u200b\u7279\u522b\u200b\u5c0f\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5c31\u200b\u53ef\u80fd\u200b\u5c31\u200b\u4f1a\u200b\u9047\u5230\u200b\u6570\u503c\u200b\u7cbe\u5ea6\u200b\u95ee\u9898\u200b\u3002\u200b\u6709\u65f6\u5019\u200b\u5982\u679c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u65e0\u6cd5\u200b\u8fdb\u884c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8bd5\u8bd5\u200b\u5c06\u200b\u210e\u200b\u8c03\u200b\u5230\u200b1e-4\u200b\u6216\u8005\u200b1e-6\uff0c\u200b\u7136\u540e\u200b\u7a81\u7136\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u53ef\u80fd\u200b\u5c31\u200b\u6062\u590d\u6b63\u5e38\u200b\u3002\u200b\u8fd9\u7bc7\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u6587\u7ae0\u200b\u4e2d\u6709\u200b\u4e00\u4e2a\u200b\u56fe\u8868\u200b\uff0c\u200b\u5176\u200bx\u200b\u8f74\u4e3a\u200b\u210e\u200b\u503c\u200b\uff0cy\u200b\u8f74\u4e3a\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u8bef\u5dee\u200b\u3002</p> <p>\u200b\u5728\u200b\u64cd\u4f5c\u200b\u7684\u200b\u7279\u6027\u200b\u6a21\u5f0f\u200b\u4e2d\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u3002\u200b\u6709\u200b\u4e00\u70b9\u200b\u5fc5\u987b\u200b\u8981\u200b\u8ba4\u8bc6\u200b\u5230\u200b\uff1a\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u662f\u200b\u5728\u200b\u53c2\u6570\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\uff08\u200b\u5f80\u5f80\u200b\u8fd8\u662f\u200b\u968f\u673a\u200b\u7684\u200b\uff09\u200b\u7684\u200b\u5355\u72ec\u200b\u70b9\u200b\u8fdb\u884c\u200b\u7684\u200b\u3002\u200b\u5373\u4f7f\u200b\u662f\u200b\u5728\u200b\u8be5\u200b\u70b9\u200b\u4e0a\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u6210\u529f\u200b\u4e86\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u80fd\u200b\u9a6c\u4e0a\u200b\u786e\u4fdd\u200b\u5168\u5c40\u200b\u4e0a\u200b\u68af\u5ea6\u200b\u7684\u200b\u5b9e\u73b0\u200b\u90fd\u200b\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\u3002\u200b\u8fd8\u6709\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u53ef\u80fd\u200b\u4e0d\u662f\u200b\u53c2\u6570\u200b\u7a7a\u95f4\u200b\u6700\u4f18\u200b\u4ee3\u8868\u6027\u200b\u7684\u200b\u70b9\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u8fdb\u5165\u200b\u67d0\u79cd\u200b\u75c5\u6001\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5373\u200b\u68af\u5ea6\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u6b63\u786e\u200b\u5b9e\u73b0\u200b\u4e86\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\u5e76\u200b\u6ca1\u6709\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0cSVM\u200b\u4f7f\u7528\u200b\u5c0f\u200b\u6570\u503c\u200b\u6743\u91cd\u200b\u521d\u59cb\u5316\u200b\uff0c\u200b\u5c31\u200b\u4f1a\u200b\u628a\u200b\u4e00\u4e9b\u200b\u63a5\u8fd1\u200b\u4e8e\u200b0\u200b\u7684\u200b\u5f97\u5206\u200b\u5206\u914d\u200b\u7ed9\u200b\u6240\u6709\u200b\u7684\u200b\u6570\u636e\u200b\u70b9\u200b\uff0c\u200b\u800c\u200b\u68af\u5ea6\u200b\u5c06\u4f1a\u200b\u5728\u200b\u6240\u6709\u200b\u7684\u200b\u6570\u636e\u200b\u70b9\u4e0a\u200b\u5c55\u73b0\u51fa\u200b\u67d0\u79cd\u200b\u6a21\u5f0f\u200b\u3002\u200b\u4e00\u4e2a\u200b\u4e0d\u200b\u6b63\u786e\u200b\u5b9e\u73b0\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e5f\u8bb8\u200b\u4f9d\u7136\u200b\u80fd\u591f\u200b\u4ea7\u751f\u200b\u51fa\u200b\u8fd9\u79cd\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u4f46\u662f\u200b\u4e0d\u80fd\u200b\u6cdb\u5316\u200b\u5230\u200b\u66f4\u5177\u200b\u4ee3\u8868\u6027\u200b\u7684\u200b\u64cd\u4f5c\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6bd4\u5982\u200b\u5728\u200b\u4e00\u4e9b\u200b\u7684\u200b\u5f97\u5206\u200b\u6bd4\u200b\u53e6\u200b\u4e00\u4e9b\u200b\u5f97\u5206\u200b\u66f4\u5927\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5c31\u200b\u4e0d\u884c\u200b\u3002\u200b\u56e0\u6b64\u200b\u4e3a\u4e86\u200b\u5b89\u5168\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6700\u597d\u200b\u8ba9\u200b\u7f51\u7edc\u200b\u5b66\u4e60\u200b\uff08\u201c\u200b\u9884\u70ed\u200b\u201d\uff09\u200b\u4e00\u5c0f\u200b\u6bb5\u65f6\u95f4\u200b\uff0c\u200b\u7b49\u5230\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5f00\u59cb\u200b\u4e0b\u964d\u200b\u7684\u200b\u4e4b\u540e\u200b\u518d\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u3002\u200b\u5728\u200b\u7b2c\u4e00\u6b21\u200b\u8fed\u4ee3\u200b\u5c31\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u7684\u200b\u5371\u9669\u200b\u5c31\u200b\u5728\u4e8e\u200b\uff0c\u200b\u6b64\u65f6\u200b\u53ef\u80fd\u200b\u6b63\u200b\u5904\u5728\u200b\u4e0d\u200b\u6b63\u5e38\u200b\u7684\u200b\u8fb9\u754c\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4ece\u800c\u200b\u63a9\u76d6\u200b\u4e86\u200b\u68af\u5ea6\u200b\u6ca1\u6709\u200b\u6b63\u786e\u200b\u5b9e\u73b0\u200b\u7684\u200b\u4e8b\u5b9e\u200b\u3002</p> <p>\u200b\u4e0d\u8981\u200b\u8ba9\u200b\u6b63\u5219\u200b\u5316\u200b\u541e\u6ca1\u200b\u6570\u636e\u200b\u3002\u200b\u901a\u5e38\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u662f\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u548c\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u7684\u200b\u548c\u200b\uff08\u200b\u4f8b\u5982\u200bL2\u200b\u5bf9\u200b\u6743\u91cd\u200b\u7684\u200b\u60e9\u7f5a\u200b\uff09\u3002\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u5371\u9669\u200b\u662f\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u53ef\u80fd\u200b\u541e\u6ca1\u200b\u6389\u200b\u6570\u636e\u200b\u635f\u5931\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u68af\u5ea6\u200b\u4e3b\u8981\u200b\u6765\u6e90\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\u90e8\u5206\u200b\uff08\u200b\u6b63\u5219\u200b\u5316\u200b\u90e8\u5206\u200b\u7684\u200b\u68af\u5ea6\u200b\u8868\u8fbe\u5f0f\u200b\u901a\u5e38\u200b\u7b80\u5355\u200b\u5f88\u591a\u200b\uff09\u3002\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4f1a\u200b\u63a9\u76d6\u200b\u6389\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u68af\u5ea6\u200b\u7684\u200b\u4e0d\u200b\u6b63\u786e\u200b\u5b9e\u73b0\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u63a8\u8350\u200b\u5148\u200b\u5173\u6389\u200b\u6b63\u5219\u200b\u5316\u5bf9\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u505a\u200b\u5355\u72ec\u200b\u68c0\u67e5\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u6b63\u5219\u200b\u5316\u505a\u200b\u5355\u72ec\u200b\u68c0\u67e5\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\u7684\u200b\u5355\u72ec\u200b\u68c0\u67e5\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4fee\u6539\u200b\u4ee3\u7801\u200b\uff0c\u200b\u53bb\u6389\u200b\u5176\u4e2d\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u63d0\u9ad8\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\uff0c\u200b\u786e\u8ba4\u200b\u5176\u200b\u6548\u679c\u200b\u5728\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u4e2d\u662f\u200b\u65e0\u6cd5\u200b\u5ffd\u7565\u200b\u7684\u200b\uff0c\u200b\u8fd9\u6837\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5c31\u200b\u4f1a\u200b\u88ab\u200b\u89c2\u5bdf\u200b\u5230\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8bb0\u5f97\u200b\u5173\u95ed\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff08dropout\uff09\u200b\u548c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff08augmentation\uff09\u3002\u200b\u5728\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u65f6\u200b\uff0c\u200b\u8bb0\u5f97\u200b\u5173\u95ed\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u4efb\u4f55\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u7684\u200b\u6548\u679c\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6bd4\u5982\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff0c\u200b\u968f\u673a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7b49\u200b\u3002\u200b\u4e0d\u7136\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u5728\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u7684\u200b\u65f6\u5019\u200b\u5bfc\u81f4\u200b\u5de8\u5927\u200b\u8bef\u5dee\u200b\u3002\u200b\u5173\u95ed\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u4e0d\u597d\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\u65e0\u6cd5\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\uff08\u200b\u4f8b\u5982\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u5b9e\u73b0\u200b\u53ef\u80fd\u200b\u6709\u200b\u9519\u8bef\u200b\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\u5c31\u662f\u200b\u5728\u200b\u8ba1\u7b97\u200b\\(f(x+h)\\)\u200b\u548c\u200b\\(f(x-h)\\)\u200b\u524d\u200b\u5f3a\u5236\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u7684\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u5728\u200b\u8ba1\u7b97\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u65f6\u200b\u4e5f\u200b\u540c\u6837\u200b\u5982\u6b64\u200b\u3002</p> <p>\u200b\u68c0\u67e5\u200b\u5c11\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u53ef\u4ee5\u200b\u6709\u200b\u4e0a\u767e\u4e07\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u53ea\u80fd\u200b\u68c0\u67e5\u200b\u5176\u4e2d\u200b\u4e00\u4e9b\u200b\u7ef4\u5ea6\u200b\u7136\u540e\u200b\u5047\u8bbe\u200b\u5176\u4ed6\u200b\u7ef4\u5ea6\u200b\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a\u200b\u786e\u8ba4\u200b\u5728\u200b\u6240\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\u4e2d\u200b\u90fd\u200b\u62bd\u53d6\u200b\u4e00\u90e8\u5206\u200b\u6765\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\u3002\u200b\u5728\u200b\u67d0\u4e9b\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\uff0c\u200b\u4eba\u4eec\u200b\u5c06\u200b\u6240\u6709\u200b\u7684\u200b\u53c2\u6570\u200b\u653e\u5230\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u4e2d\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f8b\u5982\u200b\u504f\u7f6e\u200b\u5c31\u200b\u53ef\u80fd\u200b\u53ea\u200b\u5360\u7528\u200b\u6574\u4e2a\u200b\u5411\u91cf\u200b\u4e2d\u200b\u7684\u200b\u5f88\u5c0f\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4e0d\u8981\u200b\u968f\u673a\u200b\u5730\u200b\u4ece\u200b\u5411\u91cf\u200b\u4e2d\u53d6\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u4e00\u5b9a\u200b\u8981\u200b\u628a\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u8003\u8651\u200b\u5230\u200b\uff0c\u200b\u786e\u4fdd\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u90fd\u200b\u6536\u5230\u200b\u4e86\u200b\u6b63\u786e\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_3","title":"\u5b66\u4e60\u200b\u4e4b\u524d\u200b\uff1a\u200b\u5408\u7406\u6027\u200b\u68c0\u67e5\u200b\u7684\u200b\u63d0\u793a\u200b\u4e0e\u200b\u6280\u5de7","text":"<p>\u200b\u5728\u200b\u8fdb\u884c\u200b\u8d39\u65f6\u8d39\u529b\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6700\u597d\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u5408\u7406\u6027\u200b\u68c0\u67e5\u200b\uff1a</p> <ul> <li>\u200b\u5bfb\u627e\u200b\u7279\u5b9a\u200b\u60c5\u51b5\u200b\u7684\u200b\u6b63\u786e\u200b\u635f\u5931\u200b\u503c\u200b\u3002\u200b\u5728\u200b\u4f7f\u7528\u200b\u5c0f\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u521d\u59cb\u5316\u200b\u65f6\u200b\uff0c\u200b\u786e\u4fdd\u200b\u5f97\u5230\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u4e0e\u200b\u671f\u671b\u200b\u4e00\u81f4\u200b\u3002\u200b\u6700\u597d\u200b\u5148\u200b\u5355\u72ec\u200b\u68c0\u67e5\u6570\u636e\u200b\u635f\u5931\u200b\uff08\u200b\u8ba9\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u4e3a\u200b0\uff09\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u8dd1\u200bCIFAR-10\u200b\u7684\u200bSoftmax\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u4e00\u822c\u200b\u671f\u671b\u200b\u5b83\u200b\u7684\u200b\u521d\u59cb\u200b\u635f\u5931\u200b\u503c\u200b\u662f\u200b2.302\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u521d\u59cb\u200b\u65f6\u200b\u9884\u8ba1\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u6982\u7387\u200b\u662f\u200b0.1\uff08\u200b\u56e0\u4e3a\u200b\u6709\u200b10\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\uff0c\u200b\u7136\u540e\u200bSoftmax\u200b\u635f\u5931\u200b\u503c\u200b\u6b63\u786e\u200b\u5206\u7c7b\u200b\u7684\u200b\u8d1f\u200b\u5bf9\u6570\u200b\u6982\u7387\u200b\uff1a-ln(0.1)=2.302\u3002\u200b\u5bf9\u4e8e\u200bWeston Watkins SVM\uff0c\u200b\u5047\u8bbe\u200b\u6240\u6709\u200b\u7684\u200b\u8fb9\u754c\u200b\u90fd\u200b\u88ab\u200b\u8d8a\u8fc7\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6240\u6709\u200b\u7684\u200b\u5206\u503c\u200b\u90fd\u200b\u8fd1\u4f3c\u200b\u4e3a\u200b\u96f6\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u635f\u5931\u200b\u503c\u200b\u662f\u200b9\uff08\u200b\u56e0\u4e3a\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200b\u9519\u8bef\u200b\u5206\u7c7b\u200b\uff0c\u200b\u8fb9\u754c\u503c\u200b\u662f\u200b1\uff09\u3002\u200b\u5982\u679c\u200b\u6ca1\u200b\u770b\u5230\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u503c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u521d\u59cb\u5316\u200b\u4e2d\u200b\u5c31\u200b\u53ef\u80fd\u200b\u6709\u200b\u95ee\u9898\u200b\u3002</li> <li>\u200b\u7b2c\u4e8c\u4e2a\u200b\u5408\u7406\u6027\u200b\u68c0\u67e5\u200b\uff1a\u200b\u63d0\u9ad8\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u65f6\u200b\u5bfc\u81f4\u200b\u635f\u5931\u200b\u503c\u53d8\u200b\u5927\u200b\u3002</li> <li>\u200b\u5bf9\u200b\u5c0f\u200b\u6570\u636e\u200b\u5b50\u96c6\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u6700\u540e\u200b\u4e5f\u200b\u662f\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u4e00\u6b65\u200b\uff0c\u200b\u5728\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5f88\u5c0f\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff08\u200b\u6bd4\u5982\u200b20\u200b\u4e2a\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u786e\u4fdd\u200b\u80fd\u200b\u5230\u8fbe\u200b0\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u3002\u200b\u8fdb\u884c\u200b\u8fd9\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6700\u597d\u200b\u8ba9\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u4e3a\u200b0\uff0c\u200b\u4e0d\u7136\u200b\u5b83\u4f1a\u200b\u963b\u6b62\u200b\u5f97\u5230\u200b0\u200b\u7684\u200b\u635f\u5931\u200b\u3002\u200b\u9664\u975e\u200b\u80fd\u200b\u901a\u8fc7\u200b\u8fd9\u200b\u4e00\u4e2a\u200b\u6b63\u5e38\u6027\u200b\u68c0\u67e5\u200b\uff0c\u200b\u4e0d\u7136\u200b\u8fdb\u884c\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u662f\u200b\u6ca1\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002\u200b\u4f46\u662f\u200b\u6ce8\u610f\u200b\uff0c\u200b\u80fd\u200b\u5bf9\u200b\u5c0f\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8fc7\u62df\u200b\u5408\u5e76\u200b\u4e0d\u200b\u4ee3\u8868\u200b\u4e07\u4e8b\u5927\u5409\u200b\uff0c\u200b\u4f9d\u7136\u200b\u6709\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u67d0\u4e9b\u200b\u9519\u8bef\u200b\uff0c\u200b\u6570\u636e\u200b\u70b9\u200b\u7684\u200b\u7279\u5f81\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\uff0c\u200b\u8fd9\u6837\u200b\u7b97\u6cd5\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u5bf9\u200b\u5c0f\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u8fc7\u200b\u62df\u5408\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8dd1\u200b\u7b97\u6cd5\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5c31\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-3/#_4","title":"\u68c0\u67e5\u200b\u6574\u4e2a\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5e94\u8be5\u200b\u8ddf\u8e2a\u200b\u591a\u4e2a\u200b\u91cd\u8981\u200b\u6570\u503c\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u6570\u503c\u200b\u8f93\u51fa\u200b\u7684\u200b\u56fe\u8868\u200b\u662f\u200b\u89c2\u5bdf\u200b\u8bad\u7ec3\u200b\u8fdb\u7a0b\u200b\u7684\u200b\u4e00\u6247\u200b\u7a97\u53e3\u200b\uff0c\u200b\u662f\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u6548\u679c\u200b\u7684\u200b\u5de5\u5177\u200b\uff0c\u200b\u4ece\u800c\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u4fee\u6539\u200b\u8d85\u200b\u53c2\u6570\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u200b\u9ad8\u6548\u200b\u7684\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e0b\u9762\u200b\u7684\u200b\u56fe\u8868\u200b\u4e2d\u200b\uff0cx\u200b\u8f74\u200b\u901a\u5e38\u200b\u90fd\u200b\u662f\u200b\u8868\u793a\u200b\u5468\u671f\u200b\uff08epochs\uff09\u200b\u5355\u4f4d\u200b\uff0c\u200b\u8be5\u200b\u5355\u4f4d\u200b\u8861\u91cf\u200b\u4e86\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u90fd\u200b\u88ab\u200b\u89c2\u5bdf\u200b\u8fc7\u200b\u6b21\u6570\u200b\u7684\u200b\u671f\u671b\u200b\uff08\u200b\u4e00\u4e2a\u200b\u5468\u671f\u200b\u610f\u5473\u7740\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u6570\u636e\u200b\u90fd\u200b\u88ab\u200b\u89c2\u5bdf\u200b\u8fc7\u200b\u4e86\u200b\u4e00\u6b21\u200b\uff09\u3002\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\uff08iterations\uff09\uff0c\u200b\u4e00\u822c\u200b\u66f4\u200b\u503e\u5411\u200b\u8ddf\u8e2a\u200b\u5468\u671f\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\u4e0e\u200b\u6570\u636e\u200b\u7684\u200b\u6279\u200b\u5c3a\u5bf8\u200b\uff08batchsize\uff09\u200b\u6709\u5173\u200b\uff0c\u200b\u800c\u6279\u200b\u5c3a\u5bf8\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u53c8\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_5","title":"\u635f\u5931\u200b\u51fd\u6570","text":"<p>\u200b\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u7b2c\u4e00\u4e2a\u200b\u8981\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u6570\u503c\u200b\u5c31\u662f\u200b\u635f\u5931\u200b\u503c\u200b\uff0c\u200b\u5b83\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u6279\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002\u200b\u4e0b\u56fe\u200b\u5c55\u793a\u200b\u7684\u200b\u662f\u200b\u968f\u7740\u200b\u635f\u5931\u200b\u503c\u200b\u968f\u200b\u65f6\u95f4\u200b\u7684\u200b\u53d8\u5316\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u66f2\u7ebf\u200b\u5f62\u72b6\u200b\u4f1a\u200b\u7ed9\u51fa\u200b\u5173\u4e8e\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u60c5\u51b5\u200b\uff1a</p> <p> </p> <p>\u200b\u5de6\u56fe\u200b\u5c55\u793a\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u8fc7\u4f4e\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u5bfc\u81f4\u200b\u7b97\u6cd5\u200b\u7684\u200b\u6539\u5584\u200b\u662f\u200b\u7ebf\u6027\u200b\u7684\u200b\u3002\u200b\u9ad8\u200b\u4e00\u4e9b\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u4f1a\u200b\u770b\u8d77\u6765\u200b\u5448\u51e0\u4f55\u200b\u6307\u6570\u200b\u4e0b\u964d\u200b\uff0c\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u4f1a\u200b\u8ba9\u200b\u635f\u5931\u200b\u503c\u200b\u5f88\u5feb\u200b\u4e0b\u964d\u200b\uff0c\u200b\u4f46\u662f\u200b\u63a5\u7740\u200b\u5c31\u200b\u505c\u200b\u5728\u200b\u4e00\u4e2a\u200b\u4e0d\u597d\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u4e0a\u200b\uff08\u200b\u7eff\u7ebf\u200b\uff09\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6700\u4f18\u5316\u200b\u7684\u200b\u201c\u200b\u80fd\u91cf\u200b\u201d\u200b\u592a\u200b\u5927\u200b\uff0c\u200b\u53c2\u6570\u200b\u5728\u200b\u6df7\u6c8c\u200b\u4e2d\u200b\u968f\u673a\u200b\u9707\u8361\u200b\uff0c\u200b\u4e0d\u80fd\u200b\u6700\u4f18\u5316\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u70b9\u200b\u4e0a\u200b\u3002\u200b\u53f3\u56fe\u200b\u663e\u793a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5178\u578b\u200b\u7684\u200b\u968f\u200b\u65f6\u95f4\u200b\u53d8\u5316\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\uff0c\u200b\u5728\u200bCIFAR-10\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0a\u9762\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u7684\u200b\u7f51\u7edc\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\u66f2\u7ebf\u200b\u770b\u8d77\u6765\u200b\u6bd4\u8f83\u200b\u5408\u7406\u200b\uff08\u200b\u867d\u7136\u200b\u53ef\u80fd\u200b\u5b66\u4e60\u200b\u7387\u200b\u6709\u70b9\u200b\u5c0f\u200b\uff0c\u200b\u4f46\u662f\u200b\u5f88\u96be\u8bf4\u200b\uff09\uff0c\u200b\u800c\u4e14\u200b\u6307\u51fa\u200b\u4e86\u200b\u6279\u200b\u6570\u636e\u200b\u7684\u200b\u6570\u91cf\u200b\u53ef\u80fd\u200b\u6709\u70b9\u200b\u592a\u5c0f\u200b\uff08\u200b\u56e0\u4e3a\u200b\u635f\u5931\u200b\u503c\u200b\u7684\u200b\u566a\u97f3\u200b\u5f88\u5927\u200b\uff09\u3002</p> <p>\u200b\u635f\u5931\u200b\u503c\u200b\u7684\u200b\u9707\u8361\u200b\u7a0b\u5ea6\u200b\u548c\u200b\u6279\u200b\u5c3a\u5bf8\u200b\uff08batch size\uff09\u200b\u6709\u5173\u200b\uff0c\u200b\u5f53\u6279\u200b\u5c3a\u5bf8\u200b\u4e3a\u200b1\uff0c\u200b\u9707\u8361\u200b\u4f1a\u200b\u76f8\u5bf9\u200b\u8f83\u5927\u200b\u3002\u200b\u5f53\u6279\u200b\u5c3a\u5bf8\u200b\u5c31\u662f\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u65f6\u200b\u9707\u8361\u200b\u5c31\u200b\u4f1a\u200b\u6700\u5c0f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6bcf\u4e2a\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u90fd\u200b\u662f\u200b\u5355\u8c03\u200b\u5730\u200b\u4f18\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u9664\u975e\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u5f97\u200b\u8fc7\u200b\u9ad8\u200b\uff09\u3002</p> <p>\u200b\u6709\u200b\u7684\u200b\u7814\u7a76\u8005\u200b\u559c\u6b22\u200b\u7528\u200b\u5bf9\u6570\u200b\u57df\u200b\u5bf9\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\u4f5c\u56fe\u200b\u3002\u200b\u56e0\u4e3a\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\u4e00\u822c\u200b\u90fd\u200b\u662f\u200b\u91c7\u7528\u200b\u6307\u6570\u200b\u578b\u200b\u7684\u200b\u5f62\u72b6\u200b\uff0c\u200b\u56fe\u8868\u200b\u5c31\u200b\u4f1a\u200b\u770b\u8d77\u6765\u200b\u66f4\u200b\u50cf\u662f\u200b\u80fd\u591f\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\u7684\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5448\u200b\u66f2\u68cd\u7403\u200b\u4e00\u6837\u200b\u7684\u200b\u66f2\u7ebf\u200b\u72b6\u200b\u3002\u200b\u8fd8\u6709\u200b\uff0c\u200b\u5982\u679c\u200b\u591a\u4e2a\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u6a21\u578b\u200b\u5728\u200b\u4e00\u4e2a\u200b\u56fe\u4e0a\u200b\u540c\u65f6\u200b\u8f93\u51fa\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u5c31\u200b\u4f1a\u200b\u6bd4\u8f83\u200b\u660e\u663e\u200b\u3002</p> <p>\u200b\u6709\u65f6\u5019\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u5f88\u200b\u6709\u610f\u601d\u200b\uff1alossfunctions.tumblr.com\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_6","title":"\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u51c6\u786e\u7387","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u9700\u8981\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u7b2c\u4e8c\u200b\u91cd\u8981\u200b\u7684\u200b\u6570\u503c\u200b\u662f\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u548c\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u56fe\u8868\u200b\u80fd\u591f\u200b\u5c55\u73b0\u200b\u77e5\u9053\u200b\u6a21\u578b\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u7a0b\u5ea6\u200b\uff1a</p> <p> </p> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u51c6\u786e\u7387\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u51c6\u786e\u7387\u200b\u4e2d\u95f4\u200b\u7684\u200b\u7a7a\u9699\u200b\u6307\u660e\u200b\u4e86\u200b\u6a21\u578b\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u7a0b\u5ea6\u200b\u3002\u200b\u5728\u200b\u56fe\u200b\u4e2d\u200b\uff0c\u200b\u84dd\u8272\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u66f2\u7ebf\u200b\u663e\u793a\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u4f4e\u200b\u4e86\u200b\u5f88\u591a\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u8bf4\u660e\u200b\u6a21\u578b\u200b\u6709\u200b\u5f88\u200b\u5f3a\u200b\u7684\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u9047\u5230\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5c31\u200b\u5e94\u8be5\u200b\u589e\u5927\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\uff08\u200b\u66f4\u5f3a\u200b\u7684\u200bL2\u200b\u6743\u91cd\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u7b49\u200b\uff09\u200b\u6216\u200b\u6536\u96c6\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u53e6\u200b\u4e00\u79cd\u200b\u53ef\u80fd\u200b\u5c31\u662f\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u66f2\u7ebf\u200b\u548c\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u66f2\u7ebf\u200b\u5982\u5f71\u968f\u5f62\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u8bf4\u660e\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5bb9\u91cf\u200b\u8fd8\u200b\u4e0d\u591f\u200b\u5927\u200b\uff1a\u200b\u5e94\u8be5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5bb9\u91cf\u200b\u66f4\u5927\u4e9b\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_7","title":"\u6743\u91cd\u200b\u66f4\u65b0\u200b\u6bd4\u4f8b","text":"<p>\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5e94\u8be5\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u91cf\u200b\u662f\u200b\u6743\u91cd\u200b\u4e2d\u200b\u66f4\u65b0\u200b\u503c\u200b\u7684\u200b\u6570\u91cf\u200b\u548c\u200b\u5168\u90e8\u200b\u503c\u200b\u7684\u200b\u6570\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a\u200b\u662f\u200b\u66f4\u65b0\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u539f\u59cb\u200b\u68af\u5ea6\u200b\uff08\u200b\u6bd4\u5982\u200b\uff0c\u200b\u5728\u200b\u666e\u901a\u200bsgd\u200b\u4e2d\u200b\u5c31\u662f\u200b\u68af\u5ea6\u200b\u4e58\u4ee5\u200b\u5b66\u4e60\u200b\u7387\u200b\uff09\u3002\u200b\u9700\u8981\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b\u96c6\u200b\u7684\u200b\u66f4\u65b0\u200b\u6bd4\u4f8b\u200b\u8fdb\u884c\u200b\u5355\u72ec\u200b\u7684\u200b\u8ba1\u7b97\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u3002\u200b\u4e00\u4e2a\u200b\u7ecf\u9a8c\u6027\u200b\u7684\u200b\u7ed3\u8bba\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u6bd4\u4f8b\u200b\u5e94\u8be5\u200b\u5728\u200b1e-3\u200b\u5de6\u53f3\u200b\u3002\u200b\u5982\u679c\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5b66\u4e60\u200b\u7387\u200b\u53ef\u80fd\u200b\u592a\u5c0f\u200b\uff0c\u200b\u5982\u679c\u200b\u66f4\u9ad8\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5b66\u4e60\u200b\u7387\u200b\u53ef\u80fd\u200b\u592a\u9ad8\u200b\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u5177\u4f53\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code># \u200b\u5047\u8bbe\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u4e3a\u200bW\uff0c\u200b\u5176\u200b\u68af\u5ea6\u200b\u5411\u91cf\u200b\u4e3a\u200bdW\nparam_scale = np.linalg.norm(W.ravel())\nupdate = -learning_rate*dW # \u200b\u7b80\u5355\u200bSGD\u200b\u66f4\u65b0\u200b\nupdate_scale = np.linalg.norm(update.ravel())\nW += update # \u200b\u5b9e\u9645\u200b\u66f4\u65b0\u200b\nprint update_scale / param_scale # \u200b\u8981\u200b\u5f97\u5230\u200b1e-3\u200b\u5de6\u53f3\u200b\n</code></pre> <p>\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u8ddf\u8e2a\u200b\u6700\u5927\u200b\u548c\u200b\u6700\u5c0f\u503c\u200b\uff0c\u200b\u6709\u200b\u7814\u7a76\u8005\u200b\u66f4\u200b\u559c\u6b22\u200b\u8ba1\u7b97\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u68af\u5ea6\u200b\u7684\u200b\u8303\u5f0f\u200b\u53ca\u5176\u200b\u66f4\u65b0\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u77e9\u9635\u200b\u901a\u5e38\u200b\u662f\u200b\u76f8\u5173\u200b\u7684\u200b\uff0c\u200b\u4e5f\u200b\u80fd\u200b\u5f97\u5230\u200b\u8fd1\u4f3c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_8","title":"\u6bcf\u5c42\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u53ca\u200b\u68af\u5ea6\u200b\u5206\u5e03","text":"<p>\u200b\u4e00\u4e2a\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u53ef\u80fd\u200b\u8ba9\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\u53d8\u6162\u200b\uff0c\u200b\u751a\u81f3\u200b\u5f7b\u5e95\u200b\u505c\u6b62\u200b\u3002\u200b\u8fd8\u597d\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u53ef\u4ee5\u200b\u6bd4\u8f83\u7b80\u5355\u200b\u5730\u200b\u8bca\u65ad\u200b\u51fa\u6765\u200b\u3002\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u65b9\u6cd5\u200b\u662f\u200b\u8f93\u51fa\u200b\u7f51\u7edc\u200b\u4e2d\u200b\u6240\u6709\u200b\u5c42\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u548c\u200b\u68af\u5ea6\u200b\u5206\u5e03\u200b\u7684\u200b\u67f1\u72b6\u56fe\u200b\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b\u5982\u679c\u200b\u770b\u5230\u200b\u4efb\u4f55\u200b\u5947\u602a\u200b\u7684\u200b\u5206\u5e03\u200b\u60c5\u51b5\u200b\uff0c\u200b\u90a3\u200b\u90fd\u200b\u4e0d\u662f\u200b\u597d\u200b\u5146\u5934\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4f7f\u7528\u200btanh\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u770b\u5230\u200b\u6fc0\u6d3b\u200b\u6570\u636e\u200b\u7684\u200b\u503c\u200b\u5728\u200b\u6574\u4e2a\u200b[-1,1]\u200b\u533a\u95f4\u200b\u4e2d\u200b\u90fd\u200b\u6709\u200b\u5206\u5e03\u200b\u3002\u200b\u5982\u679c\u200b\u770b\u5230\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u8f93\u51fa\u200b\u5168\u90e8\u200b\u662f\u200b0\uff0c\u200b\u6216\u8005\u200b\u5168\u90fd\u200b\u9971\u548c\u200b\u4e86\u200b\u5f80\u200b-1\u200b\u548c\u200b1\u200b\u4e0a\u200b\u8dd1\u200b\uff0c\u200b\u90a3\u200b\u80af\u5b9a\u200b\u5c31\u662f\u200b\u6709\u200b\u95ee\u9898\u200b\u4e86\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_9","title":"\u7b2c\u4e00\u5c42\u200b\u53ef\u89c6\u5316","text":"<p>\u200b\u6700\u540e\u200b\uff0c\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u662f\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u6570\u636e\u200b\uff0c\u200b\u90a3\u4e48\u200b\u628a\u200b\u7b2c\u4e00\u5c42\u200b\u7279\u5f81\u200b\u53ef\u89c6\u5316\u200b\u4f1a\u200b\u6709\u200b\u5e2e\u52a9\u200b\uff1a</p> <p> </p> <p>\u200b\u5c06\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7b2c\u4e00\u5c42\u200b\u7684\u200b\u6743\u91cd\u200b\u53ef\u89c6\u5316\u200b\u7684\u200b\u4f8b\u5b50\u200b\u3002\u200b\u5de6\u56fe\u200b\u4e2d\u200b\u7684\u200b\u7279\u5f81\u200b\u5145\u6ee1\u200b\u4e86\u200b\u566a\u97f3\u200b\uff0c\u200b\u8fd9\u200b\u6697\u793a\u200b\u4e86\u200b\u7f51\u7edc\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u4e86\u200b\u95ee\u9898\u200b\uff1a\u200b\u7f51\u7edc\u200b\u6ca1\u6709\u200b\u6536\u655b\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u4e0d\u200b\u6070\u5f53\u200b\uff0c\u200b\u6b63\u5219\u200b\u5316\u200b\u60e9\u7f5a\u200b\u7684\u200b\u6743\u91cd\u200b\u8fc7\u4f4e\u200b\u3002\u200b\u53f3\u56fe\u200b\u7684\u200b\u7279\u5f81\u200b\u4e0d\u9519\u200b\uff0c\u200b\u5e73\u6ed1\u200b\uff0c\u200b\u5e72\u51c0\u200b\u800c\u4e14\u200b\u79cd\u7c7b\u200b\u7e41\u591a\u200b\uff0c\u200b\u8bf4\u660e\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u8fdb\u884c\u200b\u826f\u597d\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_10","title":"\u53c2\u6570\u200b\u66f4\u65b0","text":"<p>\u200b\u4e00\u65e6\u200b\u80fd\u200b\u4f7f\u7528\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8ba1\u7b97\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\uff0c\u200b\u68af\u5ea6\u200b\u5c31\u200b\u80fd\u200b\u88ab\u200b\u7528\u6765\u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u4e86\u200b\u3002\u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u6709\u200b\u597d\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u90fd\u200b\u4f1a\u200b\u8fdb\u884c\u200b\u8ba8\u8bba\u200b\u3002</p> <p>\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u662f\u200b\u73b0\u5728\u200b\u975e\u5e38\u200b\u6d3b\u8dc3\u200b\u7684\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\u3002\u200b\u672c\u8282\u200b\u5c06\u200b\u91cd\u70b9\u200b\u4ecb\u7ecd\u200b\u4e00\u4e9b\u200b\u516c\u8ba4\u200b\u6709\u6548\u200b\u7684\u200b\u5e38\u7528\u200b\u7684\u200b\u6280\u5de7\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6280\u5de7\u200b\u90fd\u200b\u662f\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u7b80\u8981\u200b\u4ecb\u7ecd\u200b\u8fd9\u4e9b\u200b\u6280\u5de7\u200b\u7684\u200b\u76f4\u89c2\u200b\u6982\u5ff5\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u7ec6\u8282\u200b\u5206\u6790\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u7ec6\u8282\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u8bfb\u8005\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u62d3\u5c55\u200b\u9605\u8bfb\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_11","title":"\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u53ca\u200b\u5404\u79cd\u200b\u66f4\u65b0\u200b\u65b9\u6cd5","text":"<p>\u200b\u666e\u901a\u200b\u66f4\u65b0\u200b\u3002\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u66f4\u65b0\u200b\u5f62\u5f0f\u200b\u662f\u200b\u6cbf\u7740\u200b\u8d1f\u200b\u68af\u5ea6\u65b9\u5411\u200b\u6539\u53d8\u200b\u53c2\u6570\u200b\uff08\u200b\u56e0\u4e3a\u200b\u68af\u5ea6\u200b\u6307\u5411\u200b\u7684\u200b\u662f\u200b\u4e0a\u5347\u200b\u65b9\u5411\u200b\uff0c\u200b\u4f46\u662f\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u5e0c\u671b\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff09\u3002\u200b\u5047\u8bbe\u200b\u6709\u200b\u4e00\u4e2a\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b<code>x</code>\u200b\u53ca\u5176\u200b\u68af\u5ea6\u200b<code>dx</code>\uff0c\u200b\u90a3\u4e48\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u66f4\u65b0\u200b\u7684\u200b\u5f62\u5f0f\u200b\u662f\u200b\uff1a</p> <pre><code># \u200b\u666e\u901a\u200b\u66f4\u65b0\u200b\nx += - learning_rate * dx\n</code></pre> <p>\u200b\u5176\u4e2d\u200b<code>learning_rate</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u56fa\u5b9a\u200b\u7684\u200b\u5e38\u91cf\u200b\u3002\u200b\u5f53\u200b\u5728\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u65f6\u200b\uff0c\u200b\u53ea\u8981\u200b\u5b66\u4e60\u200b\u7387\u200b\u8db3\u591f\u200b\u4f4e\u200b\uff0c\u200b\u603b\u662f\u200b\u80fd\u200b\u5728\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e0a\u200b\u5f97\u5230\u200b\u975e\u8d1f\u200b\u7684\u200b\u8fdb\u5c55\u200b\u3002</p> <p>\u200b\u52a8\u91cf\u200b\uff08Momentum\uff09\u200b\u66f4\u65b0\u200b\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u5728\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u4e0a\u200b\u51e0\u4e4e\u200b\u603b\u80fd\u200b\u5f97\u5230\u200b\u66f4\u597d\u200b\u7684\u200b\u6536\u655b\u200b\u901f\u5ea6\u200b\u3002\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u770b\u6210\u200b\u662f\u4ece\u200b\u7269\u7406\u200b\u89d2\u5ea6\u200b\u4e0a\u200b\u5bf9\u4e8e\u200b\u6700\u4f18\u5316\u200b\u95ee\u9898\u200b\u5f97\u5230\u200b\u7684\u200b\u542f\u53d1\u200b\u3002\u200b\u635f\u5931\u200b\u503c\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u4e3a\u200b\u662f\u200b\u5c71\u200b\u7684\u200b\u9ad8\u5ea6\u200b\uff08\u200b\u56e0\u6b64\u200b\u9ad8\u5ea6\u200b\u52bf\u80fd\u200b\u662f\u200b\\(U = mgh\\)\uff0c\u200b\u6240\u4ee5\u200b\u6709\u200b \\(U \\propto h\\)\uff09\u3002\u200b\u7528\u200b\u968f\u673a\u200b\u6570\u5b57\u200b\u521d\u59cb\u5316\u200b\u53c2\u6570\u200b\u7b49\u540c\u4e8e\u200b\u5728\u200b\u67d0\u4e2a\u200b\u4f4d\u7f6e\u200b\u7ed9\u200b\u8d28\u70b9\u200b\u8bbe\u5b9a\u200b\u521d\u59cb\u200b\u901f\u5ea6\u200b\u4e3a\u200b0\u3002\u200b\u8fd9\u6837\u200b\u6700\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u53ef\u4ee5\u200b\u770b\u505a\u200b\u662f\u200b\u6a21\u62df\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\uff08\u200b\u5373\u200b\u8d28\u70b9\u200b\uff09\u200b\u5728\u200b\u5730\u5f62\u200b\u4e0a\u200b\u6eda\u52a8\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u4f5c\u7528\u200b\u4e8e\u200b\u8d28\u70b9\u200b\u7684\u200b\u529b\u200b\u4e0e\u200b\u68af\u5ea6\u200b\u7684\u200b\u6f5c\u5728\u200b\u80fd\u91cf\u200b\uff08 \\(F = - \\nabla U\\)\uff09\u200b\u6709\u5173\u200b\uff0c\u200b\u8d28\u70b9\u200b\u6240\u53d7\u200b\u7684\u200b\u529b\u200b\u5c31\u662f\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\uff08\u200b\u8d1f\u200b\uff09\u200b\u68af\u5ea6\u200b\u3002\u200b\u8fd8\u6709\u200b\uff0c\u200b\u56e0\u4e3a\u200b\\(F = ma\\)\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u89c2\u70b9\u200b\u4e0b\u200b\uff08\u200b\u8d1f\u200b\uff09\u200b\u68af\u5ea6\u200b\u4e0e\u200b\u8d28\u70b9\u200b\u7684\u200b\u52a0\u901f\u5ea6\u200b\u662f\u200b\u6210\u200b\u6bd4\u4f8b\u200b\u7684\u200b\u3002\u200b\u6ce8\u610f\u200b\u8fd9\u4e2a\u200b\u7406\u89e3\u200b\u548c\u200b\u4e0a\u9762\u200b\u7684\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08SDG\uff09\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\uff0c\u200b\u5728\u200b\u666e\u901a\u200b\u7248\u672c\u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u76f4\u63a5\u200b\u5f71\u54cd\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u800c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u7248\u672c\u200b\u7684\u200b\u66f4\u65b0\u200b\u4e2d\u200b\uff0c\u200b\u7269\u7406\u200b\u89c2\u70b9\u200b\u5efa\u8bae\u200b\u68af\u5ea6\u200b\u53ea\u662f\u200b\u5f71\u54cd\u200b\u901f\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u901f\u5ea6\u200b\u518d\u200b\u5f71\u54cd\u200b\u4f4d\u7f6e\u200b\uff1a</p> <pre><code># \u200b\u52a8\u91cf\u200b\u66f4\u65b0\u200b\nv = mu * v - learning_rate * dx # \u200b\u4e0e\u200b\u901f\u5ea6\u200b\u878d\u5408\u200b\nx += v # \u200b\u4e0e\u200b\u4f4d\u7f6e\u200b\u878d\u5408\u200b\n</code></pre> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\u5f15\u5165\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u521d\u59cb\u5316\u200b\u4e3a\u200b0\u200b\u7684\u200b\u53d8\u91cf\u200b <code>v</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b<code>mu</code>\u3002\u200b\u8bf4\u200b\u5f97\u200b\u4e0d\u200b\u6070\u5f53\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u53d8\u91cf\u200b\uff08mu\uff09\u200b\u5728\u200b\u6700\u4f18\u5316\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u88ab\u200b\u770b\u505a\u200b\u52a8\u91cf\u200b\uff08\u200b\u4e00\u822c\u200b\u503c\u8bbe\u200b\u4e3a\u200b0.9\uff09\uff0c\u200b\u4f46\u200b\u5176\u200b\u7269\u7406\u200b\u610f\u4e49\u200b\u4e0e\u200b\u6469\u64e6\u7cfb\u6570\u200b\u66f4\u200b\u4e00\u81f4\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u53d8\u91cf\u200b\u6709\u6548\u200b\u5730\u200b\u6291\u5236\u200b\u4e86\u200b\u901f\u5ea6\u200b\uff0c\u200b\u964d\u4f4e\u200b\u4e86\u200b\u7cfb\u7edf\u200b\u7684\u200b\u52a8\u80fd\u200b\uff0c\u200b\u4e0d\u7136\u200b\u8d28\u70b9\u200b\u5728\u200b\u5c71\u5e95\u200b\u6c38\u8fdc\u200b\u4e0d\u4f1a\u200b\u505c\u4e0b\u6765\u200b\u3002\u200b\u901a\u8fc7\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u53c2\u6570\u200b\u901a\u5e38\u200b\u8bbe\u200b\u4e3a\u200b[0.5,0.9,0.95,0.99]\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u3002\u200b\u548c\u200b\u5b66\u4e60\u200b\u7387\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u9000\u706b\u200b\uff08\u200b\u4e0b\u6587\u200b\u6709\u200b\u8ba8\u8bba\u200b\uff09\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u52a8\u91cf\u200b\u968f\u200b\u65f6\u95f4\u200b\u53d8\u5316\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u6709\u65f6\u200b\u80fd\u200b\u7565\u5fae\u200b\u6539\u5584\u200b\u6700\u4f18\u5316\u200b\u7684\u200b\u6548\u679c\u200b\uff0c\u200b\u5176\u4e2d\u200b\u52a8\u91cf\u200b\u5728\u200b\u5b66\u4e60\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u540e\u200b\u9636\u6bb5\u200b\u4f1a\u200b\u4e0a\u5347\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5178\u578b\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u662f\u200b\u521a\u200b\u5f00\u59cb\u200b\u5c06\u200b\u52a8\u91cf\u200b\u8bbe\u200b\u4e3a\u200b0.5\u200b\u800c\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u591a\u4e2a\u200b\u5468\u671f\u200b\uff08epoch\uff09\u200b\u4e2d\u200b\u6162\u6162\u200b\u63d0\u5347\u200b\u5230\u200b0.99\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u52a8\u91cf\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u4f1a\u200b\u5728\u200b\u4efb\u4f55\u200b\u6709\u200b\u6301\u7eed\u200b\u68af\u5ea6\u200b\u7684\u200b\u65b9\u5411\u200b\u4e0a\u200b\u589e\u52a0\u200b\u901f\u5ea6\u200b\u3002</p> <p>Nesterov\u200b\u52a8\u91cf\u200b\u4e0e\u200b\u666e\u901a\u200b\u52a8\u91cf\u200b\u6709\u200b\u4e9b\u8bb8\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6700\u8fd1\u200b\u53d8\u5f97\u200b\u6bd4\u8f83\u200b\u6d41\u884c\u200b\u3002\u200b\u5728\u200b\u7406\u8bba\u200b\u4e0a\u200b\u5bf9\u4e8e\u200b\u51f8\u51fd\u6570\u200b\u5b83\u200b\u80fd\u200b\u5f97\u5230\u200b\u66f4\u597d\u200b\u7684\u200b\u6536\u655b\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u4e5f\u200b\u786e\u5b9e\u200b\u6bd4\u200b\u6807\u51c6\u200b\u52a8\u91cf\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\u4e00\u4e9b\u200b\u3002</p> <p>Nesterov\u200b\u52a8\u91cf\u200b\u7684\u200b\u6838\u5fc3\u200b\u601d\u8def\u200b\u662f\u200b\uff0c\u200b\u5f53\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u4f4d\u4e8e\u200b\u67d0\u4e2a\u200b\u4f4d\u7f6e\u200b<code>x</code>\u200b\u65f6\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u4e0a\u9762\u200b\u7684\u200b\u52a8\u91cf\u200b\u66f4\u65b0\u200b\u516c\u5f0f\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\uff0c\u200b\u52a8\u91cf\u200b\u90e8\u5206\u200b\uff08\u200b\u5ffd\u89c6\u200b\u5e26\u200b\u68af\u5ea6\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b\u90e8\u5206\u200b\uff09\u200b\u4f1a\u200b\u901a\u8fc7\u200b<code>mu * v</code>\u200b\u7a0d\u5fae\u200b\u6539\u53d8\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u8981\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u90a3\u4e48\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u672a\u6765\u200b\u7684\u200b\u8fd1\u4f3c\u200b\u4f4d\u7f6e\u200b<code>x + mu * v</code>\u200b\u770b\u505a\u200b\u662f\u200b\u201c\u200b\u5411\u524d\u200b\u770b\u200b\u201d\uff0c\u200b\u8fd9\u4e2a\u200b\u70b9\u200b\u5728\u200b\u6211\u4eec\u200b\u4e00\u4f1a\u513f\u200b\u8981\u200b\u505c\u6b62\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u9644\u8fd1\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba1\u7b97\u200b<code>x + mu * v</code>\u200b\u7684\u200b\u68af\u5ea6\u200b\u800c\u200b\u4e0d\u662f\u200b\u201c\u200b\u65e7\u200b\u201d\u200b\u4f4d\u7f6e\u200b<code>x</code>\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c31\u200b\u6709\u200b\u610f\u4e49\u200b\u4e86\u200b\u3002</p> <p> </p> <p>Nesterov\u200b\u52a8\u91cf\u200b\u3002\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u52a8\u91cf\u200b\u5c06\u4f1a\u200b\u628a\u200b\u6211\u4eec\u200b\u5e26\u5230\u200b\u7eff\u8272\u200b\u7bad\u5934\u200b\u6307\u5411\u200b\u7684\u200b\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u4e0d\u8981\u200b\u5728\u200b\u539f\u70b9\u200b\uff08\u200b\u7ea2\u8272\u200b\u70b9\u200b\uff09\u200b\u90a3\u91cc\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u4e86\u200b\u3002\u200b\u4f7f\u7528\u200bNesterov\u200b\u52a8\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u201c\u200b\u5411\u524d\u200b\u770b\u200b\u201d\u200b\u7684\u200b\u5730\u65b9\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u4e00\u4e9b\u200b\u6ce8\u91ca\u200b\u540e\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>x_ahead = x + mu * v\n# \u200b\u8ba1\u7b97\u200bdx_ahead(\u200b\u5728\u200bx_ahead\u200b\u5904\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5728\u200bx\u200b\u5904\u200b\u7684\u200b\u68af\u5ea6\u200b)\nv = mu * v - learning_rate * dx_ahead\nx += v\n</code></pre> <p>\u200b\u7136\u800c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4eba\u4eec\u200b\u66f4\u200b\u559c\u6b22\u200b\u548c\u200b\u666e\u901a\u200bSGD\u200b\u6216\u200b\u4e0a\u9762\u200b\u7684\u200b\u52a8\u91cf\u200b\u65b9\u6cd5\u200b\u4e00\u6837\u200b\u7b80\u5355\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u3002\u200b\u901a\u8fc7\u200b\u5bf9\u200b <code>x_ahead = x + mu * v</code>\u200b\u4f7f\u7528\u200b\u53d8\u91cf\u200b\u53d8\u6362\u200b\u8fdb\u884c\u200b\u6539\u5199\u200b\u662f\u200b\u53ef\u4ee5\u200b\u505a\u5230\u200b\u7684\u200b\uff0c\u200b\u7136\u540e\u200b\u7528\u200b <code>x_ahead</code>\u200b\u800c\u200b\u4e0d\u662f\u200b<code>x</code>\u200b\u6765\u200b\u8868\u793a\u200b\u4e0a\u9762\u200b\u7684\u200b\u66f4\u65b0\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5b9e\u9645\u200b\u5b58\u50a8\u200b\u7684\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u603b\u662f\u200b\u5411\u524d\u200b\u4e00\u6b65\u200b\u7684\u200b\u90a3\u4e2a\u200b\u7248\u672c\u200b\u3002<code>x_ahead</code>\u200b\u7684\u200b\u516c\u5f0f\u200b\uff08\u200b\u5c06\u200b\u5176\u200b\u91cd\u65b0\u547d\u540d\u200b\u4e3a\u200b<code>x</code>\uff09\u200b\u5c31\u200b\u53d8\u6210\u200b\u4e86\u200b\uff1a</p> <pre><code>v_prev = v # \u200b\u5b58\u50a8\u200b\u5907\u4efd\u200b\nv = mu * v - learning_rate * dx # \u200b\u901f\u5ea6\u200b\u66f4\u65b0\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\nx += -mu * v_prev + (1 + mu) * v # \u200b\u4f4d\u7f6e\u200b\u66f4\u65b0\u200b\u53d8\u200b\u4e86\u200b\u5f62\u5f0f\u200b\n</code></pre> <p>\u200b\u5bf9\u4e8e\u200bNAG\uff08Nesterov's Accelerated Momentum\uff09\u200b\u7684\u200b\u6765\u6e90\u200b\u548c\u200b\u6570\u5b66\u516c\u5f0f\u200b\u63a8\u5bfc\u200b\uff0c\u200b\u6211\u4eec\u200b\u63a8\u8350\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u62d3\u5c55\u200b\u9605\u8bfb\u200b\uff1a</p> <ul> <li>Yoshua Bengio\u200b\u7684\u200bAdvances in optimizing Recurrent Networks\uff0cSection 3.5\u3002</li> <li>Ilya Sutskever's thesis (pdf)\u200b\u5728\u200bsection 7.2\u200b\u5bf9\u4e8e\u200b\u8fd9\u4e2a\u200b\u4e3b\u9898\u200b\u6709\u200b\u66f4\u200b\u8be6\u5c3d\u200b\u7684\u200b\u9610\u8ff0\u200b\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-3/#anneal","title":"\u5b66\u4e60\u200b\u7387\u200b\u9000\u706b\u200banneal","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u8ba9\u200b\u5b66\u4e60\u200b\u7387\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u9000\u706b\u200b\u901a\u5e38\u200b\u662f\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u3002\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u7406\u89e3\u200b\uff1a\u200b\u5982\u679c\u200b\u5b66\u4e60\u200b\u7387\u200b\u5f88\u200b\u9ad8\u200b\uff0c\u200b\u7cfb\u7edf\u200b\u7684\u200b\u52a8\u80fd\u200b\u5c31\u200b\u8fc7\u200b\u5927\u200b\uff0c\u200b\u53c2\u6570\u200b\u5411\u91cf\u200b\u5c31\u200b\u4f1a\u200b\u65e0\u89c4\u5f8b\u200b\u5730\u200b\u8df3\u52a8\u200b\uff0c\u200b\u4e0d\u200b\u80fd\u591f\u200b\u7a33\u5b9a\u200b\u5230\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u66f4\u6df1\u200b\u66f4\u7a84\u200b\u7684\u200b\u90e8\u5206\u200b\u53bb\u200b\u3002\u200b\u77e5\u9053\u200b\u4ec0\u4e48\u200b\u65f6\u5019\u200b\u5f00\u59cb\u200b\u8870\u51cf\u200b\u5b66\u4e60\u200b\u7387\u200b\u662f\u200b\u6709\u200b\u6280\u5de7\u200b\u7684\u200b\uff1a\u200b\u6162\u6162\u200b\u51cf\u5c0f\u200b\u5b83\u200b\uff0c\u200b\u53ef\u80fd\u200b\u5728\u200b\u5f88\u200b\u957f\u65f6\u95f4\u200b\u5185\u200b\u53ea\u80fd\u200b\u662f\u200b\u6d6a\u8d39\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u5730\u200b\u770b\u7740\u200b\u5b83\u200b\u6df7\u6c8c\u200b\u5730\u200b\u8df3\u52a8\u200b\uff0c\u200b\u5b9e\u9645\u200b\u8fdb\u5c55\u200b\u5f88\u5c11\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u5feb\u901f\u200b\u5730\u200b\u51cf\u5c11\u200b\u5b83\u200b\uff0c\u200b\u7cfb\u7edf\u200b\u53ef\u80fd\u200b\u8fc7\u5feb\u200b\u5730\u200b\u5931\u53bb\u200b\u80fd\u91cf\u200b\uff0c\u200b\u4e0d\u80fd\u5230\u8fbe\u200b\u539f\u672c\u200b\u53ef\u4ee5\u200b\u5230\u8fbe\u200b\u7684\u200b\u6700\u597d\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u901a\u5e38\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u5b66\u4e60\u200b\u7387\u200b\u9000\u706b\u200b\u6709\u200b3\u200b\u79cd\u200b\u65b9\u5f0f\u200b\uff1a</p> <ul> <li>\u200b\u968f\u200b\u6b65\u6570\u200b\u8870\u51cf\u200b\uff1a\u200b\u6bcf\u200b\u8fdb\u884c\u200b\u51e0\u4e2a\u200b\u5468\u671f\u200b\u5c31\u200b\u6839\u636e\u200b\u4e00\u4e9b\u200b\u56e0\u7d20\u200b\u964d\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\u3002\u200b\u5178\u578b\u200b\u7684\u200b\u503c\u200b\u662f\u200b\u6bcf\u8fc7\u200b5\u200b\u4e2a\u200b\u5468\u671f\u200b\u5c31\u200b\u5c06\u200b\u5b66\u4e60\u200b\u7387\u200b\u51cf\u5c11\u200b\u4e00\u534a\u200b\uff0c\u200b\u6216\u8005\u200b\u6bcf\u200b20\u200b\u4e2a\u200b\u5468\u671f\u200b\u51cf\u5c11\u200b\u5230\u200b\u4e4b\u524d\u200b\u7684\u200b0.1\u3002\u200b\u8fd9\u4e9b\u200b\u6570\u503c\u200b\u7684\u200b\u8bbe\u5b9a\u200b\u662f\u200b\u4e25\u91cd\u200b\u4f9d\u8d56\u200b\u5177\u4f53\u200b\u95ee\u9898\u200b\u548c\u200b\u6a21\u578b\u200b\u7684\u200b\u9009\u62e9\u200b\u7684\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u53ef\u80fd\u200b\u770b\u89c1\u200b\u8fd9\u4e48\u200b\u4e00\u79cd\u200b\u7ecf\u9a8c\u200b\u505a\u6cd5\u200b\uff1a\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u56fa\u5b9a\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u6765\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u540c\u65f6\u200b\u89c2\u5bdf\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u9519\u8bef\u7387\u200b\uff0c\u200b\u6bcf\u5f53\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u9519\u8bef\u7387\u200b\u505c\u6b62\u200b\u4e0b\u964d\u200b\uff0c\u200b\u5c31\u200b\u4e58\u4ee5\u200b\u4e00\u4e2a\u200b\u5e38\u6570\u200b\uff08\u200b\u6bd4\u5982\u200b0.5\uff09\u200b\u6765\u200b\u964d\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\u3002</li> <li>\u200b\u6307\u6570\u200b\u8870\u51cf\u200b\u3002\u200b\u6570\u5b66\u516c\u5f0f\u200b\u662f\u200b\\(\\alpha = \\alpha_0 e^{-k t}\\)\uff0c\u200b\u5176\u4e2d\u200b\\(\\alpha_0, k\\) \u200b\u662f\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\\(t\\)\u200b\u662f\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\uff08\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5468\u671f\u200b\u4f5c\u4e3a\u200b\u5355\u4f4d\u200b\uff09\u3002</li> <li>1/t\u200b\u8870\u51cf\u200b\u7684\u200b\u6570\u5b66\u516c\u5f0f\u200b\u662f\u200b\\(\\alpha = \\alpha_0 / (1 + k t )\\)\uff0c\u200b\u5176\u4e2d\u200b\\(a_0, k\\)\u200b\u662f\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\\(t\\)\u200b\u662f\u200b\u8fed\u4ee3\u200b\u6b21\u6570\u200b\u3002</li> </ul> <p>\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u968f\u200b\u6b65\u6570\u200b\u8870\u51cf\u200b\u7684\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff08dropout\uff09\u200b\u66f4\u200b\u53d7\u6b22\u8fce\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u4f7f\u7528\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u8870\u51cf\u7cfb\u6570\u200b\u548c\u200b\u4ee5\u200b\u5468\u671f\u200b\u4e3a\u200b\u65f6\u95f4\u200b\u5355\u4f4d\u200b\u7684\u200b\u6b65\u6570\u200b\uff09\u200b\u6bd4\u200b\\(k\\)\u200b\u66f4\u200b\u6709\u200b\u89e3\u91ca\u6027\u200b\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u8db3\u591f\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u8870\u51cf\u200b\u66f4\u52a0\u200b\u7f13\u6162\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u8ba9\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u66f4\u957f\u4e9b\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_12","title":"\u4e8c\u9636\u200b\u65b9\u6cd5","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u7f51\u7edc\u200b\u80cc\u666f\u200b\u4e0b\u200b\uff0c\u200b\u7b2c\u4e8c\u7c7b\u200b\u5e38\u7528\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u65b9\u6cd5\u200b\u662f\u200b\u57fa\u4e8e\u200b\u725b\u987f\u200b\u6cd5\u200b\u7684\u200b\uff0c\u200b\u5176\u200b\u8fed\u4ee3\u200b\u5982\u4e0b\u200b\uff1a</p> \\[ x \\leftarrow x - [H f(x)]^{-1} \\nabla f(x) \\] <p>\u200b\u8fd9\u91cc\u200b\\(H f(x)\\)\u200b\u662f\u200bHessian\u200b\u77e9\u9635\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u51fd\u6570\u200b\u7684\u200b\u4e8c\u9636\u200b\u504f\u200b\u5bfc\u6570\u200b\u7684\u200b\u5e73\u65b9\u200b\u77e9\u9635\u200b\u3002\\(\\nabla f(x)\\)\u200b\u662f\u200b\u68af\u5ea6\u200b\u5411\u91cf\u200b\uff0c\u200b\u8fd9\u200b\u548c\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u4e2d\u200b\u4e00\u6837\u200b\u3002\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\u4e0a\u200b\uff0cHessian\u200b\u77e9\u9635\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u5c40\u90e8\u200b\u66f2\u7387\u200b\uff0c\u200b\u4ece\u800c\u200b\u4f7f\u5f97\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u66f4\u200b\u9ad8\u6548\u200b\u7684\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b\u4e58\u4ee5\u200bHessian\u200b\u8f6c\u7f6e\u200b\u77e9\u9635\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6700\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u5728\u200b\u66f2\u7387\u200b\u5c0f\u200b\u7684\u200b\u65f6\u5019\u200b\u5927\u6b65\u200b\u524d\u8fdb\u200b\uff0c\u200b\u5728\u200b\u66f2\u7387\u200b\u5927\u200b\u7684\u200b\u65f6\u5019\u200b\u5c0f\u6b65\u200b\u524d\u8fdb\u200b\u3002\u200b\u9700\u8981\u200b\u91cd\u70b9\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u516c\u5f0f\u200b\u4e2d\u662f\u200b\u6ca1\u6709\u200b\u5b66\u4e60\u200b\u7387\u200b\u8fd9\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\uff0c\u200b\u8fd9\u76f8\u8f83\u200b\u4e8e\u200b\u4e00\u9636\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u4f18\u52bf\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\u4e0a\u8ff0\u200b\u66f4\u65b0\u200b\u65b9\u6cd5\u200b\u5f88\u96be\u200b\u8fd0\u7528\u200b\u5230\u200b\u5b9e\u9645\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e94\u7528\u200b\u4e2d\u200b\u53bb\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u8ba1\u7b97\u200b\uff08\u200b\u4ee5\u53ca\u200b\u6c42\u9006\u200b\uff09Hessian\u200b\u77e9\u9635\u200b\u64cd\u4f5c\u200b\u975e\u5e38\u200b\u8017\u8d39\u200b\u65f6\u95f4\u200b\u548c\u200b\u7a7a\u95f4\u200b\u3002\u200b\u4e3e\u4f8b\u6765\u8bf4\u200b\uff0c\u200b\u5047\u8bbe\u200b\u4e00\u4e2a\u200b\u6709\u200b\u4e00\u767e\u4e07\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5176\u200bHessian\u200b\u77e9\u9635\u200b\u5927\u5c0f\u200b\u5c31\u662f\u200b[1,000,000 x 1,000,000]\uff0c\u200b\u5c06\u200b\u5360\u7528\u200b\u5c06\u8fd1\u200b3,725GB\u200b\u7684\u200b\u5185\u5b58\u200b\u3002\u200b\u8fd9\u6837\u200b\uff0c\u200b\u5404\u79cd\u5404\u6837\u200b\u7684\u200b\u62df\u200b-\u200b\u725b\u987f\u200b\u6cd5\u200b\u5c31\u200b\u88ab\u200b\u53d1\u660e\u200b\u51fa\u6765\u200b\u7528\u4e8e\u200b\u8fd1\u4f3c\u200b\u8f6c\u7f6e\u200bHessian\u200b\u77e9\u9635\u200b\u3002\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u6700\u200b\u6d41\u884c\u200b\u7684\u200b\u662f\u200bL-BFGS\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u4f7f\u7528\u200b\u968f\u200b\u65f6\u95f4\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e2d\u200b\u7684\u200b\u4fe1\u606f\u200b\u6765\u200b\u9690\u5f0f\u200b\u5730\u200b\u8fd1\u4f3c\u200b\uff08\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\u6574\u4e2a\u200b\u77e9\u9635\u200b\u662f\u200b\u4ece\u6765\u200b\u6ca1\u6709\u200b\u88ab\u200b\u8ba1\u7b97\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5373\u4f7f\u200b\u89e3\u51b3\u200b\u4e86\u200b\u5b58\u50a8\u7a7a\u95f4\u200b\u7684\u200b\u95ee\u9898\u200b\uff0cL-BFGS\u200b\u5e94\u7528\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u52a3\u52bf\u200b\u662f\u200b\u9700\u8981\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u4e00\u822c\u200b\u5305\u542b\u200b\u51e0\u767e\u4e07\u200b\u7684\u200b\u6837\u672c\u200b\u3002\u200b\u548c\u200b\u5c0f\u6279\u91cf\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08mini-batch SGD\uff09\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8ba9\u200bL-BFGS\u200b\u5728\u200b\u5c0f\u6279\u91cf\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u8d77\u6765\u200b\u662f\u200b\u5f88\u200b\u9700\u8981\u200b\u6280\u5de7\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u662f\u200b\u7814\u7a76\u200b\u70ed\u70b9\u200b\u3002</p> <p>\u200b\u5b9e\u8df5\u200b\u3002\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200bL-BFGS\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u4e8c\u9636\u200b\u65b9\u6cd5\u200b\u5e76\u200b\u4e0d\u200b\u5e38\u89c1\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u57fa\u4e8e\u200b\uff08Nesterov\u200b\u7684\u200b\uff09\u200b\u52a8\u91cf\u200b\u66f4\u65b0\u200b\u7684\u200b\u5404\u79cd\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u65b9\u6cd5\u200b\u66f4\u52a0\u200b\u5e38\u7528\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u66f4\u52a0\u200b\u7b80\u5355\u200b\u4e14\u200b\u5bb9\u6613\u200b\u6269\u5c55\u200b\u3002</p> <p>\u200b\u53c2\u8003\u8d44\u6599\u200b\uff1a</p> <ul> <li>Large Scale Distributed Deep Networks \u200b\u4e00\u6587\u200b\u6765\u81ea\u200b\u8c37\u6b4c\u200b\u5927\u8111\u200b\u56e2\u961f\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u4e86\u200b\u5728\u200b\u5927\u89c4\u6a21\u200b\u6570\u636e\u200b\u60c5\u51b5\u200b\u4e0b\u200bL-BFGS\u200b\u548c\u200bSGD\u200b\u7b97\u6cd5\u200b\u7684\u200b\u8868\u73b0\u200b\u3002</li> <li>SFO\u200b\u7b97\u6cd5\u200b\u60f3\u8981\u200b\u628a\u200bSGD\u200b\u548c\u200bL-BFGS\u200b\u7684\u200b\u4f18\u52bf\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-3/#_13","title":"\u9010\u200b\u53c2\u6570\u200b\u9002\u5e94\u200b\u5b66\u4e60\u200b\u7387\u200b\u65b9\u6cd5","text":"<p>\u200b\u524d\u9762\u200b\u8ba8\u8bba\u200b\u7684\u200b\u6240\u6709\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u662f\u200b\u5bf9\u200b\u5b66\u4e60\u200b\u7387\u200b\u8fdb\u884c\u200b\u5168\u5c40\u200b\u5730\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5bf9\u200b\u6240\u6709\u200b\u7684\u200b\u53c2\u6570\u200b\u90fd\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\u3002\u200b\u5b66\u4e60\u200b\u7387\u8c03\u200b\u53c2\u662f\u200b\u5f88\u200b\u8017\u8d39\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5f88\u591a\u200b\u5de5\u4f5c\u200b\u6295\u5165\u200b\u5230\u200b\u53d1\u660e\u200b\u80fd\u591f\u200b\u9002\u5e94\u6027\u200b\u5730\u200b\u5bf9\u200b\u5b66\u4e60\u200b\u7387\u8c03\u200b\u53c2\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u751a\u81f3\u200b\u662f\u200b\u9010\u4e2a\u200b\u53c2\u6570\u200b\u9002\u5e94\u200b\u5b66\u4e60\u200b\u7387\u8c03\u200b\u53c2\u200b\u3002\u200b\u5f88\u591a\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u4f9d\u7136\u200b\u9700\u8981\u200b\u5176\u4ed6\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\uff0c\u200b\u4f46\u662f\u200b\u5176\u200b\u89c2\u70b9\u200b\u662f\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u5bf9\u4e8e\u200b\u66f4\u5e7f\u200b\u8303\u56f4\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u6bd4\u200b\u539f\u59cb\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u65b9\u6cd5\u200b\u6709\u200b\u66f4\u200b\u826f\u597d\u200b\u7684\u200b\u8868\u73b0\u200b\u3002\u200b\u5728\u200b\u672c\u200b\u5c0f\u8282\u200b\u6211\u4eec\u200b\u4f1a\u200b\u4ecb\u7ecd\u200b\u4e00\u4e9b\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u5e38\u7528\u200b\u9002\u5e94\u200b\u7b97\u6cd5\u200b\uff1a</p> <p>Adagrad\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7531\u200bDuchi\u200b\u7b49\u200b\u63d0\u51fa\u200b\u7684\u200b\u9002\u5e94\u6027\u200b\u5b66\u4e60\u200b\u7387\u200b\u7b97\u6cd5\u200b</p> <pre><code># \u200b\u5047\u8bbe\u200b\u6709\u200b\u68af\u5ea6\u200b\u548c\u200b\u53c2\u6570\u200b\u5411\u91cf\u200bx\ncache += dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u53d8\u91cf\u200b<code>cache</code>\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u548c\u200b\u68af\u5ea6\u200b\u77e9\u9635\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\uff0c\u200b\u8fd8\u200b\u8ddf\u8e2a\u200b\u4e86\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u7684\u200b\u5e73\u65b9\u548c\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u4e00\u4f1a\u513f\u200b\u5c06\u200b\u7528\u6765\u200b\u5f52\u4e00\u5316\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u6b65\u957f\u200b\uff0c\u200b\u5f52\u4e00\u5316\u200b\u662f\u200b\u9010\u200b\u5143\u7d20\u200b\u8fdb\u884c\u200b\u7684\u200b\u3002\u200b\u6ce8\u610f\u200b\uff0c\u200b\u63a5\u6536\u200b\u5230\u200b\u9ad8\u200b\u68af\u5ea6\u200b\u503c\u200b\u7684\u200b\u6743\u91cd\u200b\u66f4\u65b0\u200b\u7684\u200b\u6548\u679c\u200b\u88ab\u200b\u51cf\u5f31\u200b\uff0c\u200b\u800c\u200b\u63a5\u6536\u200b\u5230\u200b\u4f4e\u200b\u68af\u5ea6\u200b\u503c\u200b\u7684\u200b\u6743\u91cd\u200b\u7684\u200b\u66f4\u65b0\u200b\u6548\u679c\u200b\u5c06\u4f1a\u200b\u589e\u5f3a\u200b\u3002\u200b\u6709\u8da3\u200b\u7684\u200b\u662f\u200b\u5e73\u65b9\u6839\u200b\u7684\u200b\u64cd\u4f5c\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\uff0c\u200b\u5982\u679c\u200b\u53bb\u6389\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u7684\u200b\u8868\u73b0\u200b\u5c06\u4f1a\u200b\u7cdf\u7cd5\u200b\u5f88\u591a\u200b\u3002\u200b\u7528\u4e8e\u200b\u5e73\u6ed1\u200b\u7684\u200b\u5f0f\u5b50\u200b<code>eps</code> \uff08\u200b\u4e00\u822c\u200b\u8bbe\u200b\u4e3a\u200b1e-4\u200b\u5230\u200b1e-8\u200b\u4e4b\u95f4\u200b\uff09\u200b\u662f\u200b\u9632\u6b62\u51fa\u73b0\u200b\u9664\u4ee5\u200b0\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002Adagrad\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7f3a\u70b9\u200b\u662f\u200b\uff0c\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u5355\u8c03\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u88ab\u200b\u8bc1\u660e\u200b\u901a\u5e38\u200b\u8fc7\u4e8e\u200b\u6fc0\u8fdb\u200b\u4e14\u200b\u8fc7\u65e9\u200b\u505c\u6b62\u200b\u5b66\u4e60\u200b\u3002</p> <p>RMSprop\u3002\u200b\u662f\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u9ad8\u6548\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u516c\u5f00\u200b\u53d1\u8868\u200b\u7684\u200b\u9002\u5e94\u6027\u200b\u5b66\u4e60\u200b\u7387\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6709\u8da3\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4eba\u200b\u5728\u200b\u4ed6\u4eec\u200b\u7684\u200b\u8bba\u6587\u200b\u4e2d\u200b\u90fd\u200b\u5f15\u7528\u200b\u81ea\u200bGeoff Hinton\u200b\u7684\u200bCoursera\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u7b2c\u516d\u8bfe\u200b\u7684\u200b\u7b2c\u200b29\u200b\u9875\u200bPPT\u3002\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u7528\u200b\u4e00\u79cd\u200b\u5f88\u200b\u7b80\u5355\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4fee\u6539\u200b\u4e86\u200bAdagrad\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8ba9\u200b\u5b83\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u6fc0\u8fdb\u200b\uff0c\u200b\u5355\u8c03\u200b\u5730\u200b\u964d\u4f4e\u200b\u4e86\u200b\u5b66\u4e60\u200b\u7387\u200b\u3002\u200b\u5177\u4f53\u8bf4\u6765\u200b\uff0c\u200b\u5c31\u662f\u200b\u5b83\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u68af\u5ea6\u200b\u5e73\u65b9\u200b\u7684\u200b\u6ed1\u52a8\u200b\u5e73\u5747\u200b\uff1a</p> <pre><code>cache =  decay_rate * cache + (1 - decay_rate) * dx**2\nx += - learning_rate * dx / (np.sqrt(cache) + eps)\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0cdecay_rate\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5e38\u7528\u200b\u7684\u200b\u503c\u200b\u662f\u200b[0.9,0.99,0.999]\u3002\u200b\u5176\u4e2d\u200bx+=\u200b\u548c\u200bAdagrad\u200b\u4e2d\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\uff0c\u200b\u4f46\u662f\u200bcache\u200b\u53d8\u91cf\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0cRMSProp\u200b\u4ecd\u7136\u200b\u662f\u200b\u57fa\u4e8e\u200b\u68af\u5ea6\u200b\u7684\u200b\u5927\u5c0f\u200b\u6765\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u6743\u91cd\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\uff0c\u200b\u8fd9\u200b\u540c\u6837\u200b\u6548\u679c\u200b\u4e0d\u9519\u200b\u3002\u200b\u4f46\u662f\u200b\u548c\u200bAdagrad\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5176\u200b\u66f4\u65b0\u200b\u4e0d\u4f1a\u200b\u8ba9\u200b\u5b66\u4e60\u200b\u7387\u200b\u5355\u8c03\u200b\u53d8\u5c0f\u200b\u3002</p> <p>Adam\u3002Adam\u200b\u662f\u200b\u6700\u8fd1\u200b\u624d\u200b\u63d0\u51fa\u200b\u7684\u200b\u4e00\u79cd\u200b\u66f4\u65b0\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5b83\u200b\u770b\u8d77\u6765\u200b\u50cf\u662f\u200bRMSProp\u200b\u7684\u200b\u52a8\u91cf\u200b\u7248\u200b\u3002\u200b\u7b80\u5316\u200b\u7684\u200b\u4ee3\u7801\u200b\u662f\u200b\u4e0b\u9762\u200b\u8fd9\u6837\u200b\uff1a</p> <pre><code>m = beta1*m + (1-beta1)*dx\nv = beta2*v + (1-beta2)*(dx**2)\nx += - learning_rate * m / (np.sqrt(v) + eps)\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\u8fd9\u4e2a\u200b\u66f4\u65b0\u200b\u65b9\u6cd5\u200b\u770b\u8d77\u6765\u200b\u771f\u7684\u200b\u548c\u200bRMSProp\u200b\u5f88\u200b\u50cf\u200b\uff0c\u200b\u9664\u4e86\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u5e73\u6ed1\u200b\u7248\u200b\u7684\u200b\u68af\u5ea6\u200b<code>m</code>\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u7528\u200b\u7684\u200b\u539f\u59cb\u200b\u68af\u5ea6\u200b\u5411\u91cf\u200b<code>dx</code>\u3002\u200b\u8bba\u6587\u200b\u4e2d\u200b\u63a8\u8350\u200b\u7684\u200b\u53c2\u6570\u503c\u200b<code>eps = 1e-8</code>, <code>beta1 = 0.9</code>, <code>beta2 = 0.999</code>\u3002\u200b\u5728\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u63a8\u8350\u200bAdam\u200b\u4f5c\u4e3a\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u4e00\u822c\u800c\u8a00\u200b\u8dd1\u200b\u8d77\u6765\u200b\u6bd4\u200bRMSProp\u200b\u8981\u200b\u597d\u200b\u4e00\u70b9\u200b\u3002\u200b\u4f46\u662f\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8bd5\u8bd5\u200bSGD+Nesterov\u200b\u52a8\u91cf\u200b\u3002\u200b\u5b8c\u6574\u200b\u7684\u200bAdam\u200b\u66f4\u65b0\u200b\u7b97\u6cd5\u200b\u4e5f\u200b\u5305\u542b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u504f\u7f6e\u200b\u77eb\u6b63\u200b\u673a\u5236\u200b\uff0c\u200b\u56e0\u4e3a\u200b<code>m,v</code>\u200b\u4e24\u4e2a\u200b\u77e9\u9635\u200b\u521d\u59cb\u200b\u4e3a\u200b0\uff0c\u200b\u5728\u200b\u6ca1\u6709\u200b\u5b8c\u5168\u200b\u70ed\u8eab\u200b\u4e4b\u524d\u200b\u5b58\u5728\u200b\u504f\u5dee\u200b\uff0c\u200b\u9700\u8981\u200b\u91c7\u53d6\u200b\u4e00\u4e9b\u200b\u8865\u507f\u200b\u63aa\u65bd\u200b\u3002\u200b\u4f7f\u7528\u200b\u504f\u7f6e\u200b\u77eb\u6b63\u200b\u673a\u5236\u200b\uff0c\u200b\u66f4\u65b0\u200b\u5982\u4e0b\u200b\uff1a</p> <p><pre><code># t is your iteration counter going from 1 to infinity\nm = beta1*m + (1-beta1)*dx\nmt = m / (1-beta1**t)\nv = beta2*v + (1-beta2)*(dx**2)\nvt = v / (1-beta2**t)\nx += - learning_rate * mt / (np.sqrt(vt) + eps)\n</code></pre> \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u66f4\u65b0\u200b\u73b0\u5728\u200b\u662f\u200b\u8fed\u4ee3\u200b\u4ee5\u53ca\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8bf7\u200b\u8bfb\u8005\u200b\u53c2\u9605\u200b\u8bba\u6587\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff0c\u200b\u6216\u200b\u53c2\u9605\u200b\u8bfe\u7a0b\u200b\u5e7b\u706f\u7247\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002</p> <p>\u200b\u62d3\u5c55\u200b\u9605\u8bfb\u200b\uff1a</p> <ul> <li>Unit Tests for Stochastic Optimization\u200b\u63d0\u51fa\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u6d4b\u8bd5\u200b\u4f5c\u4e3a\u200b\u968f\u673a\u200b\u4f18\u5316\u200b\u7684\u200b\u6807\u51c6\u5316\u200b\u57fa\u51c6\u200b\u3002</li> </ul> <p> </p> <p>\u200b\u4e0a\u9762\u200b\u7684\u200b\u52a8\u753b\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u7406\u89e3\u200b\u5b66\u4e60\u200b\u7684\u200b\u52a8\u6001\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u4e0a\u8fb9\u200b\u662f\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u7b49\u9ad8\u7ebf\u56fe\u200b\uff0c\u200b\u4e0a\u9762\u200b\u8dd1\u200b\u7684\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u7b97\u6cd5\u200b\u3002\u200b\u6ce8\u610f\u200b\u57fa\u4e8e\u200b\u52a8\u91cf\u200b\u7684\u200b\u65b9\u6cd5\u200b\u51fa\u73b0\u200b\u4e86\u200b\u5c04\u200b\u504f\u200b\u4e86\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u6700\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u770b\u8d77\u6765\u200b\u50cf\u662f\u200b\u4e00\u4e2a\u200b\u7403\u6eda\u200b\u4e0b\u5c71\u200b\u7684\u200b\u6837\u5b50\u200b\u3002\u200b\u4e0b\u8fb9\u200b\u5c55\u793a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u9a6c\u978d\u200b\u72b6\u200b\u7684\u200b\u6700\u4f18\u5316\u200b\u5730\u5f62\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5bf9\u4e8e\u200b\u4e0d\u540c\u200b\u7ef4\u5ea6\u200b\u5b83\u200b\u7684\u200b\u66f2\u7387\u200b\u4e0d\u540c\u200b\uff08\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e0b\u964d\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e0a\u5347\u200b\uff09\u3002\u200b\u6ce8\u610f\u200bSGD\u200b\u5f88\u96be\u200b\u7a81\u7834\u200b\u5bf9\u79f0\u6027\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5361\u200b\u5728\u200b\u9876\u90e8\u200b\u3002\u200b\u800c\u200bRMSProp\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u65b9\u6cd5\u200b\u80fd\u591f\u200b\u770b\u5230\u200b\u9a6c\u978d\u200b\u65b9\u5411\u200b\u6709\u200b\u5f88\u200b\u4f4e\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u56e0\u4e3a\u200b\u5728\u200bRMSProp\u200b\u66f4\u65b0\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u7684\u200b\u5206\u6bcd\u200b\u9879\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u5728\u200b\u8be5\u200b\u65b9\u5411\u200b\u7684\u200b\u6709\u6548\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u4f7f\u5f97\u200bRMSProp\u200b\u80fd\u591f\u200b\u7ee7\u7eed\u524d\u8fdb\u200b\u3002\u200b\u56fe\u7247\u200b\u7248\u6743\u200b\uff1aAlec Radford\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_14","title":"\u8d85\u200b\u53c2\u6570\u200b\u8c03\u4f18","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4f1a\u200b\u9047\u5230\u200b\u5f88\u591a\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u6709\u200b\uff1a</p> <ul> <li>\u200b\u521d\u59cb\u200b\u5b66\u4e60\u200b\u7387\u200b\u3002</li> <li>\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u65b9\u5f0f\u200b\uff08\u200b\u4f8b\u5982\u200b\u4e00\u4e2a\u200b\u8870\u51cf\u5e38\u91cf\u200b\uff09\u3002</li> <li>\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\uff08L2\u200b\u60e9\u7f5a\u200b\uff0c\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\u5f3a\u5ea6\u200b\uff09\u3002</li> </ul> <p>\u200b\u4f46\u662f\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u8fd8\u6709\u200b\u5f88\u591a\u200b\u76f8\u5bf9\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u654f\u611f\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u6bd4\u5982\u200b\u5728\u200b\u9010\u200b\u53c2\u6570\u200b\u9002\u5e94\u200b\u5b66\u4e60\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u52a8\u91cf\u200b\u53ca\u5176\u200b\u65f6\u95f4\u8868\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u7b49\u200b\u3002\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u5c06\u200b\u4ecb\u7ecd\u200b\u4e00\u4e9b\u200b\u989d\u5916\u200b\u7684\u200b\u8c03\u53c2\u200b\u8981\u70b9\u200b\u548c\u200b\u6280\u5de7\u200b\uff1a</p> <p>\u200b\u5b9e\u73b0\u200b\u3002\u200b\u66f4\u5927\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9700\u8981\u200b\u66f4\u957f\u200b\u7684\u200b\u65f6\u95f4\u200b\u53bb\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8c03\u53c2\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u51e0\u5929\u200b\u751a\u81f3\u200b\u51e0\u5468\u200b\u3002\u200b\u8bb0\u4f4f\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u4f1a\u200b\u5f71\u54cd\u200b\u4f60\u200b\u8bbe\u8ba1\u200b\u4ee3\u7801\u200b\u7684\u200b\u601d\u8def\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5177\u4f53\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u662f\u200b\u7528\u200b\u4ec6\u200b\u7a0b\u5e8f\u200bworker\u200b\u6301\u7eed\u200b\u5730\u200b\u968f\u673a\u200b\u8bbe\u7f6e\u200b\u53c2\u6570\u200b\u7136\u540e\u200b\u8fdb\u884c\u200b\u6700\u4f18\u5316\u200b\u3002\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4ec6\u200b\u7a0b\u5e8f\u200bworker\u200b\u4f1a\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u540e\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u8fdb\u884c\u200b\u76d1\u63a7\u200b\uff0c\u200b\u7136\u540e\u200b\u5411\u200b\u6587\u4ef6\u7cfb\u7edf\u200b\u5199\u4e0b\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u8bb0\u5f55\u200b\u70b9\u200bcheckpoint\uff08\u200b\u8bb0\u5f55\u200b\u70b9\u4e2d\u200b\u6709\u200b\u5404\u79cd\u5404\u6837\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7edf\u8ba1\u6570\u636e\u200b\uff0c\u200b\u6bd4\u5982\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u53d8\u5316\u200b\u7b49\u200b\uff09\uff0c\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u7cfb\u7edf\u200b\u6700\u597d\u200b\u662f\u200b\u53ef\u200b\u5171\u4eab\u200b\u7684\u200b\u3002\u200b\u5728\u200b\u6587\u4ef6\u540d\u200b\u4e2d\u200b\u6700\u597d\u200b\u5305\u542b\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u7684\u200b\u7b97\u6cd5\u200b\u8868\u73b0\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5c31\u200b\u80fd\u200b\u65b9\u4fbf\u200b\u5730\u200b\u67e5\u627e\u200b\u548c\u200b\u6392\u5e8f\u200b\u4e86\u200b\u3002\u200b\u7136\u540e\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u4e3b\u7a0b\u5e8f\u200bmaster\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u542f\u52a8\u200b\u6216\u8005\u200b\u7ed3\u675f\u200b\u8ba1\u7b97\u200b\u96c6\u7fa4\u200b\u4e2d\u200b\u7684\u200b\u4ec6\u200b\u7a0b\u5e8f\u200bworker\uff0c\u200b\u6709\u65f6\u5019\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u6839\u636e\u200b\u6761\u4ef6\u200b\u67e5\u770b\u200b\u4ec6\u200b\u7a0b\u5e8f\u200bworker\u200b\u5199\u4e0b\u200b\u7684\u200b\u8bb0\u5f55\u200b\u70b9\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5b83\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7edf\u8ba1\u6570\u636e\u200b\u7b49\u200b\u3002</p> <p>\u200b\u6bd4\u8d77\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u6700\u597d\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u3002\u200b\u5728\u200b\u5927\u591a\u6570\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5c3a\u5bf8\u200b\u5408\u7406\u200b\u7684\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u4ee3\u7801\u200b\u66f4\u200b\u7b80\u5355\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u7528\u200b\u51e0\u4e2a\u200b\u6570\u636e\u200b\u96c6\u6765\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u3002\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u542c\u5230\u200b\u4eba\u4eec\u200b\u8bf4\u200b\u4ed6\u4eec\u200b\u201c\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u201d\u200b\u4e00\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u662f\u200b\u5927\u591a\u6570\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u5b9e\u9645\u200b\u662f\u200b\u4f7f\u7528\u200b\u7684\u200b\u4e00\u4e2a\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u3002</p> <p>\u200b\u8d85\u200b\u53c2\u6570\u200b\u8303\u56f4\u200b\u3002\u200b\u5728\u200b\u5bf9\u6570\u200b\u5c3a\u5ea6\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8d85\u200b\u53c2\u6570\u200b\u641c\u7d22\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5178\u578b\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u5e94\u8be5\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u8fd9\u6837\u200b\uff1a<code>learning_rate = 10 ** uniform(-6, 1)</code>\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ece\u200b\u6807\u51c6\u200b\u5206\u5e03\u200b\u4e2d\u200b\u968f\u673a\u200b\u751f\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\uff0c\u200b\u7136\u540e\u200b\u8ba9\u200b\u5b83\u200b\u6210\u4e3a\u200b10\u200b\u7684\u200b\u9636\u6570\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u540c\u6837\u200b\u7684\u200b\u7b56\u7565\u200b\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u200b\u548c\u200b\u6b63\u5219\u200b\u5316\u200b\u5f3a\u5ea6\u200b\u90fd\u200b\u5bf9\u4e8e\u200b\u8bad\u7ec3\u200b\u7684\u200b\u52a8\u6001\u200b\u8fdb\u7a0b\u200b\u6709\u200b\u4e58\u200b\u7684\u200b\u6548\u679c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff1a\u200b\u5f53\u200b\u5b66\u4e60\u200b\u7387\u200b\u662f\u200b0.001\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5982\u679c\u200b\u5bf9\u200b\u5176\u200b\u56fa\u5b9a\u200b\u5730\u200b\u589e\u52a0\u200b0.01\uff0c\u200b\u90a3\u4e48\u200b\u5bf9\u4e8e\u200b\u5b66\u4e60\u200b\u8fdb\u7a0b\u200b\u4f1a\u200b\u6709\u200b\u5f88\u5927\u200b\u5f71\u54cd\u200b\u3002\u200b\u7136\u800c\u200b\u5f53\u200b\u5b66\u4e60\u200b\u7387\u200b\u662f\u200b10\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u5f71\u54cd\u200b\u5c31\u200b\u5fae\u4e4e\u5176\u5fae\u200b\u4e86\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u56e0\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u200b\u4e58\u4ee5\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6bd4\u8d77\u200b\u52a0\u4e0a\u200b\u6216\u8005\u200b\u51cf\u5c11\u200b\u67d0\u4e9b\u200b\u503c\u200b\uff0c\u200b\u601d\u8003\u200b\u5b66\u4e60\u200b\u7387\u200b\u7684\u200b\u8303\u56f4\u200b\u662f\u200b\u4e58\u4ee5\u200b\u6216\u8005\u200b\u9664\u4ee5\u200b\u67d0\u4e9b\u200b\u503c\u200b\u66f4\u52a0\u200b\u81ea\u7136\u200b\u3002\u200b\u4f46\u662f\u200b\u6709\u200b\u4e00\u4e9b\u200b\u53c2\u6570\u200b\uff08\u200b\u6bd4\u5982\u200b\u968f\u673a\u200b\u5931\u6d3b\u200b\uff09\u200b\u8fd8\u662f\u200b\u5728\u200b\u539f\u59cb\u200b\u5c3a\u5ea6\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u641c\u7d22\u200b\uff08\u200b\u4f8b\u5982\u200b\uff1a<code>dropout = uniform(0,1)</code>\uff09\u3002</p> <p>\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\u4f18\u4e8e\u200b\u7f51\u683c\u200b\u641c\u7d22\u200b\u3002Bergstra\u200b\u548c\u200bBengio\u200b\u5728\u200b\u6587\u7ae0\u200bRandom Search for Hyper-Parameter Optimization\u200b\u4e2d\u8bf4\u200b\u201c\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u6bd4\u200b\u7f51\u683c\u5316\u200b\u7684\u200b\u9009\u62e9\u200b\u66f4\u52a0\u200b\u6709\u6548\u200b\u201d\uff0c\u200b\u800c\u4e14\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u4e5f\u200b\u66f4\u200b\u5bb9\u6613\u200b\u5b9e\u73b0\u200b\u3002</p> <p> </p> <p>\u200b\u5728\u200bRandom Search for Hyper-Parameter Optimization\u200b\u4e2d\u200b\u7684\u200b\u6838\u5fc3\u200b\u8bf4\u660e\u200b\u56fe\u200b\u3002\u200b\u901a\u5e38\u200b\uff0c\u200b\u6709\u4e9b\u200b\u8d85\u200b\u53c2\u6570\u200b\u6bd4\u200b\u5176\u4f59\u200b\u7684\u200b\u66f4\u200b\u91cd\u8981\u200b\uff0c\u200b\u901a\u8fc7\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u7f51\u683c\u5316\u200b\u7684\u200b\u641c\u7d22\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u4f60\u200b\u66f4\u200b\u7cbe\u786e\u200b\u5730\u200b\u53d1\u73b0\u200b\u90a3\u4e9b\u200b\u6bd4\u8f83\u200b\u91cd\u8981\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u597d\u200b\u6570\u503c\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u8fb9\u754c\u200b\u4e0a\u200b\u7684\u200b\u6700\u4f18\u200b\u503c\u8981\u200b\u5c0f\u5fc3\u200b\u3002\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e00\u822c\u200b\u53d1\u751f\u200b\u5728\u200b\u4f60\u200b\u5728\u200b\u4e00\u4e2a\u200b\u4e0d\u597d\u200b\u7684\u200b\u8303\u56f4\u200b\u5185\u200b\u641c\u7d22\u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u6bd4\u5982\u200b\u5b66\u4e60\u200b\u7387\u200b\uff09\u200b\u7684\u200b\u65f6\u5019\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b<code>learning_rate = 10 ** uniform(-6, 1)</code>\u200b\u6765\u200b\u8fdb\u884c\u200b\u641c\u7d22\u200b\u3002\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u6bd4\u8f83\u200b\u597d\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u4e00\u5b9a\u200b\u8981\u200b\u786e\u8ba4\u200b\u4f60\u200b\u7684\u200b\u503c\u200b\u4e0d\u662f\u200b\u51fa\u4e8e\u200b\u8fd9\u4e2a\u200b\u8303\u56f4\u200b\u7684\u200b\u8fb9\u754c\u200b\u4e0a\u200b\uff0c\u200b\u4e0d\u7136\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9519\u8fc7\u200b\u66f4\u597d\u200b\u7684\u200b\u5176\u4ed6\u200b\u641c\u7d22\u200b\u8303\u56f4\u200b\u3002</p> <p>\u200b\u4ece\u200b\u7c97\u5230\u200b\u7ec6\u5730\u200b\u5206\u9636\u6bb5\u200b\u641c\u7d22\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u5148\u200b\u8fdb\u884c\u200b\u521d\u7565\u200b\u8303\u56f4\u200b\uff08\u200b\u6bd4\u5982\u200b10 ** [-6, 1]\uff09\u200b\u641c\u7d22\u200b\uff0c\u200b\u7136\u540e\u200b\u6839\u636e\u200b\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u51fa\u73b0\u200b\u7684\u200b\u5730\u65b9\u200b\uff0c\u200b\u7f29\u5c0f\u200b\u8303\u56f4\u200b\u8fdb\u884c\u200b\u641c\u7d22\u200b\u3002\u200b\u8fdb\u884c\u200b\u7c97\u200b\u641c\u7d22\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u8ba9\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5468\u671f\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e86\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5f88\u591a\u200b\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u8bbe\u5b9a\u200b\u4f1a\u200b\u8ba9\u200b\u6a21\u578b\u200b\u6ca1\u6cd5\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6216\u8005\u200b\u7a81\u7136\u200b\u5c31\u200b\u7206\u51fa\u200b\u5f88\u5927\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u3002\u200b\u7b2c\u4e8c\u4e2a\u200b\u9636\u6bb5\u200b\u5c31\u662f\u200b\u5bf9\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u8303\u56f4\u200b\u8fdb\u884c\u200b\u641c\u7d22\u200b\uff0c\u200b\u8fd9\u65f6\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b5\u200b\u4e2a\u200b\u5468\u671f\u200b\uff0c\u200b\u800c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u9636\u6bb5\u200b\u5c31\u200b\u5728\u200b\u6700\u7ec8\u200b\u7684\u200b\u8303\u56f4\u200b\u5185\u200b\u8fdb\u884c\u200b\u4ed4\u7ec6\u641c\u7d22\u200b\uff0c\u200b\u8fd0\u884c\u200b\u5f88\u200b\u591a\u6b21\u200b\u5468\u671f\u200b\u3002</p> <p>\u200b\u8d1d\u53f6\u65af\u200b\u8d85\u200b\u53c2\u6570\u200b\u6700\u4f18\u5316\u200b\u662f\u200b\u4e00\u200b\u6574\u4e2a\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\uff0c\u200b\u4e3b\u8981\u200b\u662f\u200b\u7814\u7a76\u200b\u5728\u200b\u8d85\u200b\u53c2\u6570\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u66f4\u200b\u9ad8\u6548\u200b\u7684\u200b\u5bfc\u822a\u200b\u7b97\u6cd5\u200b\u3002\u200b\u5176\u200b\u6838\u5fc3\u200b\u7684\u200b\u601d\u8def\u200b\u662f\u200b\u5728\u200b\u4e0d\u540c\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e0b\u200b\u67e5\u770b\u200b\u7b97\u6cd5\u200b\u6027\u80fd\u200b\u65f6\u200b\uff0c\u200b\u8981\u200b\u5728\u200b\u63a2\u7d22\u200b\u548c\u200b\u4f7f\u7528\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u5408\u7406\u200b\u7684\u200b\u6743\u8861\u200b\u3002\u200b\u57fa\u4e8e\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\uff0c\u200b\u53d1\u5c55\u200b\u51fa\u200b\u5f88\u591a\u200b\u7684\u200b\u5e93\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u6709\u540d\u200b\u7684\u200b\u6709\u200b\uff1a Spearmint, SMAC, \u200b\u548c\u200bHyperopt\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6bd4\u8d77\u200b\u4e0a\u9762\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u5148\u200b\u8ba4\u771f\u200b\u6311\u9009\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8303\u56f4\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u8be5\u200b\u8303\u56f4\u200b\u5185\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u8fd8\u662f\u200b\u5dee\u200b\u4e00\u4e9b\u200b\u3002\u200b\u8fd9\u91cc\u200b\u6709\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u7684\u200b\u8ba8\u8bba\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#evaluation","title":"\u8bc4\u4f30\u200b Evaluation","text":""},{"location":"courses/neural_network/neural-networks-3/#model-ensembles","title":"\u6a21\u578b\u200b\u96c6\u6210\u200b Model Ensembles","text":"<p>\u200b\u5728\u200b\u5b9e\u8df5\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e2a\u200b\u603b\u662f\u200b\u80fd\u200b\u63d0\u5347\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u51e0\u4e2a\u200b\u767e\u5206\u70b9\u200b\u51c6\u786e\u7387\u200b\u7684\u200b\u529e\u6cd5\u200b\uff0c\u200b\u5c31\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\u8bad\u7ec3\u200b\u51e0\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u65f6\u5019\u200b\u5e73\u5747\u200b\u5b83\u4eec\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002\u200b\u96c6\u6210\u200b\u7684\u200b\u6a21\u578b\u200b\u6570\u91cf\u200b\u589e\u52a0\u200b\uff0c\u200b\u7b97\u6cd5\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e5f\u200b\u5355\u8c03\u200b\u63d0\u5347\u200b\uff08\u200b\u4f46\u200b\u63d0\u5347\u200b\u6548\u679c\u200b\u8d8a\u6765\u8d8a\u5c11\u200b\uff09\u3002\u200b\u8fd8\u6709\u200b\u6a21\u578b\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u5ea6\u8d8a\u200b\u5927\u200b\uff0c\u200b\u63d0\u5347\u200b\u6548\u679c\u200b\u53ef\u80fd\u200b\u8d8a\u200b\u597d\u200b\u3002\u200b\u8fdb\u884c\u200b\u96c6\u6210\u200b\u6709\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\uff1a</p> <ul> <li>\u200b\u540c\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e0d\u540c\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u3002\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u6765\u200b\u5f97\u5230\u200b\u6700\u597d\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u7528\u200b\u6700\u597d\u200b\u7684\u200b\u53c2\u6570\u200b\u6765\u200b\u8bad\u7ec3\u200b\u4e0d\u540c\u200b\u521d\u59cb\u5316\u200b\u6761\u4ef6\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u7684\u200b\u98ce\u9669\u200b\u5728\u4e8e\u200b\u591a\u6837\u6027\u200b\u53ea\u200b\u6765\u81ea\u200b\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u521d\u59cb\u5316\u200b\u6761\u4ef6\u200b\u3002</li> <li>\u200b\u5728\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u6765\u200b\u5f97\u5230\u200b\u6700\u597d\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u53d6\u200b\u5176\u4e2d\u200b\u6700\u597d\u200b\u7684\u200b\u51e0\u4e2a\u200b\uff08\u200b\u6bd4\u5982\u200b10\u200b\u4e2a\u200b\uff09\u200b\u6a21\u578b\u200b\u6765\u200b\u8fdb\u884c\u200b\u96c6\u6210\u200b\u3002\u200b\u8fd9\u6837\u200b\u5c31\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u96c6\u6210\u200b\u7684\u200b\u591a\u6837\u6027\u200b\uff0c\u200b\u4f46\u200b\u98ce\u9669\u200b\u5728\u4e8e\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5305\u542b\u200b\u4e0d\u591f\u200b\u7406\u60f3\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u6837\u200b\u64cd\u4f5c\u200b\u8d77\u6765\u200b\u6bd4\u8f83\u7b80\u5355\u200b\uff0c\u200b\u5728\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u540e\u200b\u5c31\u200b\u4e0d\u200b\u9700\u8981\u200b\u989d\u5916\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u591a\u4e2a\u200b\u8bb0\u5f55\u200b\u70b9\u200b\u3002\u200b\u5982\u679c\u200b\u8bad\u7ec3\u200b\u975e\u5e38\u200b\u8017\u65f6\u200b\uff0c\u200b\u90a3\u200b\u5c31\u200b\u5728\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u5bf9\u200b\u7f51\u7edc\u200b\u7559\u4e0b\u200b\u8bb0\u5f55\u200b\u70b9\u200b\uff08\u200b\u6bd4\u5982\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u7ed3\u675f\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u7528\u200b\u5b83\u4eec\u200b\u6765\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\u3002\u200b\u5f88\u200b\u663e\u7136\u200b\uff0c\u200b\u8fd9\u6837\u200b\u505a\u200b\u591a\u6837\u6027\u200b\u4e0d\u8db3\u200b\uff0c\u200b\u4f46\u662f\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u6548\u679c\u200b\u8fd8\u662f\u200b\u4e0d\u9519\u200b\u7684\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u7684\u200b\u4f18\u52bf\u200b\u662f\u200b\u4ee3\u4ef7\u200b\u6bd4\u8f83\u200b\u5c0f\u200b\u3002</li> <li>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\u8dd1\u200b\u53c2\u6570\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u3002\u200b\u548c\u200b\u4e0a\u9762\u200b\u4e00\u70b9\u200b\u76f8\u5173\u200b\u7684\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e00\u4e2a\u200b\u4e5f\u200b\u80fd\u200b\u5f97\u5230\u200b1-2\u200b\u4e2a\u200b\u767e\u5206\u70b9\u200b\u7684\u200b\u63d0\u5347\u200b\u7684\u200b\u5c0f\u200b\u4ee3\u4ef7\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u5c31\u662f\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u5982\u679c\u200b\u635f\u5931\u200b\u503c\u200b\u76f8\u8f83\u200b\u4e8e\u200b\u524d\u200b\u4e00\u6b21\u200b\u6743\u91cd\u200b\u51fa\u73b0\u200b\u6307\u6570\u200b\u4e0b\u964d\u65f6\u200b\uff0c\u200b\u5c31\u200b\u5728\u200b\u5185\u5b58\u200b\u4e2d\u200b\u5bf9\u200b\u7f51\u7edc\u200b\u7684\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u5907\u4efd\u200b\u3002\u200b\u8fd9\u6837\u200b\u4f60\u200b\u5c31\u200b\u5bf9\u200b\u524d\u51e0\u6b21\u200b\u5faa\u73af\u200b\u4e2d\u200b\u7684\u200b\u7f51\u7edc\u200b\u72b6\u6001\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5e73\u5747\u200b\u3002\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u8fd9\u4e2a\u200b\u201c\u200b\u5e73\u6ed1\u200b\u201d\u200b\u8fc7\u200b\u7684\u200b\u7248\u672c\u200b\u7684\u200b\u6743\u91cd\u200b\u603b\u662f\u200b\u80fd\u200b\u5f97\u5230\u200b\u66f4\u5c11\u200b\u7684\u200b\u8bef\u5dee\u200b\u3002\u200b\u76f4\u89c2\u200b\u7684\u200b\u7406\u89e3\u200b\u5c31\u662f\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u662f\u200b\u4e00\u4e2a\u7897\u200b\u72b6\u200b\u7684\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u7f51\u7edc\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u5468\u56f4\u200b\u8df3\u8dc3\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u5e73\u5747\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5c31\u200b\u66f4\u200b\u53ef\u80fd\u200b\u8df3\u200b\u5230\u200b\u4e2d\u5fc3\u200b\u53bb\u200b\u3002</li> </ul> <p>\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\u7684\u200b\u4e00\u4e2a\u200b\u52a3\u52bf\u200b\u5c31\u662f\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u7684\u200b\u65f6\u5019\u200b\u4f1a\u200b\u82b1\u8d39\u200b\u66f4\u200b\u591a\u200b\u65f6\u95f4\u200b\u3002\u200b\u6700\u8fd1\u200bGeoff Hinton\u200b\u5728\u200b\u201cDark Knowledge\u201d\u200b\u4e0a\u200b\u7684\u200b\u5de5\u4f5c\u200b\u5f88\u200b\u6709\u200b\u542f\u53d1\u200b\uff1a\u200b\u5176\u200b\u601d\u8def\u200b\u662f\u200b\u901a\u8fc7\u200b\u5c06\u200b\u96c6\u6210\u200b\u4f3c\u7136\u200b\u4f30\u8ba1\u200b\u7eb3\u5165\u200b\u5230\u200b\u4fee\u6539\u200b\u7684\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u96c6\u6210\u200b\u4e2d\u200b\u62bd\u51fa\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"courses/neural_network/neural-networks-3/#_15","title":"\u603b\u7ed3","text":"<p>\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u9700\u8981\u200b\uff1a</p> <ul> <li>\u200b\u5229\u7528\u200b\u5c0f\u6279\u91cf\u200b\u6570\u636e\u200b\u5bf9\u200b\u5b9e\u73b0\u200b\u8fdb\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\uff0c\u200b\u8fd8\u8981\u200b\u6ce8\u610f\u200b\u5404\u79cd\u200b\u9519\u8bef\u200b\u3002</li> <li>\u200b\u8fdb\u884c\u200b\u5408\u7406\u6027\u200b\u68c0\u67e5\u200b\uff0c\u200b\u786e\u8ba4\u200b\u521d\u59cb\u200b\u635f\u5931\u200b\u503c\u200b\u662f\u200b\u5408\u7406\u200b\u7684\u200b\uff0c\u200b\u5728\u200b\u5c0f\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u80fd\u200b\u5f97\u5230\u200b100%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</li> <li>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u8ddf\u8e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u5982\u679c\u200b\u613f\u610f\u200b\uff0c\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8ddf\u8e2a\u200b\u66f4\u65b0\u200b\u7684\u200b\u53c2\u200b\u6570\u91cf\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u603b\u53c2\u200b\u6570\u91cf\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff08\u200b\u4e00\u822c\u200b\u5728\u200b1e-3\u200b\u5de6\u53f3\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u5982\u679c\u200b\u662f\u200b\u5bf9\u4e8e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u7b2c\u4e00\u5c42\u200b\u7684\u200b\u6743\u91cd\u200b\u53ef\u89c6\u5316\u200b\u3002</li> <li>\u200b\u63a8\u8350\u200b\u7684\u200b\u4e24\u4e2a\u200b\u66f4\u65b0\u200b\u65b9\u6cd5\u200b\u662f\u200bSGD+Nesterov\u200b\u52a8\u91cf\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6216\u8005\u200bAdam\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200b\u8fdb\u884c\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u3002\u200b\u6bd4\u5982\u200b\uff0c\u200b\u5728\u200b\u56fa\u5b9a\u200b\u591a\u5c11\u200b\u4e2a\u200b\u5468\u671f\u200b\u540e\u200b\u8ba9\u200b\u5b66\u4e60\u200b\u7387\u200b\u51cf\u534a\u200b\uff0c\u200b\u6216\u8005\u200b\u5f53\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\u51c6\u786e\u7387\u200b\u4e0b\u964d\u200b\u7684\u200b\u65f6\u5019\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\uff08\u200b\u4e0d\u8981\u200b\u7528\u200b\u7f51\u683c\u200b\u641c\u7d22\u200b\uff09\u200b\u6765\u200b\u641c\u7d22\u200b\u6700\u4f18\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002\u200b\u5206\u9636\u6bb5\u200b\u4ece\u200b\u7c97\u200b\uff08\u200b\u6bd4\u8f83\u200b\u5bbd\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u8303\u56f4\u200b\u8bad\u7ec3\u200b1-5\u200b\u4e2a\u200b\u5468\u671f\u200b\uff09\u200b\u5230\u200b\u7ec6\u200b\uff08\u200b\u7a84\u200b\u8303\u56f4\u200b\u8bad\u7ec3\u200b\u5f88\u591a\u200b\u4e2a\u200b\u5468\u671f\u200b\uff09\u200b\u5730\u6765\u200b\u641c\u7d22\u200b\u3002</li> <li>\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u96c6\u6210\u200b\u6765\u200b\u83b7\u5f97\u200b\u989d\u5916\u200b\u7684\u200b\u6027\u80fd\u200b\u63d0\u9ad8\u200b\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-3/#_16","title":"\u62d3\u5c55\u200b\u9605\u8bfb","text":"<ul> <li>Leon Bottou\u200b\u7684\u200b\u300aSGD\u200b\u8981\u70b9\u200b\u548c\u200b\u6280\u5de7\u200b\u300b\u3002</li> <li>Yann LeCun\u200b\u7684\u200b\u300a\u200b\u9ad8\u6548\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u300b\u3002</li> <li>Yoshua Bengio\u200b\u7684\u200b\u300a\u200b\u57fa\u4e8e\u200b\u68af\u5ea6\u200b\u7684\u200b\u6df1\u5c42\u200b\u67b6\u6784\u200b\u8bad\u7ec3\u200b\u5b9e\u7528\u200b\u5efa\u8bae\u200b\u300b\u3002</li> </ul>"},{"location":"courses/neural_network/neural-networks-case-study/","title":"\u6700\u5c0f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6848\u4f8b\u200b\u7814\u7a76","text":"<p>Table of Contents:</p> <ul> <li>Generating some data</li> <li>Training a Softmax Linear Classifier</li> <li>Initialize the parameters</li> <li>Compute the class scores</li> <li>Compute the loss</li> <li>Computing the analytic gradient with backpropagation</li> <li>Performing a parameter update</li> <li>Putting it all together: Training a Softmax Classifier</li> <li>Training a Neural Network</li> <li>Summary</li> </ul> <p>In this section we'll walk through a complete implementation of a toy Neural Network in 2 dimensions. We'll first implement a simple linear classifier and then extend the code to a 2-layer Neural Network. As we'll see, this extension is surprisingly simple and very few changes are necessary.</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#generating-some-data","title":"Generating some data","text":"<p>Lets generate a classification dataset that is not easily linearly separable. Our favorite example is the spiral dataset, which can be generated as follows:</p> <pre><code>N = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# lets visualize the data:\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</code></pre>      The toy spiral data consists of three classes (blue, red, yellow) that are not linearly separable.    <p>Normally we would want to preprocess the dataset so that each feature has zero mean and unit standard deviation, but in this case the features are already in a nice range from -1 to 1, so we skip this step.</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#training-a-softmax-linear-classifier","title":"Training a Softmax Linear Classifier","text":""},{"location":"courses/neural_network/neural-networks-case-study/#initialize-the-parameters","title":"Initialize the parameters","text":"<p>Lets first train a Softmax classifier on this classification dataset. As we saw in the previous sections, the Softmax classifier has a linear score function and uses the cross-entropy loss. The parameters of the linear classifier consist of a weight matrix <code>W</code> and a bias vector <code>b</code> for each class. Lets first initialize these parameters to be random numbers:</p> <pre><code># initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n</code></pre> <p>Recall that we <code>D = 2</code> is the dimensionality and <code>K = 3</code> is the number of classes.</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#compute-the-class-scores","title":"Compute the class scores","text":"<p>Since this is a linear classifier, we can compute all class scores very simply in parallel with a single matrix multiplication:</p> <pre><code># compute class scores for a linear classifier\nscores = np.dot(X, W) + b\n</code></pre> <p>In this example we have 300 2-D points, so after this multiplication the array <code>scores</code> will have size [300 x 3], where each row gives the class scores corresponding to the 3 classes (blue, red, yellow).</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#compute-the-loss","title":"Compute the loss","text":"<p>The second key ingredient we need is a loss function, which is a differentiable objective that quantifies our unhappiness with the computed class scores. Intuitively, we want the correct class to have a higher score than the other classes. When this is the case, the loss should be low and otherwise the loss should be high. There are many ways to quantify this intuition, but in this example lets use the cross-entropy loss that is associated with the Softmax classifier. Recall that if \\(f\\) is the array of class scores for a single example (e.g. array of 3 numbers here), then the Softmax classifier computes the loss for that example as:</p> \\[ L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\] <p>We can see that the Softmax classifier interprets every element of \\(f\\) as holding the (unnormalized) log probabilities of the three classes. We exponentiate these to get (unnormalized) probabilities, and then normalize them to get probabilites. Therefore, the expression inside the log is the normalized probability of the correct class. Note how this expression works: this quantity is always between 0 and 1. When the probability of the correct class is very small (near 0), the loss will go towards (positive) infinity. Conversely, when the correct class probability goes towards 1, the loss will go towards zero because \\(log(1) = 0\\). Hence, the expression for \\(L_i\\) is low when the correct class probability is high, and it's very high when it is low.</p> <p>Recall also that the full Softmax classifier loss is then defined as the average cross-entropy loss over the training examples and the regularization:</p> \\[ L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\frac{1}{2} \\lambda \\sum_k\\sum_l W_{k,l}^2 }_\\text{regularization loss} \\\\\\\\ \\] <p>Given the array of <code>scores</code> we've computed above, we can compute the loss. First, the way to obtain the probabilities is straight forward:</p> <pre><code>num_examples = X.shape[0]\n# get unnormalized probabilities\nexp_scores = np.exp(scores)\n# normalize them for each example\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n</code></pre> <p>We now have an array <code>probs</code> of size [300 x 3], where each row now contains the class probabilities. In particular, since we've normalized them every row now sums to one. We can now query for the  log probabilities assigned to the correct classes in each example:</p> <pre><code>correct_logprobs = -np.log(probs[range(num_examples),y])\n</code></pre> <p>The array <code>correct_logprobs</code> is a 1D array of just the probabilities assigned to the correct classes for each example. The full loss is then the average of these log probabilities and the regularization loss:</p> <pre><code># compute the loss: average cross-entropy loss and regularization\ndata_loss = np.sum(correct_logprobs)/num_examples\nreg_loss = 0.5*reg*np.sum(W*W)\nloss = data_loss + reg_loss\n</code></pre> <p>In this code, the regularization strength \\(\\lambda\\) is stored inside the <code>reg</code>. The convenience factor of <code>0.5</code> multiplying the regularization will become clear in a second. Evaluating this in the beginning (with random parameters) might give us <code>loss = 1.1</code>, which is <code>-np.log(1.0/3)</code>, since with small initial random weights all probabilities assigned to all classes are about one third. We now want to make the loss as low as possible, with <code>loss = 0</code> as the absolute lower bound. But the lower the loss is, the higher are the probabilities assigned to the correct classes for all examples.</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#computing-the-analytic-gradient-with-backpropagation","title":"Computing the Analytic Gradient with Backpropagation","text":"<p>We have a way of evaluating the loss, and now we have to minimize it. We'll do so with gradient descent. That is, we start with random parameters (as shown above), and evaluate the gradient of the loss function with respect to the parameters, so that we know how we should change the parameters to decrease the loss. Lets introduce the intermediate variable \\(p\\), which is a vector of the (normalized) probabilities. The loss for one example is:</p> \\[ p_k = \\frac{e^{f_k}}{ \\sum_j e^{f_j} } \\hspace{1in} L_i =-\\log\\left(p_{y_i}\\right) \\] <p>We now wish to understand how the computed scores inside \\(f\\) should change to decrease the loss \\(L_i\\) that this example contributes to the full objective. In other words, we want to derive the gradient $ \\partial L_i / \\partial f_k $. The loss \\(L_i\\) is computed from \\(p\\), which in turn depends on \\(f\\). It's a fun exercise to the reader to use the chain rule to derive the gradient, but it turns out to be extremely simple and interpretible in the end, after a lot of things cancel out:</p> \\[ \\frac{\\partial L_i }{ \\partial f_k } = p_k - \\mathbb{1}(y_i = k) \\] <p>Notice how elegant and simple this expression is. Suppose the probabilities we computed were <code>p = [0.2, 0.3, 0.5]</code>, and that the correct class was the middle one (with probability 0.3). According to this derivation the gradient on the scores would be <code>df = [0.2, -0.7, 0.5]</code>. Recalling what the interpretation of the gradient, we see that this result is highly intuitive: increasing the first or last element of the score vector <code>f</code> (the scores of the incorrect classes) leads to an increased loss (due to the positive signs +0.2 and +0.5) - and increasing the loss is bad, as expected. However, increasing the score of the correct class has negative influence on the loss. The gradient of -0.7 is telling us that increasing the correct class score would lead to a decrease of the loss \\(L_i\\), which makes sense.</p> <p>All of this boils down to the following code. Recall that <code>probs</code> stores the probabilities of all classes (as rows) for each example. To get the gradient on the scores, which we call <code>dscores</code>, we proceed as follows:</p> <pre><code>dscores = probs\ndscores[range(num_examples),y] -= 1\ndscores /= num_examples\n</code></pre> <p>Lastly, we had that <code>scores = np.dot(X, W) + b</code>, so armed with the gradient on <code>scores</code> (stored in <code>dscores</code>), we can now backpropagate into <code>W</code> and <code>b</code>:</p> <pre><code>dW = np.dot(X.T, dscores)\ndb = np.sum(dscores, axis=0, keepdims=True)\ndW += reg*W # don't forget the regularization gradient\n</code></pre> <p>Where we see that we have backpropped through the matrix multiply operation, and also added the contribution from the regularization. Note that the regularization gradient has the very simple form <code>reg*W</code> since we used the constant <code>0.5</code> for its loss contribution (i.e. \\(\\frac{d}{dw} ( \\frac{1}{2} \\lambda w^2) = \\lambda w\\). This is a common convenience trick that simplifies the gradient expression.</p> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#performing-a-parameter-update","title":"Performing a parameter update","text":"<p>Now that we've evaluated the gradient we know how every parameter influences the loss function. We will now perform a parameter update in the negative gradient direction to decrease the loss:</p> <pre><code># perform a parameter update\nW += -step_size * dW\nb += -step_size * db\n</code></pre> <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#putting-it-all-together-training-a-softmax-classifier","title":"Putting it all together: Training a Softmax Classifier","text":"<p>Putting all of this together, here is the full code for training a Softmax classifier with Gradient descent:</p> <pre><code>#Train a Linear Classifier\n\n# initialize parameters randomly\nW = 0.01 * np.random.randn(D,K)\nb = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(200):\n\n  # evaluate class scores, [N x K]\n  scores = np.dot(X, W) + b\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W)\n  loss = data_loss + reg_loss\n  if i % 10 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters (W,b)\n  dW = np.dot(X.T, dscores)\n  db = np.sum(dscores, axis=0, keepdims=True)\n\n  dW += reg*W # regularization gradient\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n</code></pre> <p>Running this prints the output:</p> <pre><code>iteration 0: loss 1.096956\niteration 10: loss 0.917265\niteration 20: loss 0.851503\niteration 30: loss 0.822336\niteration 40: loss 0.807586\niteration 50: loss 0.799448\niteration 60: loss 0.794681\niteration 70: loss 0.791764\niteration 80: loss 0.789920\niteration 90: loss 0.788726\niteration 100: loss 0.787938\niteration 110: loss 0.787409\niteration 120: loss 0.787049\niteration 130: loss 0.786803\niteration 140: loss 0.786633\niteration 150: loss 0.786514\niteration 160: loss 0.786431\niteration 170: loss 0.786373\niteration 180: loss 0.786331\niteration 190: loss 0.786302\n</code></pre> <p>We see that we've converged to something after about 190 iterations. We can evaluate the training set accuracy:</p> <pre><code># evaluate training set accuracy\nscores = np.dot(X, W) + b\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))\n</code></pre> <p>This prints 49%. Not very good at all, but also not surprising given that the dataset is constructed so it is not linearly separable. We can also plot the learned decision boundaries:</p>      Linear classifier fails to learn the toy spiral dataset.    <p></p>"},{"location":"courses/neural_network/neural-networks-case-study/#training-a-neural-network","title":"Training a Neural Network","text":"<p>Clearly, a linear classifier is inadequate for this dataset and we would like to use a Neural Network. One additional hidden layer will suffice for this toy data. We will now need two sets of weights and biases (for the first and second layers):</p> <pre><code># initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n</code></pre> <p>The forward pass to compute scores now changes form:</p> <pre><code># evaluate class scores with a 2-layer Neural Network\nhidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\nscores = np.dot(hidden_layer, W2) + b2\n</code></pre> <p>Notice that the only change from before is one extra line of code, where we first compute the hidden layer representation and then the scores based on this hidden layer. Crucially, we've also added a non-linearity, which in this case is simple ReLU that thresholds the activations on the hidden layer at zero.</p> <p>Everything else remains the same. We compute the loss based on the scores exactly as before, and get the gradient for the scores <code>dscores</code> exactly as before. However, the way we backpropagate that gradient into the model parameters now changes form, of course. First lets backpropagate the second layer of the Neural Network. This looks identical to the code we had for the Softmax classifier, except we're replacing <code>X</code> (the raw data), with the variable <code>hidden_layer</code>):</p> <pre><code># backpropate the gradient to the parameters\n# first backprop into parameters W2 and b2\ndW2 = np.dot(hidden_layer.T, dscores)\ndb2 = np.sum(dscores, axis=0, keepdims=True)\n</code></pre> <p>However, unlike before we are not yet done, because <code>hidden_layer</code> is itself a function of other parameters and the data! We need to continue backpropagation through this variable. Its gradient can be computed as:</p> <pre><code>dhidden = np.dot(dscores, W2.T)\n</code></pre> <p>Now we have the gradient on the outputs of the hidden layer. Next, we have to backpropagate the ReLU non-linearity. This turns out to be easy because ReLU during the backward pass is effectively a switch. Since \\(r = max(0, x)\\), we have that $\\frac{dr}{dx} = 1(x &gt; 0) $. Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass. Hence, we can backpropagate the ReLU in place simply with:</p> <pre><code># backprop the ReLU non-linearity\ndhidden[hidden_layer &lt;= 0] = 0\n</code></pre> <p>And now we finally continue to the first layer weights and biases:</p> <pre><code># finally into W,b\ndW = np.dot(X.T, dhidden)\ndb = np.sum(dhidden, axis=0, keepdims=True)\n</code></pre> <p>We're done! We have the gradients <code>dW,db,dW2,db2</code> and can perform the parameter update. Everything else remains unchanged. The full code looks very similar:</p> <pre><code># initialize parameters randomly\nh = 100 # size of hidden layer\nW = 0.01 * np.random.randn(D,h)\nb = np.zeros((1,h))\nW2 = 0.01 * np.random.randn(h,K)\nb2 = np.zeros((1,K))\n\n# some hyperparameters\nstep_size = 1e-0\nreg = 1e-3 # regularization strength\n\n# gradient descent loop\nnum_examples = X.shape[0]\nfor i in range(10000):\n\n  # evaluate class scores, [N x K]\n  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n  scores = np.dot(hidden_layer, W2) + b2\n\n  # compute the class probabilities\n  exp_scores = np.exp(scores)\n  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n\n  # compute the loss: average cross-entropy loss and regularization\n  correct_logprobs = -np.log(probs[range(num_examples),y])\n  data_loss = np.sum(correct_logprobs)/num_examples\n  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n  loss = data_loss + reg_loss\n  if i % 1000 == 0:\n    print \"iteration %d: loss %f\" % (i, loss)\n\n  # compute the gradient on scores\n  dscores = probs\n  dscores[range(num_examples),y] -= 1\n  dscores /= num_examples\n\n  # backpropate the gradient to the parameters\n  # first backprop into parameters W2 and b2\n  dW2 = np.dot(hidden_layer.T, dscores)\n  db2 = np.sum(dscores, axis=0, keepdims=True)\n  # next backprop into hidden layer\n  dhidden = np.dot(dscores, W2.T)\n  # backprop the ReLU non-linearity\n  dhidden[hidden_layer &lt;= 0] = 0\n  # finally into W,b\n  dW = np.dot(X.T, dhidden)\n  db = np.sum(dhidden, axis=0, keepdims=True)\n\n  # add regularization gradient contribution\n  dW2 += reg * W2\n  dW += reg * W\n\n  # perform a parameter update\n  W += -step_size * dW\n  b += -step_size * db\n  W2 += -step_size * dW2\n  b2 += -step_size * db2\n</code></pre> <p>This prints:</p> <pre><code>iteration 0: loss 1.098744\niteration 1000: loss 0.294946\niteration 2000: loss 0.259301\niteration 3000: loss 0.248310\niteration 4000: loss 0.246170\niteration 5000: loss 0.245649\niteration 6000: loss 0.245491\niteration 7000: loss 0.245400\niteration 8000: loss 0.245335\niteration 9000: loss 0.245292\n</code></pre> <p>The training accuracy is now:</p> <pre><code># evaluate training set accuracy\nhidden_layer = np.maximum(0, np.dot(X, W) + b)\nscores = np.dot(hidden_layer, W2) + b2\npredicted_class = np.argmax(scores, axis=1)\nprint 'training accuracy: %.2f' % (np.mean(predicted_class == y))\n</code></pre> <p>Which prints 98%!. We can also visualize the decision boundaries:</p>      Neural Network classifier crushes the spiral dataset."},{"location":"courses/neural_network/neural-networks-case-study/#summary","title":"Summary","text":"<p>We've worked with a toy 2D dataset and trained both a linear network and a 2-layer Neural Network. We saw that the change from a linear classifier to a Neural Network involves very few changes in the code. The score function changes its form (1 line of code difference), and the backpropagation changes its form (we have to perform one more round of backprop through the hidden layer to the first layer of the network).</p> <ul> <li>You may want to look at this IPython Notebook code rendered as HTML.</li> <li>Or download the ipynb file</li> </ul>"},{"location":"courses/neural_network/optimization-1/","title":"\u4f18\u5316\u200b\uff1a\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d","text":""},{"location":"courses/neural_network/optimization-1/#_1","title":"\u5f15\u8a00","text":"<p>\u200b\u5728\u200b\u524d\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4efb\u52a1\u200b\u7684\u200b\u80cc\u666f\u200b\u4e0b\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u5173\u952e\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff1a</p> <ol> <li>\u200b\u4e00\u4e2a\u200b\uff08\u200b\u53c2\u6570\u200b\u5316\u200b\u7684\u200b\uff09\u200b\u5206\u6570\u200b\u51fd\u6570\u200b\uff0c\u200b\u5c06\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u6620\u5c04\u200b\u5230\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff09\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u6839\u636e\u200b\u611f\u77e5\u200b\u5206\u6570\u200binduced scores\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\u6765\u200b\u8861\u91cf\u200b\u7279\u5b9a\u200b\u53c2\u6570\u200b\u96c6\u200b\u7684\u200b\u8d28\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u6709\u200b\u8bb8\u591a\u200b\u65b9\u6cd5\u200b\u548c\u200b\u7248\u672c\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u4e2a\u200b\u76ee\u6807\u200b\uff08\u200b\u4f8b\u5982\u200bSoftmax/SVM\uff09\u3002</li> </ol> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u56de\u60f3\u200b\u4e00\u4e0b\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4e3a\u200b $ f(x_i, W) =  W x_i $\uff0c\u200b\u6211\u4eec\u200b\u5f00\u53d1\u200b\u7684\u200bSVM\u200b\u88ab\u200b\u516c\u5f0f\u5316\u200b\u4e3a\u200b\uff1a</p> \\[ L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \\right] + \\alpha R(W) \\] <p>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\uff0c\u200b\u4ea7\u751f\u200b\u4e0e\u200b\u793a\u4f8b\u200b \\(x_i\\) \u200b\u7684\u200b\u57fa\u7840\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b \\(y_i\\) \u200b\u4e00\u81f4\u200b\u7684\u200b\u53c2\u6570\u200b \\(W\\) \u200b\u7684\u200b\u8bbe\u7f6e\u200b\u4e5f\u200b\u5c06\u200b\u5177\u6709\u200b\u975e\u5e38\u4f4e\u200b\u7684\u200b\u635f\u5931\u200b \\(L\\)\u3002\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u7b2c\u4e09\u4e2a\u200b\u548c\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u5173\u952e\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff1a\u200b\u4f18\u5316\u200b\u3002\u200b\u4f18\u5316\u200b\u662f\u200b\u627e\u5230\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u53c2\u6570\u200b \\(W\\) \u200b\u96c6\u5408\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u94fa\u57ab\u200b\uff1a\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u4e86\u89e3\u200b\u4e86\u200b\u8fd9\u200b\u4e09\u4e2a\u200b\u6838\u5fc3\u200b\u7ec4\u4ef6\u200b\u662f\u200b\u5982\u4f55\u200b\u76f8\u4e92\u4f5c\u7528\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u65b0\u200b\u5ba1\u89c6\u200b\u7b2c\u4e00\u4e2a\u200b\u7ec4\u4ef6\u200b\uff08\u200b\u53c2\u6570\u200b\u5316\u200b\u51fd\u6570\u200b\u6620\u5c04\u200b\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u6269\u5c55\u200b\u5230\u200b\u6bd4\u200b\u7ebf\u6027\u200b\u6620\u5c04\u200b\u590d\u6742\u200b\u5f97\u200b\u591a\u200b\u7684\u200b\u51fd\u6570\u200b\uff1a\u200b\u9996\u5148\u200b\u662f\u200b\u6574\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u7136\u540e\u200b\u662f\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u5c06\u200b\u4fdd\u6301\u200b\u76f8\u5bf9\u200b\u4e0d\u53d8\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#_2","title":"\u53ef\u89c6\u5316\u200b\u635f\u5931\u200b\u51fd\u6570","text":"<p>\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7814\u7a76\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u5b9a\u4e49\u200b\u5728\u200b\u975e\u5e38\u200b\u9ad8\u7ef4\u200b\u7684\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200bCIFAR-10\u200b\u4e2d\u200b\uff0c\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u7684\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b[10 x 3073]\uff0c\u200b\u603b\u5171\u200b\u6709\u200b30,730\u200b\u4e2a\u200b\u53c2\u6570\u200b\uff09\uff0c\u200b\u4f7f\u5f97\u200b\u5b83\u4eec\u200b\u5f88\u96be\u200b\u53ef\u89c6\u5316\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6cbf\u7740\u200b\u5149\u7ebf\u200brays\uff081\u200b\u7ef4\u200b\uff09\u200b\u6216\u200b\u5e73\u9762\u200b\uff082\u200b\u7ef4\u200b\uff09\u200b\u5207\u5272\u200b\u9ad8\u200b\u7ef4\u7a7a\u95f4\u200b\u6765\u200b\u83b7\u5f97\u200b\u4e00\u4e9b\u200b\u5173\u4e8e\u200b\u5b83\u4eec\u200b\u7684\u200b\u76f4\u89c2\u200b\u611f\u53d7\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u751f\u6210\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\\(W\\)\uff08\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u70b9\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u6cbf\u7740\u200b\u4e00\u6761\u200b\u5149\u7ebf\u200b\u524d\u8fdb\u200b\u5e76\u200b\u8bb0\u5f55\u200b\u6cbf\u9014\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u751f\u6210\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u65b9\u5411\u200b\\(W_1\\)\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u8bc4\u4f30\u200b\u4e0d\u540c\u200b\\(a\\)\u200b\u503c\u200b\u7684\u200b\\(L(W + a W_1)\\)\u200b\u6765\u200b\u6cbf\u7740\u200b\u8fd9\u4e2a\u200b\u65b9\u5411\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u751f\u6210\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u7ed8\u56fe\u200b\uff0c\u200b\u5176\u4e2d\u200b\\(a\\)\u200b\u503c\u200b\u4f5c\u4e3a\u200bx\u200b\u8f74\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u503c\u200b\u4f5c\u4e3a\u200by\u200b\u8f74\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b\u4e8c\u7ef4\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u8bc4\u4f30\u200b\u635f\u5931\u200b $ L(W + a W_1 + b W_2) $ \u200b\u6765\u200b\u4ee5\u200b\u76f8\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6267\u884c\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u6539\u53d8\u200b \\(a, b\\) \u200b\u65f6\u200b\uff0c\\(a, b\\) \u200b\u53ef\u4ee5\u200b\u5bf9\u5e94\u200b\u4e8e\u200bx\u200b\u8f74\u200b\u548c\u200by\u200b\u8f74\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u989c\u8272\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u503c\u200b\uff1a</p> <p> </p> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200bSVM\uff08\u200b\u65e0\u200b\u6b63\u5219\u200b\u5316\u200b\uff09\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u56fe\u666f\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u62ec\u200bCIFAR-10\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u793a\u4f8b\u200b\uff08\u200b\u5de6\u200b\u3001\u200b\u4e2d\u200b\uff09\u200b\u548c\u200b\u4e00\u767e\u4e2a\u200b\u793a\u4f8b\u200b\uff08\u200b\u53f3\u200b\uff09\u3002\u200b\u5de6\u56fe\u200b\uff1a\u200b\u4ec5\u200b\u901a\u8fc7\u200b\u53d8\u5316\u200b a \u200b\u6765\u200b\u83b7\u5f97\u200b\u7684\u200b\u4e00\u7ef4\u200b\u635f\u5931\u200b\u3002\u200b\u4e2d\u200b\u3001\u200b\u53f3\u56fe\u200b\uff1a\u200b\u635f\u5931\u200b\u7684\u200b\u4e8c\u7ef4\u200b\u5207\u7247\u200b\uff0c\u200b\u84dd\u8272\u200b=\u200b\u4f4e\u200b\u635f\u5931\u200b\uff0c\u200b\u7ea2\u8272\u200b=\u200b\u9ad8\u200b\u635f\u5931\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u7ed3\u6784\u200b\u3002\u200b\u591a\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u635f\u5931\u200b\u662f\u200b\u901a\u8fc7\u200b\u5e73\u5747\u200b\u7ec4\u5408\u200b\u7684\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53f3\u56fe\u200b\u7684\u200b\u7897\u200b\u5f62\u662f\u200b\u8bb8\u591a\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u7897\u200b\uff08\u200b\u4f8b\u5982\u200b\u4e2d\u95f4\u200b\u7684\u200b\u7897\u200b\uff09\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u68c0\u67e5\u200b\u6570\u5b66\u516c\u5f0f\u200b\u6765\u200b\u89e3\u91ca\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u7ed3\u6784\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5355\u4e2a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\uff1a</p> \\[ L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \\right] \\] <p>\u200b\u4ece\u200b\u65b9\u7a0b\u5f0f\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u6e05\u695a\u200b\u5730\u200b\u770b\u51fa\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u662f\u200b \\(W\\) \u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\u4e4b\u200b\u548c\u200b\uff08\u200b\u7531\u4e8e\u200b \\(\\max(0,-)\\) \u200b\u51fd\u6570\u200b\u7684\u200b\u96f6\u200b\u9608\u503c\u200b\uff09\u3002\u200b\u6b64\u5916\u200b\uff0c\\(W\\) \u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\uff08\u200b\u5373\u200b \\(w_j\\)\uff09\u200b\u6709\u65f6\u5019\u200b\u5728\u200b\u5176\u200b\u524d\u9762\u200b\u6709\u200b\u6b63\u200b\u53f7\u200b\uff08\u200b\u5f53\u200b\u5b83\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u793a\u4f8b\u200b\u7684\u200b\u9519\u8bef\u7c7b\u522b\u200b\u65f6\u200b\uff09\uff0c\u200b\u6709\u65f6\u5019\u200b\u6709\u200b\u8d1f\u53f7\u200b\uff08\u200b\u5f53\u200b\u5b83\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u8be5\u200b\u793a\u4f8b\u200b\u7684\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u65f6\u200b\uff09\u3002\u200b\u4e3a\u4e86\u200b\u66f4\u52a0\u200b\u660e\u786e\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8003\u8651\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u4e09\u4e2a\u200b\u4e00\u7ef4\u200b\u70b9\u200b\u548c\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u7b80\u5355\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u4e0d\u5e26\u200b\u6b63\u5219\u200b\u5316\u200b\u7684\u200b\u5b8c\u6574\u200bSVM\u200b\u635f\u5931\u200b\u53d8\u6210\u200b\u4e86\u200b\uff1a</p> \\[ \\begin{align} L_0 = &amp; \\max(0, w_1^Tx_0 - w_0^Tx_0 + 1) + \\max(0, w_2^Tx_0 - w_0^Tx_0 + 1) \\\\\\\\ L_1 = &amp; \\max(0, w_0^Tx_1 - w_1^Tx_1 + 1) + \\max(0, w_2^Tx_1 - w_1^Tx_1 + 1) \\\\\\\\ L_2 = &amp; \\max(0, w_0^Tx_2 - w_2^Tx_2 + 1) + \\max(0, w_1^Tx_2 - w_2^Tx_2 + 1) \\\\\\\\ L = &amp; (L_0 + L_1 + L_2)/3 \\end{align} \\] <p>\u200b\u7531\u4e8e\u200b\u8fd9\u4e9b\u200b\u793a\u4f8b\u200b\u662f\u200b\u4e00\u7ef4\u200b\u7684\u200b\uff0c\u200b\u6570\u636e\u200b \\(x_i\\) \u200b\u548c\u200b\u6743\u91cd\u200b \\(w_j\\) \u200b\u90fd\u200b\u662f\u200b\u6570\u5b57\u200b\u3002\u200b\u4ee5\u200b \\(w_0\\) \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u67d0\u4e9b\u200b\u9879\u662f\u200b \\(w_0\\) \u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u9879\u200b\u90fd\u200b\u5728\u200b\u96f6\u5904\u200b\u88ab\u200b\u622a\u65ad\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u53ef\u89c6\u5316\u200b\u5982\u4e0b\u200b\uff1a</p> <p> </p> <p>1\u200b\u7ef4\u200b\u56fe\u793a\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u3002x\u200b\u8f74\u200b\u662f\u200b\u5355\u4e2a\u200b\u6743\u91cd\u200b\uff0cy\u200b\u8f74\u200b\u662f\u200b\u635f\u5931\u200b\u3002\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u662f\u200b\u591a\u4e2a\u200b\u9879\u200b\u7684\u200b\u603b\u548c\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u9879\u200b\u8981\u4e48\u200b\u4e0e\u200b\u7279\u5b9a\u200b\u6743\u91cd\u200b\u65e0\u5173\u200b\uff0c\u200b\u8981\u4e48\u200b\u662f\u200b\u5b83\u200b\u7684\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff0c\u200b\u88ab\u200b\u622a\u65ad\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u5b8c\u6574\u200b\u7684\u200bSVM\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u5f62\u72b6\u200b\u7684\u200b30,730\u200b\u7ef4\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u65c1\u6ce8\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u4ece\u200b\u5b83\u200b\u7684\u200b\u7897\u200b\u72b6\u200b\u5916\u89c2\u200b\u731c\u5230\u200b\u4e86\u200bSVM\u200b\u6210\u672c\u200b\u51fd\u6570\u200b\u662f\u200b\u51f8\u51fd\u6570\u200bconvex function\u200b\u7684\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\u3002\u200b\u6709\u200b\u5927\u91cf\u200b\u7684\u200b\u6587\u732e\u200b\u81f4\u529b\u4e8e\u200b\u9ad8\u6548\u200b\u5730\u200b\u6700\u5c0f\u5316\u200b\u8fd9\u200b\u7c7b\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u53c2\u52a0\u200b\u65af\u5766\u798f\u200b\u7684\u200b\u4e00\u95e8\u200b\u8bfe\u7a0b\u200b\uff08\u200b\u51f8\u200b\u4f18\u5316\u200bconvex optimization\uff09\u3002\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f97\u5206\u200b\u51fd\u6570\u200b\\(f\\)\u200b\u6269\u5c55\u200b\u5230\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u51fd\u6570\u200b\u5c06\u200b\u53d8\u6210\u200b\u975e\u200b\u51f8\u51fd\u6570\u200b\uff0c\u200b\u4e0a\u9762\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u5c06\u200b\u4e0d\u518d\u200b\u5448\u73b0\u200b\u7897\u200b\u72b6\u200b\uff0c\u200b\u800c\u662f\u200b\u590d\u6742\u200b\u800c\u200b\u5d0e\u5c96\u200b\u7684\u200b\u5730\u5f62\u200b\u3002</p> <p>\u200b\u4e0d\u53ef\u200b\u5fae\u200bNon-differentiable\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6280\u672f\u200b\u8bf4\u660e\u200b\uff0c\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\u7684\u200b\u62d0\u70b9\u200bkinks\uff08\u200b\u7531\u4e8e\u200bmax\u200b\u64cd\u4f5c\u200b\uff09\u200b\u5728\u6280\u672f\u4e0a\u200b\u4f7f\u5f97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e0d\u53ef\u200b\u5fae\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u62d0\u70b9\u200b\u5904\u200b\u68af\u5ea6\u200b\u672a\u200b\u88ab\u200b\u5b9a\u4e49\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6b21\u68af\u5ea6\u200bsubgradient\u200b\u4ecd\u7136\u200b\u5b58\u5728\u200b\u5e76\u4e14\u200b\u901a\u5e38\u200b\u88ab\u200b\u4f7f\u7528\u200b\u3002\u200b\u5728\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e92\u6362\u200b\u4f7f\u7528\u200b\u6b21\u68af\u5ea6\u200b\u548c\u200b\u68af\u5ea6\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u672f\u8bed\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#_3","title":"\u4f18\u5316","text":"<p>\u200b\u518d\u6b21\u200b\u5f3a\u8c03\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u91cf\u5316\u200b\u4efb\u4f55\u200b\u7279\u5b9a\u200b\u6743\u91cd\u200bW\u200b\u7684\u200b\u8d28\u91cf\u200b\u3002\u200b\u4f18\u5316\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u627e\u5230\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200bW\u3002\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5c06\u200b\u6fc0\u52b1\u200b\u5e76\u200b\u6162\u6162\u200b\u5f00\u53d1\u200b\u4e00\u79cd\u200b\u4f18\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u90a3\u4e9b\u200b\u5177\u6709\u200b\u4ee5\u524d\u200b\u7ecf\u9a8c\u200b\u7684\u200b\u4eba\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u90e8\u5206\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u6709\u4e9b\u200b\u5947\u602a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b\u5de5\u4f5c\u200b\u793a\u4f8b\u200b\uff08SVM\u200b\u635f\u5931\u200b\uff09\u200b\u662f\u200b\u4e00\u4e2a\u200b\u51f8\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f46\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u6700\u7ec8\u200b\u4f18\u5316\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5728\u200b\u90a3\u91cc\u200b\u6211\u4eec\u200b\u4e0d\u80fd\u200b\u8f7b\u677e\u200b\u4f7f\u7528\u200b\u51f8\u200b\u4f18\u5316\u200b\u6587\u732e\u200b\u4e2d\u200b\u5f00\u53d1\u200b\u7684\u200b\u4efb\u4f55\u200b\u5de5\u5177\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#1","title":"\u7b56\u7565\u200b\uff031\uff1a\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u7cdf\u7cd5\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\uff1a\u200b\u968f\u673a\u200b\u641c\u7d22","text":"<p>\u200b\u7531\u4e8e\u200b\u68c0\u67e5\u200b\u7ed9\u5b9a\u200b\u7684\u200b\u6743\u91cd\u200bW\u200b\u5982\u6b64\u200b\u7b80\u5355\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9996\u5148\u200b\uff08\u200b\u975e\u5e38\u200b\u7cdf\u7cd5\u200b\uff09\u200b\u60f3\u5230\u200b\u7684\u200b\u4e00\u4e2a\u200b\u60f3\u6cd5\u200b\u662f\u200b\u5c1d\u8bd5\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u968f\u673a\u200b\u6743\u91cd\u200b\uff0c\u200b\u7136\u540e\u200b\u8ddf\u8e2a\u200b\u54ea\u4e2a\u200b\u6548\u679c\u200b\u6700\u597d\u200b\u3002\u200b\u8be5\u200b\u8fc7\u7a0b\u200b\u53ef\u80fd\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n# assume Y_train are the labels (e.g. 1D array of 50,000)\n# assume the function L evaluates the loss function\n\nbestloss = float(\"inf\") # Python assigns the highest possible float value\nfor num in range(1000):\n  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n  if loss &lt; bestloss: # keep track of the best solution\n    bestloss = loss\n    bestW = W\n  print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)\n\n# prints:\n# in attempt 0 the loss was 9.401632, best 9.401632\n# in attempt 1 the loss was 8.959668, best 8.959668\n# in attempt 2 the loss was 9.044034, best 8.959668\n# in attempt 3 the loss was 9.278948, best 8.959668\n# in attempt 4 the loss was 8.857370, best 8.857370\n# in attempt 5 the loss was 8.943151, best 8.857370\n# in attempt 6 the loss was 8.605604, best 8.605604\n# ... (trunctated: continues for 1000 lines)\n</code></pre> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200bW\uff0c\u200b\u5176\u4e2d\u200b\u4e00\u4e9b\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u7684\u200b\u6548\u679c\u200b\u597d\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d6\u200b\u5728\u200b\u8fd9\u6b21\u200b\u641c\u7d22\u200b\u4e2d\u200b\u627e\u5230\u200b\u7684\u200b\u6700\u4f73\u200b\u6743\u91cd\u200bW\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u5c1d\u8bd5\u200b\u5b83\u200b\uff1a</p> <pre><code># Assume X_test is [3073 x 10000], Y_test [10000 x 1]\nscores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n# find the index with max score in each column (the predicted class)\nYte_predict = np.argmax(scores, axis = 0)\n# and calculate accuracy (fraction of predictions that are correct)\nnp.mean(Yte_predict == Yte)\n# returns 0.1555\n</code></pre> <p>\u200b\u6700\u4f73\u200b\u7684\u200bW\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u7ea6\u200b15.5%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002\u200b\u8003\u8651\u200b\u5230\u200b\u5b8c\u5168\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u7c7b\u522b\u200b\u53ea\u80fd\u200b\u8fbe\u5230\u200b10%\uff0c\u200b\u5bf9\u4e8e\u200b\u8fd9\u79cd\u200b\u6beb\u65e0\u200b\u5934\u7eea\u200b\u7684\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\u89e3\u51b3\u65b9\u6848\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u7cdf\u7cd5\u200b\u7684\u200b\u7ed3\u679c\u200b\uff01</p> <p>\u200b\u6838\u5fc3\u601d\u60f3\u200b\uff1a\u200b\u8fed\u4ee3\u200b\u4f18\u5316\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u505a\u200b\u5f97\u200b\u66f4\u597d\u200b\u3002\u200b\u6838\u5fc3\u601d\u60f3\u200b\u662f\u200b\u627e\u5230\u200b\u6700\u4f73\u200b\u6743\u91cd\u200bW\u200b\u662f\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u56f0\u96be\u200b\u751a\u81f3\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u7684\u200b\u95ee\u9898\u200b\uff08\u200b\u7279\u522b\u200b\u662f\u200b\u4e00\u65e6\u200bW\u200b\u5305\u542b\u200b\u4e86\u200b\u6574\u4e2a\u200b\u590d\u6742\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6743\u91cd\u200b\uff09\uff0c\u200b\u4f46\u200b\u5c06\u200b\u7279\u5b9a\u200b\u7684\u200b\u6743\u91cd\u200bW\u200b\u8fdb\u884c\u200b\u5fae\u8c03\u200b\u4ee5\u4f7f\u200b\u5176\u200b\u7565\u5fae\u200b\u6539\u5584\u200b\u7684\u200b\u95ee\u9898\u200b\u8981\u200b\u5bb9\u6613\u200b\u5f97\u200b\u591a\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u662f\u4ece\u200b\u968f\u673a\u200b\u7684\u200bW\u200b\u5f00\u59cb\u200b\uff0c\u200b\u7136\u540e\u200b\u8fed\u4ee3\u200b\u5730\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u6539\u8fdb\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u6bcf\u6b21\u200b\u90fd\u200b\u7565\u5fae\u200b\u6539\u5584\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u7b56\u7565\u200b\u662f\u4ece\u200b\u968f\u673a\u200b\u6743\u91cd\u200b\u5f00\u59cb\u200b\uff0c\u200b\u7136\u540e\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u7684\u200b\u63a8\u79fb\u200b\u8fed\u4ee3\u200b\u5730\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u6539\u8fdb\u200b\uff0c\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u200b\u4f4e\u200b\u7684\u200b\u635f\u5931\u200b</p> <p>\u200b\u76f2\u76ee\u200b\u7684\u200b\u5f92\u6b65\u65c5\u884c\u200b\u8005\u200b\u6bd4\u55bb\u200b\u3002\u200b\u4e00\u4e2a\u200b\u6709\u7528\u200b\u7684\u200b\u7c7b\u6bd4\u200b\u662f\u200b\u5c06\u200b\u81ea\u5df1\u200b\u770b\u4f5c\u200b\u662f\u200b\u5728\u200b\u4e00\u4e2a\u591a\u200b\u5c71\u200b\u7684\u200b\u5730\u5f62\u200b\u4e0a\u200b\u5f92\u6b65\u200b\uff0c\u200b\u800c\u4e14\u200b\u53cc\u773c\u200b\u88ab\u200b\u8499\u4f4f\u200b\uff0c\u200b\u8bd5\u56fe\u200b\u5230\u8fbe\u200b\u5e95\u90e8\u200b\u3002\u200b\u5728\u200bCIFAR-10\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5c71\u662f\u200b30,730\u200b\u7ef4\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200bW\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u662f\u200b10 x 3073\u3002\u200b\u5728\u200b\u5c71\u4e0a\u200b\u7684\u200b\u6bcf\u200b\u4e00\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u8fbe\u5230\u200b\u7279\u5b9a\u200b\u7684\u200b\u635f\u5931\u200b\uff08\u200b\u5730\u5f62\u200b\u7684\u200b\u9ad8\u5ea6\u200b\uff09\u3002</p>"},{"location":"courses/neural_network/optimization-1/#2","title":"\u7b56\u7565\u200b#2\uff1a\u200b\u968f\u673a\u200b\u672c\u5730\u200b\u641c\u7d22","text":"<p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u60f3\u5230\u200b\u7684\u200b\u7b2c\u4e00\u79cd\u200b\u7b56\u7565\u200b\u662f\u200b\u5c1d\u8bd5\u200b\u6cbf\u7740\u200b\u968f\u673a\u200b\u65b9\u5411\u200b\u5ef6\u4f38\u200b\u4e00\u53ea\u200b\u811a\u200b\uff0c\u200b\u7136\u540e\u200b\u53ea\u6709\u200b\u5f53\u200b\u5b83\u200b\u5411\u4e0b\u503e\u659c\u200b\u65f6\u624d\u200b\u8fc8\u51fa\u200b\u4e00\u6b65\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u968f\u673a\u200b\u7684\u200b\\(W\\)\u200b\u5f00\u59cb\u200b\uff0c\u200b\u751f\u6210\u200b\u5bf9\u200b\u5176\u200b\u7684\u200b\u968f\u673a\u200b\u6270\u52a8\u200b\\(\\delta W\\)\uff0c\u200b\u5982\u679c\u200b\u6270\u52a8\u200b\u540e\u200b\u7684\u200b\\(W + \\delta W\\)\u200b\u5904\u200b\u7684\u200b\u635f\u5931\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6267\u884c\u200b\u66f4\u65b0\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u4ee3\u7801\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>W = np.random.randn(10, 3073) * 0.001 # generate random starting W\nbestloss = float(\"inf\")\nfor i in range(1000):\n  step_size = 0.0001\n  Wtry = W + np.random.randn(10, 3073) * step_size\n  loss = L(Xtr_cols, Ytr, Wtry)\n  if loss &lt; bestloss:\n    W = Wtry\n    bestloss = loss\n  print 'iter %d loss is %f' % (i, bestloss)\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u76f8\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8bc4\u4f30\u200b\uff081000\u200b\u6b21\u200b\uff09\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u5b9e\u73b0\u200b\u4e86\u200b21.4%\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u5206\u7c7b\u200b\u51c6\u786e\u7387\u200b\u3002\u200b\u8fd9\u6bd4\u200b\u4e4b\u524d\u200b\u597d\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u4f46\u200b\u4ecd\u7136\u200b\u6d6a\u8d39\u200b\u548c\u200b\u8ba1\u7b97\u200b\u6602\u8d35\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#3","title":"\u7b56\u7565\u200b #3\uff1a\u200b\u8ddf\u968f\u200b\u68af\u5ea6","text":"<p>\u200b\u5728\u200b\u524d\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u8bd5\u56fe\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u5728\u200b\u6743\u91cd\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\u7684\u200b\u65b9\u5411\u200b\uff08\u200b\u5e76\u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u66f4\u200b\u4f4e\u200b\u7684\u200b\u635f\u5931\u200b\uff09\u200b\u7684\u200b\u65b9\u5411\u200b\u3002\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u6ca1\u6709\u200b\u5fc5\u8981\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u65b9\u5411\u200b\uff1a\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u4e00\u4e2a\u200b\u6700\u4f73\u200b\u65b9\u5411\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u6539\u53d8\u200b\u6211\u4eec\u200b\u7684\u200b\u6743\u91cd\u200b\u5411\u91cf\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u65b9\u5411\u200b\u5728\u200b\u6570\u5b66\u200b\u4e0a\u200b\u88ab\u200b\u4fdd\u8bc1\u200b\u662f\u200b\u6700\u200b\u9661\u200b\u4e0b\u964d\u200b\u7684\u200b\u65b9\u5411\u200b\uff08\u200b\u81f3\u5c11\u200b\u5728\u200b\u6b65\u957f\u200b\u8d8b\u5411\u4e8e\u200b\u96f6\u65f6\u200b\uff09\u3002\u200b\u8fd9\u4e2a\u200b\u65b9\u5411\u200b\u4e0e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u68af\u5ea6\u200bgradient\u200b\u6709\u5173\u200b\u3002\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u5f92\u6b65\u65c5\u884c\u200b\u7c7b\u6bd4\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u5927\u81f4\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u611f\u89c9\u200b\u5230\u200b\u811a\u4e0b\u200b\u7684\u200b\u5c71\u5761\u200b\uff0c\u200b\u5e76\u200b\u671d\u7740\u200b\u611f\u89c9\u200b\u6700\u200b\u9661\u200b\u7684\u200b\u65b9\u5411\u200b\u524d\u8fdb\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e00\u7ef4\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u659c\u7387\u200b\u662f\u200b\u4f60\u200b\u53ef\u80fd\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u4efb\u4f55\u200b\u70b9\u200b\u7684\u200b\u51fd\u6570\u200b\u7684\u200b\u77ac\u65f6\u200b\u53d8\u5316\u7387\u200b\u3002\u200b\u68af\u5ea6\u200b\u662f\u200b\u5bf9\u200b\u4e0d\u4ec5\u200b\u63a5\u53d7\u200b\u5355\u4e2a\u200b\u6570\u5b57\u200b\u800c\u4e14\u200b\u63a5\u53d7\u200b\u4e00\u7ec4\u200b\u6570\u5b57\u200b\u7684\u200b\u51fd\u6570\u200b\u7684\u200b\u659c\u7387\u200b\u7684\u200b\u4e00\u79cd\u200b\u6982\u62ec\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u68af\u5ea6\u200b\u53ea\u662f\u200b\u8f93\u5165\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u659c\u7387\u200b\u7684\u200b\u5411\u91cf\u200b\uff08\u200b\u66f4\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u5bfc\u6570\u200bderivatives\uff09\u3002\u200b\u5173\u4e8e\u200b\u5176\u200b\u8f93\u5165\u200b\u7684\u200b1-D\u200b\u51fd\u6570\u200b\u7684\u200b\u5bfc\u6570\u200b\u7684\u200b\u6570\u5b66\u200b\u8868\u8fbe\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> \\[ \\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h} \\] <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u4e00\u7ec4\u200b\u6570\u5b57\u200b\u800c\u200b\u4e0d\u662f\u200b\u5355\u4e2a\u200b\u6570\u5b57\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bfc\u6570\u200b\u79f0\u4e3a\u200b\u504f\u200b\u5bfc\u6570\u200bpartial derivatives\uff0c\u200b\u68af\u5ea6\u200b\u53ea\u662f\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\u7684\u200b\u504f\u200b\u5bfc\u6570\u200b\u7684\u200b\u5411\u91cf\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#_4","title":"\u8ba1\u7b97\u200b\u68af\u5ea6","text":"<p>\u200b\u6709\u200b\u4e24\u79cd\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1a\u200b\u4e00\u79cd\u200b\u662f\u200b\u6162\u901f\u200b\u3001\u200b\u8fd1\u4f3c\u200b\u4f46\u200b\u5bb9\u6613\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff08\u200b\u6570\u503c\u200b\u68af\u5ea6\u200bnumerical gradient\uff09\uff0c\u200b\u53e6\u200b\u4e00\u79cd\u200b\u662f\u200b\u5feb\u901f\u200b\u3001\u200b\u7cbe\u786e\u200b\u4f46\u200b\u66f4\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u9700\u8981\u200b\u5fae\u79ef\u5206\u200b\uff08\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200banalytic gradient\uff09\u3002\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u65b9\u6cd5\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#finite-differences","title":"\u4f7f\u7528\u200b\u6709\u9650\u200b\u5dee\u5206\u200bfinite differences\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u68af\u5ea6","text":"<p>\u200b\u4e0a\u9762\u200b\u7ed9\u51fa\u200b\u7684\u200b\u516c\u5f0f\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u6570\u503c\u200b\u65b9\u6cd5\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u901a\u7528\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b83\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b<code>f</code>\u3001\u200b\u8981\u200b\u5728\u200b\u5176\u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u68af\u5ea6\u200b\u7684\u200b\u5411\u91cf\u200b<code>x</code>\uff0c\u200b\u5e76\u200b\u8fd4\u56de\u200b\u5728\u200b<code>x</code>\u200b\u5904\u200b<code>f</code>\u200b\u7684\u200b\u68af\u5ea6\u200b\uff1a</p> <pre><code>def eval_numerical_gradient(f, x):\n\"\"\"\n  a naive implementation of numerical gradient of f at x\n  - f should be a function that takes a single argument\n  - x is the point (numpy array) to evaluate the gradient at\n  \"\"\"\n\n  fx = f(x) # evaluate function value at original point\n  grad = np.zeros(x.shape)\n  h = 0.00001\n\n  # iterate over all indexes in x\n  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n  while not it.finished:\n\n    # evaluate function at x+h\n    ix = it.multi_index\n    old_value = x[ix]\n    x[ix] = old_value + h # increment by h\n    fxh = f(x) # evalute f(x + h)\n    x[ix] = old_value # restore to previous value (very important!)\n\n    # compute the partial derivative\n    grad[ix] = (fxh - fx) / h # the slope\n    it.iternext() # step to next dimension\n\n  return grad\n</code></pre> <p>\u200b\u6309\u7167\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u7ed9\u51fa\u200b\u7684\u200b\u68af\u5ea6\u200b\u516c\u5f0f\u200b\uff0c\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u9010\u4e2a\u200b\u7ef4\u5ea6\u200b\u8fed\u4ee3\u200b\uff0c\u200b\u6cbf\u7740\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u5c0f\u200b\u7684\u200b\u53d8\u5316\u200b<code>h</code>\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u67e5\u770b\u200b\u51fd\u6570\u200b\u53d8\u5316\u200b\u4e86\u200b\u591a\u5c11\u200b\u6765\u200b\u8ba1\u7b97\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u504f\u200b\u5bfc\u6570\u200b\u3002\u200b\u6700\u7ec8\u200b\uff0c\u200b\u53d8\u91cf\u200b<code>grad</code>\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u5b8c\u6574\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002</p> <p>\u200b\u5b9e\u9645\u200b\u8003\u8651\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u6570\u5b66\u516c\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u68af\u5ea6\u200b\u7684\u200b\u5b9a\u4e49\u200b\u662f\u200b\u5728\u200bh\u200b\u8d8b\u200b\u8fd1\u4e8e\u96f6\u200b\u7684\u200b\u6781\u9650\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u901a\u5e38\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u503c\u200b\uff08\u200b\u4f8b\u5982\u200b\u793a\u4f8b\u200b\u4e2d\u200b\u7684\u200b1e-5\uff09\u3002\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u60a8\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u4e0d\u4f1a\u200b\u5bfc\u81f4\u200b\u6570\u503c\u200b\u95ee\u9898\u200b\u7684\u200b\u6700\u5c0f\u200b\u6b65\u957f\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e2d\u5fc3\u200b\u5dee\u5206\u200b\u516c\u5f0f\u200bcentered difference formula\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u901a\u5e38\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\uff1a$ [f(x+h) - f(x-h)] / 2 h $ \u3002\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u8bf7\u53c2\u9605\u200b\u7ef4\u57fa\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u63d0\u4f9b\u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u8ba1\u7b97\u200b\u4efb\u4f55\u200b\u70b9\u200b\u548c\u200b\u4efb\u4f55\u200b\u51fd\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8ba1\u7b97\u200bCIFAR-10\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5728\u200b\u6743\u91cd\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u7684\u200b\u67d0\u4e2a\u200b\u968f\u673a\u200b\u70b9\u200b\u7684\u200b\u68af\u5ea6\u200b\uff1a</p> <pre><code># to use the generic code above we want a function that takes a single argument\n# (the weights in our case) so we close over X_train and Y_train\ndef CIFAR10_loss_fun(W):\n  return L(X_train, Y_train, W)\n\nW = np.random.rand(10, 3073) * 0.001 # random weight vector\ndf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient\n</code></pre> <p>\u200b\u68af\u5ea6\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6cbf\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u659c\u7387\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\uff1a</p> <pre><code>loss_original = CIFAR10_loss_fun(W) # the original loss\nprint 'original loss: %f' % (loss_original, )\n\n# lets see the effect of multiple step sizes\nfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n  step_size = 10 ** step_size_log\n  W_new = W - step_size * df # new position in the weight space\n  loss_new = CIFAR10_loss_fun(W_new)\n  print 'for step size %f new loss: %f' % (step_size, loss_new)\n\n# prints:\n# original loss: 2.200718\n# for step size 1.000000e-10 new loss: 2.200652\n# for step size 1.000000e-09 new loss: 2.200057\n# for step size 1.000000e-08 new loss: 2.194116\n# for step size 1.000000e-07 new loss: 2.135493\n# for step size 1.000000e-06 new loss: 1.647802\n# for step size 1.000000e-05 new loss: 2.844355\n# for step size 1.000000e-04 new loss: 25.558142\n# for step size 1.000000e-03 new loss: 254.086573\n# for step size 1.000000e-02 new loss: 2539.370888\n# for step size 1.000000e-01 new loss: 25392.214036\n</code></pre> <p>\u200b\u671d\u7740\u200b\u8d1f\u200b\u68af\u5ea6\u65b9\u5411\u200b\u66f4\u65b0\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u8ba1\u7b97\u200b<code>W_new</code>\uff0c\u200b\u6211\u4eec\u200b\u662f\u200b\u5728\u200b\u68af\u5ea6\u200b<code>df</code>\u200b\u7684\u200b\u8d1f\u200b\u65b9\u5411\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u51cf\u5c0f\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u589e\u52a0\u200b\u3002</p> <p>\u200b\u6b65\u957f\u200bstep\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u68af\u5ea6\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u51fd\u6570\u200b\u5177\u6709\u200b\u6700\u5feb\u200b\u589e\u52a0\u200b\u901f\u7387\u200b\u7684\u200b\u65b9\u5411\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u4e0d\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u6cbf\u7740\u200b\u8fd9\u4e2a\u200b\u65b9\u5411\u200b\u5e94\u8be5\u200b\u8fc8\u591a\u8fdc\u200b\u7684\u200b\u4e00\u6b65\u200b\u3002\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u540e\u671f\u200b\u770b\u5230\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u9009\u62e9\u200b\u6b65\u957f\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u200blearning rate\uff09\u200b\u5c06\u200b\u6210\u4e3a\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\uff08\u200b\u4e5f\u200b\u662f\u200b\u6700\u200b\u5934\u75bc\u200b\u7684\u200b\uff09\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e4b\u4e00\u200b\u3002\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u76f2\u76ee\u200b\u4e0b\u5761\u200b\u4e0b\u964d\u200b\u6bd4\u55bb\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u611f\u89c9\u200b\u5230\u200b\u811a\u4e0b\u200b\u7684\u200b\u5c71\u5761\u200b\u671d\u200b\u67d0\u4e2a\u200b\u65b9\u5411\u200b\u503e\u659c\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u8fc8\u591a\u8fdc\u200b\u7684\u200b\u4e00\u6b65\u200b\u662f\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u7684\u200b\u3002\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c0f\u5fc3\u7ffc\u7ffc\u200b\u5730\u200b\u632a\u52a8\u200b\u811a\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u671f\u671b\u200b\u53d6\u5f97\u200b\u4e00\u81f4\u200b\u4f46\u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u8fdb\u5c55\u200b\uff08\u200b\u8fd9\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u91c7\u7528\u200b\u5c0f\u6b65\u200b\u957f\u200b\uff09\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9009\u62e9\u200b\u8fc8\u51fa\u200b\u5927\u6b65\u200b\uff0c\u200b\u4ee5\u200b\u66f4\u200b\u5feb\u200b\u5730\u200b\u4e0b\u964d\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u5f97\u4e0d\u507f\u5931\u200b\u3002\u200b\u6b63\u5982\u200b\u60a8\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\u4e2d\u200b\u770b\u5230\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e9b\u200b\u65f6\u5019\u200b\u91c7\u53d6\u200b\u66f4\u5927\u200b\u7684\u200b\u6b65\u9aa4\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u635f\u5931\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u201c\u200b\u8d8a\u8fc7\u200b\u4e86\u200b\u76ee\u6807\u200b\u201d\u3002</p> <p> </p> <p>\u200b\u53ef\u89c6\u5316\u200b\u6b65\u957f\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u6211\u4eec\u200b\u4ece\u200b\u67d0\u4e2a\u200b\u7279\u5b9a\u200b\u7684\u200b\u4f4d\u7f6e\u200bW\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff08\u200b\u6216\u200b\u66f4\u200b\u786e\u5207\u200b\u5730\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u8d1f\u503c\u200b - \u200b\u767d\u8272\u200b\u7bad\u5934\u200b\uff09\uff0c\u200b\u5b83\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e2d\u200b\u6700\u200b\u9661\u5ced\u200b\u7684\u200b\u51cf\u5c0f\u200b\u65b9\u5411\u200b\u3002\u200b\u5c0f\u200b\u6b65\u9aa4\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u7a33\u5b9a\u200b\u4f46\u200b\u7f13\u6162\u200b\u7684\u200b\u8fdb\u5c55\u200b\u3002\u200b\u5927\u200b\u6b65\u9aa4\u200b\u53ef\u4ee5\u200b\u5bfc\u81f4\u200b\u66f4\u597d\u200b\u7684\u200b\u8fdb\u5c55\u200b\uff0c\u200b\u4f46\u200b\u66f4\u52a0\u200b\u5192\u9669\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6700\u7ec8\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5927\u6b65\u200b\u957f\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8d8a\u8fc7\u200b\u76ee\u6807\u200b\uff0c\u200b\u4f7f\u200b\u635f\u5931\u200b\u53d8\u5f97\u200b\u66f4\u7cdf\u200b\u3002\u200b\u6b65\u957f\u200b\uff08\u200b\u6216\u8005\u200b\u6211\u4eec\u200b\u4ee5\u540e\u200b\u5c06\u200b\u79f0\u4e4b\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u200b\uff09\u200b\u5c06\u200b\u6210\u4e3a\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u4ed4\u7ec6\u200b\u8c03\u6574\u200b\u7684\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u6548\u7387\u200b\u95ee\u9898\u200b\u3002\u200b\u60a8\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u7684\u200b\u590d\u6742\u5ea6\u200b\u4e0e\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u5448\u200b\u7ebf\u6027\u5173\u7cfb\u200b\u3002\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u603b\u5171\u200b\u6709\u200b30730\u200b\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8fdb\u884c\u200b30731\u200b\u6b21\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u4ee5\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u5e76\u200b\u6267\u884c\u200b\u5355\u4e2a\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u53ea\u4f1a\u200b\u53d8\u5f97\u200b\u66f4\u7cdf\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u73b0\u4ee3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5f88\u200b\u5bb9\u6613\u200b\u6709\u200b\u6570\u5343\u4e07\u200b\u4e2a\u200b\u53c2\u6570\u200b\u3002\u200b\u663e\u7136\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u7b56\u7565\u200b\u4e0d\u200b\u5177\u5907\u200b\u53ef\u6269\u5c55\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u66f4\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#calculus","title":"\u4f7f\u7528\u200b\u5fae\u79ef\u5206\u200bCalculus\u200b\u89e3\u6790\u200b\u8ba1\u7b97\u200b\u68af\u5ea6","text":"<p>\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u4f7f\u7528\u200b\u6709\u9650\u200b\u5dee\u5206\u200b\u903c\u8fd1\u200b\u975e\u5e38\u7b80\u5355\u200b\uff0c\u200b\u4f46\u200b\u7f3a\u70b9\u200b\u662f\u200b\u5b83\u200b\u662f\u200b\u8fd1\u4f3c\u200b\u7684\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200bh\u200b\u503c\u200b\uff0c\u200b\u800c\u200b\u771f\u6b63\u200b\u7684\u200b\u68af\u5ea6\u200b\u5b9a\u4e49\u200b\u4e3a\u200bh\u200b\u8d8b\u200b\u8fd1\u4e8e\u96f6\u200b\u7684\u200b\u6781\u9650\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u8ba1\u7b97\u200b\u8d77\u6765\u200b\u975e\u5e38\u200b\u6602\u8d35\u200b\u3002\u200b\u7b2c\u4e8c\u79cd\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4f7f\u7528\u200b\u5fae\u79ef\u5206\u200b\u8fdb\u884c\u200b\u5206\u6790\u200b\uff0c\u200b\u5b83\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u5bfc\u51fa\u200b\u68af\u5ea6\u200b\u7684\u200b\u76f4\u63a5\u200b\u516c\u5f0f\u200b\uff08\u200b\u6ca1\u6709\u200b\u8fd1\u4f3c\u200b\uff09\uff0c\u200b\u800c\u4e14\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u4e5f\u200b\u975e\u5e38\u200b\u5feb\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e0e\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5b83\u200b\u5728\u200b\u5b9e\u73b0\u200b\u65f6\u200b\u66f4\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4e3a\u4ec0\u4e48\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u975e\u5e38\u200b\u5e38\u89c1\u200b\u7684\u200b\u662f\u200b\u8ba1\u7b97\u200b\u5206\u6790\u200b\u68af\u5ea6\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u4ee5\u200b\u68c0\u67e5\u200b\u5b9e\u73b0\u200b\u7684\u200b\u6b63\u786e\u6027\u200b\u3002\u200b\u8fd9\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200bgradient check\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ee5\u200b\u5355\u4e2a\u200b\u6570\u636e\u200b\u70b9\u200b\u7684\u200bSVM\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e3a\u4f8b\u200b\uff1a</p> \\[ L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right] \\] <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9488\u5bf9\u200b\u6743\u91cd\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u9488\u5bf9\u200b\\(w_{y_i}\\)\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\uff1a</p> \\[ \\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) \\right) x_i \\] <p>\u200b\u5176\u4e2d\u200b\\(\\mathbb{1}\\)\u200b\u662f\u200b\u6307\u793a\u200b\u51fd\u6570\u200b\uff0c\u200b\u5982\u679c\u200b\u5185\u90e8\u200b\u6761\u4ef6\u200b\u4e3a\u200b\u771f\u200b\uff0c\u200b\u5219\u200b\u4e3a\u200b1\uff0c\u200b\u5426\u5219\u200b\u4e3a\u200b\u96f6\u200b\u3002\u200b\u867d\u7136\u200b\u5728\u200b\u4e66\u5199\u200b\u65f6\u200b\u8fd9\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u5413\u4eba\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u65f6\u200b\uff0c\u200b\u60a8\u200b\u53ea\u200b\u9700\u200b\u8ba1\u7b97\u200b\u672a\u200b\u6ee1\u8db3\u200b\u6240\u200b\u9700\u200b\u95f4\u9694\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\uff08\u200b\u4ece\u800c\u200b\u6709\u5229\u4e8e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u6570\u636e\u200b\u5411\u91cf\u200b\\(x_i\\)\u200b\u4e58\u200b\u4ee5\u6b64\u200b\u6570\u5b57\u200b\u5373\u200b\u4e3a\u200b\u68af\u5ea6\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u4e0e\u200b\u6b63\u786e\u200b\u7c7b\u522b\u200b\u5bf9\u5e94\u200b\u7684\u200b\\(W\\)\u200b\u884c\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u200b\u884c\u200b\uff0c\u200b\u5176\u4e2d\u200b\\(j \\neq y_i\\)\uff0c\u200b\u68af\u5ea6\u200b\u662f\u200b\uff1a</p> \\[ \\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) x_i \\] <p>\u200b\u4e00\u65e6\u200b\u60a8\u200b\u5bfc\u51fa\u200b\u4e86\u200b\u68af\u5ea6\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u8fd9\u4e9b\u200b\u8868\u8fbe\u5f0f\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u4eec\u200b\u6267\u884c\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u5c31\u200b\u5f88\u200b\u7b80\u5355\u200b\u4e86\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#_5","title":"\u68af\u5ea6\u200b\u4e0b\u964d","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e86\u200b\uff0c\u200b\u53cd\u590d\u200b\u8bc4\u4f30\u200b\u68af\u5ea6\u200b\u7136\u540e\u200b\u6267\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200bGradient Descent\u3002\u200b\u5176\u200b\u666e\u901a\u200bvanilla\u200b\u7248\u672c\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> <pre><code># Vanilla Gradient Descent\n\nwhile True:\n  weights_grad = evaluate_gradient(loss_fun, data, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>\u200b\u8fd9\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u5faa\u73af\u200b\u662f\u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5e93\u200b\u7684\u200b\u6838\u5fc3\u200b\u3002\u200b\u6709\u200b\u5176\u4ed6\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u6267\u884c\u200b\u4f18\u5316\u200b\uff08\u200b\u4f8b\u5982\u200bLBFGS\uff09\uff0c\u200b\u4f46\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u76ee\u524d\u200b\u662f\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u200b\u5e38\u89c1\u200b\u548c\u200b\u6700\u200b\u6210\u719f\u200b\u7684\u200b\u4f18\u5316\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u5728\u200b\u6574\u4e2a\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bf9\u200b\u8fd9\u4e2a\u200b\u5faa\u73af\u200b\u7684\u200b\u7ec6\u8282\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u6539\u8fdb\u200b\uff08\u200b\u4f8b\u5982\u200b\u66f4\u65b0\u200b\u65b9\u7a0b\u200b\u7684\u200b\u8be6\u7ec6\u200b\u7ec6\u8282\u200b\uff09\uff0c\u200b\u4f46\u200b\u8ddf\u968f\u200b\u68af\u5ea6\u200b\u76f4\u5230\u200b\u6211\u4eec\u200b\u6ee1\u610f\u200b\u7ed3\u679c\u200b\u7684\u200b\u6838\u5fc3\u601d\u60f3\u200b\u5c06\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> <p>\u200b\u5c0f\u6279\u91cf\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u3002\u200b\u5728\u200b\u5927\u89c4\u6a21\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff08\u200b\u4f8b\u5982\u200bILSVRC\u200b\u6311\u6218\u8d5b\u200b\uff09\uff0c\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u53ef\u80fd\u200b\u6709\u200b\u6570\u767e\u4e07\u4e2a\u200b\u793a\u4f8b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u6267\u884c\u200b\u5355\u4e2a\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6574\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u4e0a\u200b\u7684\u200b\u5b8c\u6574\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4f3c\u4e4e\u200b\u662f\u200b\u4e00\u79cd\u200b\u6d6a\u8d39\u200b\u3002\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u7684\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u5e38\u89c1\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u8ba1\u7b97\u200b\u6279\u91cf\u200bbatches\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u5f53\u524d\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200bConvNets\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5178\u578b\u200b\u7684\u200b\u6279\u91cf\u200b\u5305\u542b\u200b\u6765\u81ea\u200b\u6574\u4e2a\u200b120\u200b\u4e07\u4e2a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b256\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u3002\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u6279\u91cf\u200b\u6765\u200b\u6267\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\uff1a</p> <pre><code># Vanilla Minibatch Gradient Descent\n\nwhile True:\n  data_batch = sample_training_data(data, 256) # sample 256 examples\n  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n  weights += - step_size * weights_grad # perform parameter update\n</code></pre> <p>\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u4e4b\u6240\u4ee5\u200b\u6709\u6548\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u793a\u4f8b\u200b\u662f\u200b\u76f8\u5173\u200b\u7684\u200b\u3002\u200b\u8981\u200b\u7406\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8003\u8651\u200b\u4e00\u4e2a\u200b\u6781\u7aef\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5373\u200bILSVRC\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b120\u200b\u4e07\u5e45\u200b\u56fe\u50cf\u200b\u5b9e\u9645\u4e0a\u200b\u90fd\u200b\u7531\u200b1000\u200b\u5e45\u200b\u552f\u4e00\u200b\u56fe\u50cf\u200b\u7684\u200b\u7cbe\u786e\u200b\u526f\u672c\u200b\u7ec4\u6210\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u4e00\u4e2a\u200b\uff0c\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6bcf\u5e45\u200b\u56fe\u50cf\u200b\u6709\u200b1200\u200b\u4e2a\u200b\u76f8\u540c\u200b\u7684\u200b\u526f\u672c\u200b\uff09\u3002\u200b\u90a3\u4e48\u200b\u5f88\u200b\u660e\u663e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e3a\u200b\u6240\u6709\u200b1200\u200b\u4e2a\u200b\u76f8\u540c\u200b\u526f\u672c\u200b\u8ba1\u7b97\u200b\u7684\u200b\u68af\u5ea6\u200b\u90fd\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u5c06\u200b\u6240\u6709\u200b120\u200b\u4e07\u5e45\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u7684\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u5e73\u5747\u200b\u65f6\u200b\uff0c\u200b\u5f97\u5230\u200b\u7684\u200b\u635f\u5931\u200b\u4e0e\u200b\u4ec5\u200b\u5728\u200b1000\u200b\u4e2a\u200b\u5c0f\u5b50\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u65f6\u200b\u5f97\u5230\u200b\u7684\u200b\u635f\u5931\u200b\u5b8c\u5168\u76f8\u540c\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u4e0d\u4f1a\u200b\u5305\u542b\u200b\u91cd\u590d\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5c0f\u6279\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u8fd1\u4f3c\u4e8e\u200b\u6574\u4e2a\u200b\u76ee\u6807\u200b\u51fd\u6570\u200bobjective\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bc4\u4f30\u200b\u5c0f\u6279\u91cf\u200b\u68af\u5ea6\u200b\u4ee5\u200b\u6267\u884c\u200b\u66f4\u200b\u9891\u7e41\u200b\u7684\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u66f4\u5feb\u200b\u7684\u200b\u6536\u655b\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u6781\u7aef\u200b\u60c5\u51b5\u200b\u662f\u200b\u5c0f\u6279\u91cf\u200b\u53ea\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200bStochastic Gradient Descent\uff08SGD\uff09\uff08\u200b\u6709\u65f6\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5728\u7ebf\u200bon-line\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09\u3002\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u76f8\u5bf9\u200b\u4e0d\u592a\u200b\u5e38\u89c1\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u77e2\u91cf\u5316\u200b\u4ee3\u7801\u4f18\u5316\u200b\uff0c\u200b\u8bc4\u4f30\u200b100\u200b\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u68af\u5ea6\u200b\u901a\u5e38\u200b\u6bd4\u200b100\u200b\u6b21\u200b\u8bc4\u4f30\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u7684\u200b\u68af\u5ea6\u200b\u8ba1\u7b97\u200b\u6548\u7387\u200b\u66f4\u9ad8\u200b\u3002\u200b\u5c3d\u7ba1\u200bSGD\u200b\u5728\u6280\u672f\u4e0a\u200b\u662f\u200b\u6307\u200b\u4e00\u6b21\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u6765\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u4f46\u200b\u4eba\u4eec\u200b\u5728\u200b\u63d0\u5230\u200b\u5c0f\u6279\u91cf\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u65f6\u200b\uff08\u200b\u901a\u5e38\u200b\u5047\u8bbe\u200b\u4f7f\u7528\u200b\u5c0f\u6279\u91cf\u200b\uff09\u200b\u4e5f\u200b\u4f1a\u200b\u4f7f\u7528\u200bSGD\u200b\u8fd9\u4e2a\u200b\u672f\u8bed\u200b\u3002\u200b\u5c0f\u6279\u91cf\u200b\u7684\u200b\u5927\u5c0f\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u5e76\u200b\u4e0d\u200b\u5e38\u89c1\u200b\u3002\u200b\u901a\u5e38\u200b\uff0c\u200b\u5b83\u200b\u57fa\u4e8e\u200b\u5185\u5b58\u200b\u9650\u5236\u200b\uff08\u200b\u5982\u679c\u200b\u6709\u200b\u7684\u8bdd\u200b\uff09\uff0c\u200b\u6216\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u67d0\u4e2a\u200b\u503c\u200b\uff0c\u200b\u4f8b\u5982\u200b32\u300164\u200b\u6216\u200b128\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b2\u200b\u7684\u200b\u5e42\u200b\u6b21\u65b9\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8bb8\u591a\u200b\u77e2\u91cf\u5316\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5b9e\u73b0\u200b\u5728\u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\u4e3a\u200b2\u200b\u7684\u200b\u5e42\u200b\u6b21\u65b9\u200b\u65f6\u200b\u8fd0\u884c\u200b\u5f97\u200b\u66f4\u200b\u5feb\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-1/#_6","title":"\u603b\u7ed3","text":"<p>\u200b\u4fe1\u606f\u6d41\u200b\u7684\u200b\u603b\u7ed3\u200b\u3002\u200b\u6210\u5bf9\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b(x,y)\u200b\u662f\u200b\u56fa\u5b9a\u200b\u7684\u200b\u3002\u200b\u6743\u91cd\u200b\u5f00\u59cb\u200b\u662f\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6539\u53d8\u200b\u3002\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u671f\u95f4\u200b\uff0c\u200b\u5206\u6570\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u7c7b\u522b\u200b\u5206\u6570\u200b\uff0c\u200b\u5b58\u50a8\u200b\u5728\u200b\u5411\u91cf\u200bf\u200b\u4e2d\u200b\u3002\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u7ec4\u4ef6\u200b\uff1a\u200b\u6570\u636e\u200b\u635f\u5931\u200b\u8ba1\u7b97\u200b\u5206\u6570\u200bf\u200b\u4e0e\u200b\u6807\u7b7e\u200by\u200b\u4e4b\u95f4\u200b\u7684\u200b\u517c\u5bb9\u6027\u200b\u3002\u200b\u6b63\u5219\u200b\u5316\u200b\u635f\u5931\u200b\u4ec5\u200b\u4e0e\u200b\u6743\u91cd\u200b\u6709\u5173\u200b\u3002\u200b\u5728\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u671f\u95f4\u200b\uff0c\u200b\u6211\u4eec\u200b\u8ba1\u7b97\u200b\u6743\u91cd\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u53ef\u9009\u5730\u200b\u5728\u200b\u6570\u636e\u200b\u4e0a\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u5982\u679c\u200b\u9700\u8981\u7684\u8bdd\u200b\uff09\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u4eec\u200b\u5728\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u671f\u95f4\u200b\u6267\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002</p> <p>\u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\uff0c</p> <ul> <li>\u200b\u6211\u4eec\u200b\u628a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u770b\u505a\u200b\u662f\u200b\u4e00\u4e2a\u200b \u200b\u9ad8\u7ef4\u200b\u4f18\u5316\u200b\u7a7a\u95f4\u200bhigh-dimensional optimization landscape\uff0c\u200b\u5728\u200b\u5176\u4e2d\u200b\u6211\u4eec\u200b\u8bd5\u56fe\u200b\u5230\u8fbe\u200b\u5e95\u90e8\u200b\u3002\u200b\u6211\u4eec\u200b\u5f00\u53d1\u200b\u7684\u200b\u5de5\u4f5c\u200b\u7c7b\u6bd4\u200b\u662f\u200b\u4e00\u4f4d\u200b\u5e0c\u671b\u200b\u5230\u8fbe\u200b\u5e95\u90e8\u200b\u7684\u200b\u88ab\u200b\u8499\u4f4f\u200b\u773c\u775b\u200b\u7684\u200b\u5f92\u6b65\u200b\u8005\u200b\u3002\u200b\u7279\u522b\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200bSVM\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u662f\u200b\u5206\u6bb5\u200b\u7ebf\u6027\u200b\u548c\u200b\u7897\u200b\u72b6\u200b\u7684\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u9610\u8ff0\u200b\u4e86\u200b\u901a\u8fc7\u200b\u8fed\u4ee3\u200b\u6539\u8fdb\u200b\u6765\u200b\u4f18\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u7ec4\u200b\u968f\u673a\u200b\u6743\u91cd\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9010\u6b65\u200b\u7ec6\u5316\u200b\u5b83\u4eec\u200b\uff0c\u200b\u76f4\u5230\u200b\u635f\u5931\u200b\u6700\u5c0f\u5316\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u51fd\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6700\u200b\u9661\u5347\u200b\u65b9\u5411\u200b\uff0c\u200b\u5e76\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u4e00\u79cd\u200b\u7b80\u5355\u200b\u4f46\u200b\u4f4e\u6548\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5373\u200b\u4f7f\u7528\u200b\u6709\u9650\u200b\u5dee\u5206\u200b\u8fd1\u4f3c\u200b\uff08\u200b\u6709\u9650\u200b\u5dee\u5206\u200b\u662f\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200bh\u200b\u7684\u200b\u503c\u200b\uff09\u200b\u6765\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u68d8\u624b\u200b\u7684\u200b\u6b65\u957f\u200b\uff08\u200b\u6216\u200b\u5b66\u4e60\u200b\u7387\u200b\uff09\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5fc5\u987b\u200b\u8bbe\u7f6e\u200b\u5f97\u200b\u6070\u5230\u597d\u5904\u200b\uff1a\u200b\u5982\u679c\u200b\u592a\u4f4e\u200b\uff0c\u200b\u8fdb\u5c55\u200b\u7a33\u5b9a\u200b\u4f46\u200b\u7f13\u6162\u200b\u3002\u200b\u5982\u679c\u200b\u592a\u9ad8\u200b\uff0c\u200b\u8fdb\u5c55\u200b\u53ef\u4ee5\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u4f46\u200b\u66f4\u200b\u6709\u200b\u98ce\u9669\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672a\u6765\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u5730\u200b\u63a2\u8ba8\u200b\u8fd9\u4e2a\u200b\u6743\u8861\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u6570\u503c\u200b\u548c\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6743\u8861\u200b\u3002\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u7b80\u5355\u200b\uff0c\u200b\u4f46\u662f\u200b\u662f\u200b\u8fd1\u4f3c\u200b\u7684\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u4ee3\u4ef7\u200b\u9ad8\u200b\u3002\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\u7cbe\u786e\u200b\uff0c\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u5feb\u200b\uff0c\u200b\u4f46\u200b\u66f4\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u9700\u8981\u200b\u6570\u5b66\u200b\u63a8\u5bfc\u200b\u68af\u5ea6\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u603b\u662f\u200b\u4f7f\u7528\u200b\u89e3\u6790\u200b\u68af\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u6267\u884c\u200b\u68af\u5ea6\u200b\u68c0\u67e5\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5c06\u200b\u5176\u200b\u5b9e\u73b0\u200b\u4e0e\u200b\u6570\u503c\u200b\u68af\u5ea6\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u8be5\u200b\u7b97\u6cd5\u200b\u5faa\u73af\u200b\u8fed\u4ee3\u200b\u5730\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u5e76\u200b\u6267\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002</li> </ul> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff1a\u200b\u672c\u8282\u200b\u7684\u200b\u6838\u5fc3\u200b\u8981\u70b9\u200b\u662f\u200b\uff0c\u200b\u80fd\u591f\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u5176\u200b\u6743\u91cd\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u6709\u200b\u76f4\u89c2\u200b\u7684\u200b\u7406\u89e3\u200b\uff09\u200b\u662f\u200b\u8bbe\u8ba1\u200b\u3001\u200b\u8bad\u7ec3\u200b\u548c\u200b\u7406\u89e3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6700\u200b\u91cd\u8981\u200b\u6280\u80fd\u200b\u3002\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u719f\u7ec3\u5730\u200b\u4f7f\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200bchain rule\u200b\u5206\u6790\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u3002\u200b\u8fd9\u200b\u5c06\u200b\u4f7f\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u9ad8\u6548\u200b\u5730\u200b\u4f18\u5316\u200b\u76f8\u5bf9\u200b\u4efb\u610f\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u8868\u8fbe\u200b\u4e86\u200b\u5404\u79cd\u7c7b\u578b\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5305\u62ec\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/","title":"\u53cd\u5411\u200b\u4f20\u64ad","text":""},{"location":"courses/neural_network/optimization-2/#_1","title":"\u4ecb\u7ecd","text":"<p>\u200b\u52a8\u673a\u200b. \u200b\u5728\u200b\u672c\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200bbackpropagation\u200b\u7684\u200b\u76f4\u89c2\u200b\u542b\u4e49\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u901a\u8fc7\u200b\u9012\u5f52\u200b\u5e94\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200bchain rule\u200b\u6765\u200b\u8ba1\u7b97\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u68af\u5ea6\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u7406\u89e3\u200b\u8fd9\u4e00\u200b\u8fc7\u7a0b\u200b\u53ca\u5176\u200b\u7ec6\u5fae\u200b\u4e4b\u200b\u5904\u200b\u5bf9\u4e8e\u200b\u60a8\u200b\u7406\u89e3\u200b\u3001\u200b\u6709\u6548\u200b\u5f00\u53d1\u200b\u3001\u200b\u8bbe\u8ba1\u200b\u548c\u200b\u8c03\u8bd5\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u81f3\u5173\u91cd\u8981\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\u63cf\u8ff0\u200b. \u200b\u672c\u200b\u8282\u200b\u7814\u7a76\u200b\u7684\u200b\u6838\u5fc3\u200b\u95ee\u9898\u200b\u5982\u4e0b\u200b\uff1a\u200b\u6211\u4eec\u200b\u7ed9\u5b9a\u200b\u4e86\u200b\u67d0\u4e2a\u200b\u51fd\u6570\u200b \\(f(x)\\) \uff0c\u200b\u5176\u4e2d\u200b \\(x\\) \u200b\u662f\u200b\u8f93\u5165\u200b\u5411\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u60f3\u200b\u8ba1\u7b97\u200b \\(f\\) \u200b\u5728\u200b \\(x\\) \u200b\u5904\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u5373\u200b \\(\\nabla f(x)\\) \uff09\u3002</p> <p>\u200b\u52a8\u673a\u200b. \u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5bf9\u6b64\u200b\u95ee\u9898\u200b\u611f\u5174\u8da3\u200b\u7684\u200b\u4e3b\u8981\u200b\u539f\u56e0\u200b\u662f\u200b\uff0c\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7279\u5b9a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\\(f\\) \u200b\u5c06\u200b\u5bf9\u5e94\u200b\u4e8e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08 \\(L\\) \uff09\uff0c\u200b\u800c\u200b\u8f93\u5165\u200b \\(x\\) \u200b\u5c06\u200b\u7531\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u548c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6743\u91cd\u200b\u7ec4\u6210\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u635f\u5931\u200b\u53ef\u80fd\u200b\u662f\u200bSVM\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u8f93\u5165\u200b\u65e2\u200b\u5305\u62ec\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b \\((x_i,y_i), i=1 \\ldots N\\) \uff0c\u200b\u4e5f\u200b\u5305\u62ec\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u5dee\u200b \\(W,b\\)\u3002 \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff08\u200b\u5982\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u901a\u5e38\u200b\u7684\u200b\u60c5\u51b5\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u8ba4\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u662f\u200b\u7ed9\u5b9a\u200b\u548c\u200b\u56fa\u5b9a\u200b\u7684\u200b\uff0c\u200b\u6743\u91cd\u200b\u662f\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u63a7\u5236\u200b\u7684\u200b\u53d8\u91cf\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u5bb9\u6613\u200b\u5730\u200b\u4f7f\u7528\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u6765\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\u793a\u4f8b\u200b \\(x_i\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u53ea\u200b\u8ba1\u7b97\u200b\u53c2\u6570\u200b\uff08\u200b\u4f8b\u5982\u200b \\(W,b\\)\uff09\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u5728\u200b\u8bfe\u5802\u200b\u4e0a\u5c06\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\\(x_i\\) \u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u6709\u65f6\u200b\u4ecd\u7136\u200b\u5f88\u200b\u6709\u7528\u200b\uff0c\u200b\u4f8b\u5982\u200b\u7528\u4e8e\u200b\u53ef\u89c6\u5316\u200b\u548c\u200b\u89e3\u91ca\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ef\u80fd\u200b\u5728\u200b\u505a\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u53c2\u52a0\u200b\u8fd9\u4e2a\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5bf9\u200b\u4f7f\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u63a8\u5bfc\u200b\u68af\u5ea6\u200b\u611f\u5230\u200b\u5f88\u200b\u8212\u9002\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u5e0c\u671b\u200b\u9f13\u52b1\u200b\u4f60\u200b\u81f3\u5c11\u200b\u6d4f\u89c8\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u4ece\u5b9e\u200b\u503c\u200b\u7535\u8def\u200b\u7684\u200b\u53cd\u5411\u200b\u6d41\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u4e0d\u200b\u5e38\u89c1\u200b\u89c6\u89d2\u200b\uff0c\u200b\u4f60\u200b\u4ece\u4e2d\u200b\u83b7\u5f97\u200b\u7684\u200b\u4efb\u4f55\u200b\u6d1e\u5bdf\u200b\u90fd\u200b\u53ef\u80fd\u200b\u5728\u200b\u6574\u4e2a\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u5bf9\u200b\u4f60\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_2","title":"\u68af\u5ea6\u200b\u7684\u200b\u7b80\u5355\u200b\u8868\u8fbe\u200b\u548c\u200b\u89e3\u91ca","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u7b80\u5355\u200b\u7684\u200b\u60c5\u51b5\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5efa\u7acb\u200b\u66f4\u200b\u590d\u6742\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u7b26\u53f7\u200b\u548c\u200b\u7ea6\u5b9a\u200b\u3002\u200b\u8003\u8651\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6570\u5b57\u200b\u76f8\u4e58\u200b\u7684\u200b\u51fd\u6570\u200b \\(f(x,y) = x y\\)\u3002\u200b\u901a\u8fc7\u200b\u7b80\u5355\u200b\u7684\u200b\u5fae\u79ef\u5206\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5f97\u51fa\u200b\u5bf9\u4e8e\u200b\u4efb\u4e00\u200b\u8f93\u5165\u200b\u7684\u200b\u504f\u200b\u5bfc\u6570\u200b\uff1a</p> \\[ f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x \\] <p>\u200b\u89e3\u91ca\u200b\uff1a\u200b\u8bb0\u4f4f\u200b\u5bfc\u6570\u200b\u544a\u8bc9\u200b\u4f60\u200b\u4ec0\u4e48\u200b\uff1a\u200b\u5b83\u4eec\u200b\u6307\u793a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u5728\u200b\u67d0\u200b\u4e00\u70b9\u200b\u9644\u8fd1\u200b\u65e0\u9650\u5c0f\u200b\u533a\u57df\u200b\u5185\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u8be5\u200b\u53d8\u91cf\u200b\u7684\u200b\u53d8\u5316\u200b\u901f\u7387\u200b\uff1a</p> \\[ \\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h} \\] <p>\u200b\u6280\u672f\u200b\u4e0a\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5de6\u8fb9\u200b\u7684\u200b\u9664\u200b\u53f7\u200b\u4e0e\u200b\u53f3\u8fb9\u200b\u7684\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5b83\u200b\u4e0d\u200b\u8868\u793a\u200b\u9664\u6cd5\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u7b26\u53f7\u200b\u8868\u793a\u200b\u8fd0\u7b97\u7b26\u200b \\(\\frac{d}{dx}\\) \u200b\u5e94\u7528\u200b\u4e8e\u200b\u51fd\u6570\u200b \\(f\\)\uff0c\u200b\u5e76\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u51fd\u6570\u200b\uff08\u200b\u5bfc\u6570\u200b\uff09\u3002\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u7406\u89e3\u200b\u4e0a\u8ff0\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u65b9\u5f0f\u200b\u662f\u200b\uff0c\u200b\u5f53\u200b \\(h\\) \u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u65f6\u5019\u200b\uff0c\u200b\u51fd\u6570\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u8fd1\u4f3c\u200b\u4e3a\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u800c\u200b\u5bfc\u6570\u200b\u5c31\u662f\u200b\u5b83\u200b\u7684\u200b\u659c\u7387\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u7684\u200b\u5bfc\u6570\u200b\u544a\u8bc9\u200b\u4f60\u200b\u6574\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u5bf9\u200b\u5b83\u200b\u7684\u200b\u503c\u200b\u7684\u200b\u654f\u611f\u5ea6\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b \\(x = 4, y = -3\\)\uff0c\u200b\u90a3\u4e48\u200b \\(f(x,y) = -12\\)\uff0c\u200b\u5e76\u4e14\u200b\u5bf9\u4e8e\u200b \\(x\\) \u200b\u7684\u200b\u5bfc\u6570\u200b \\(\\frac{\\partial f}{\\partial x} = -3\\)\u3002\u200b\u8fd9\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u53d8\u91cf\u200b\u7684\u200b\u503c\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u5fae\u5c0f\u200b\u7684\u200b\u91cf\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8d1f\u53f7\u200b\u7684\u200b\u4f5c\u7528\u200b\uff0c\u200b\u6574\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u6548\u679c\u200b\u5c06\u200b\u51cf\u5c0f\u200b\uff08\u200b\u51cf\u5c0f\u200b\u4e86\u200b\u4e09\u500d\u200b\uff09\uff0c\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u91cd\u65b0\u6392\u5217\u200b\u4e0a\u9762\u200b\u7684\u200b\u65b9\u7a0b\u200b (\\(f(x + h) = f(x) + h \\frac{df(x)}{dx}\\) ) \u200b\u6765\u770b\u200b\u51fa\u200b\u3002\u200b\u7c7b\u4f3c\u200b\u5730\u200b\uff0c\u200b\u7531\u4e8e\u200b \\(\\frac{\\partial f}{\\partial y} = 4\\)\uff0c\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u901a\u8fc7\u200b\u4e00\u4e9b\u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u91cf\u200b \\(h\\) \u200b\u589e\u52a0\u200b \\(y\\) \u200b\u7684\u200b\u503c\u200b\u4e5f\u200b\u4f1a\u200b\u589e\u52a0\u200b\u51fd\u6570\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u7531\u4e8e\u200b\u6b63\u200b\u53f7\u200b\uff09\uff0c\u200b\u589e\u52a0\u200b\u7684\u200b\u91cf\u200b\u4e3a\u200b \\(4h\\)\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u7684\u200b\u5bfc\u6570\u200b\u544a\u8bc9\u200b\u4f60\u200b\u6574\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u5bf9\u200b\u5176\u503c\u200b\u7684\u200b\u654f\u611f\u5ea6\u200b\u3002</p> <p>\u200b\u6b63\u5982\u200b\u524d\u9762\u200b\u63d0\u5230\u200b\u7684\u200b\uff0c\u200b\u68af\u5ea6\u200b \\(\\nabla f\\) \u200b\u662f\u200b\u504f\u200b\u5bfc\u6570\u200b\u7684\u200b\u5411\u91cf\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u6709\u200b \\(\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y, x]\\)\u3002\u200b\u5c3d\u7ba1\u200b\u68af\u5ea6\u200b\u5728\u6280\u672f\u4e0a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u7b80\u5355\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u5e38\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u8bf8\u5982\u200b \"x \u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\" \u200b\u8fd9\u6837\u200b\u7684\u200b\u672f\u8bed\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6280\u672f\u200b\u4e0a\u200b\u6b63\u786e\u200b\u7684\u200b \"x \u200b\u4e0a\u200b\u7684\u200b\u504f\u200b\u5bfc\u6570\u200b\"\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u63a8\u5bfc\u200b\u51fa\u200b\u52a0\u6cd5\u200b\u8fd0\u7b97\u200b\u7684\u200b\u5bfc\u6570\u200b\uff1a</p> \\[ f(x,y) = x + y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1 \\] <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u65e0\u8bba\u200b \\(x,y\\) \u200b\u7684\u200b\u503c\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff0c\\(x\\) \u200b\u548c\u200b \\(y\\) \u200b\u7684\u200b\u5bfc\u6570\u200b\u90fd\u200b\u662f\u200b\u4e00\u200b\u3002\u200b\u8fd9\u200b\u662f\u200b\u6709\u200b\u9053\u7406\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u589e\u52a0\u200b \\(x\\) \u200b\u6216\u200b \\(y\\) \u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e00\u4e2a\u200b\u90fd\u200b\u4f1a\u200b\u589e\u52a0\u200b \\(f\\) \u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fd9\u79cd\u200b\u589e\u52a0\u200b\u7684\u200b\u901f\u5ea6\u200b\u4e0d\u200b\u53d7\u200b \\(x,y\\) \u200b\u7684\u200b\u5b9e\u9645\u200b\u503c\u200b\u7684\u200b\u5f71\u54cd\u200b\uff08\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u7684\u200b\u4e58\u6cd5\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0d\u540c\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u4f1a\u200b\u7ecf\u5e38\u200b\u4f7f\u7528\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u662f\u200b \u200b\u6700\u5927\u503c\u200bmax \u200b\u8fd0\u7b97\u200b\uff1a</p> \\[ f(x,y) = \\max(x, y) \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = \\mathbb{1}(x &gt;= y) \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = \\mathbb{1}(y &gt;= x) \\] <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c(\u200b\u5b50\u200b)\u200b\u68af\u5ea6\u200b\u5728\u200b\u8f83\u5927\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0a\u200b\u4e3a\u200b1\uff0c\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u4e0a\u200b\u4e3a\u200b0\u3002\u200b\u76f4\u89c2\u200b\u5730\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u8f93\u5165\u200b\u662f\u200b \\(x = 4, y = 2\\)\uff0c\u200b\u90a3\u4e48\u200b\u6700\u5927\u503c\u200b\u662f\u200b4\uff0c\u200b\u51fd\u6570\u200b\u5bf9\u4e8e\u200b \\(y\\) \u200b\u7684\u200b\u8bbe\u7f6e\u200b\u4e0d\u200b\u654f\u611f\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u5fae\u5c0f\u200b\u7684\u200b\u91cf\u200b \\(h\\)\uff0c\u200b\u51fd\u6570\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u8f93\u51fa\u200b4\uff0c\u200b\u56e0\u6b64\u200b\u68af\u5ea6\u200b\u4e3a\u200b\u96f6\u200b\uff1a\u200b\u6ca1\u6709\u200b\u5f71\u54cd\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b \\(y\\) \u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u5927\u91cf\u200b\uff08\u200b\u4f8b\u5982\u200b\u5927\u4e8e\u200b2\uff09\uff0c\u200b\u90a3\u4e48\u200b \\(f\\) \u200b\u7684\u200b\u503c\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u4f46\u200b\u5bfc\u6570\u200b\u4e0d\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u5173\u4e8e\u200b\u8fd9\u79cd\u200b\u5927\u5e45\u5ea6\u200b\u53d8\u5316\u200b\u5bf9\u200b\u51fd\u6570\u200b\u8f93\u5165\u200b\u7684\u200b\u5f71\u54cd\u200b\u3002\u200b\u5b83\u4eec\u200b\u53ea\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5173\u4e8e\u200b\u8f93\u5165\u200b\u7684\u200b\u5fae\u5c0f\u200b\u3001\u200b\u65e0\u9650\u5c0f\u200b\u53d8\u5316\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6b63\u5982\u200b\u5728\u200b\u5176\u200b\u5b9a\u4e49\u200b\u4e2d\u200b\u6240\u793a\u200b\uff0c\u200b\u7531\u200b \\(\\lim_{h \\rightarrow 0}\\) \u200b\u8868\u793a\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_3","title":"\u4f7f\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u7684\u200b\u590d\u5408\u200b\u8868\u8fbe\u5f0f","text":"<p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8003\u8651\u200b\u6d89\u53ca\u200b\u591a\u4e2a\u200b\u7ec4\u5408\u200b\u51fd\u6570\u200b\u7684\u200b\u66f4\u200b\u590d\u6742\u200b\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u6bd4\u5982\u200b \\(f(x, y, z) = (x + y)z\\)\u3002\u200b\u8fd9\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u4ecd\u7136\u200b\u8db3\u591f\u200b\u7b80\u5355\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8fdb\u884c\u200b\u6c42\u5bfc\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5c06\u200b\u91c7\u53d6\u200b\u4e00\u79cd\u200b\u7279\u5b9a\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6709\u52a9\u4e8e\u200b\u7406\u89e3\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u80cc\u540e\u200b\u7684\u200b\u76f4\u89c9\u200b\u3002\u200b\u7279\u522b\u200b\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u53ef\u4ee5\u200b\u5206\u89e3\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\uff1a\\(q = x + y\\) \u200b\u548c\u200b \\(f = qz\\)\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u5206\u522b\u200b\u8ba1\u7b97\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u5bfc\u6570\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b\u524d\u200b\u4e00\u8282\u200b\u4e2d\u200b\u770b\u5230\u200b\u7684\u200b\u90a3\u6837\u200b\u3002\\(f\\) \u200b\u53ea\u662f\u200b \\(q\\) \u200b\u548c\u200b \\(z\\) \u200b\u7684\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u6240\u4ee5\u200b \\(\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z} = q\\)\uff0c\u200b\u800c\u200b \\(q\\) \u200b\u662f\u200b \\(x\\) \u200b\u548c\u200b \\(y\\) \u200b\u7684\u200b\u52a0\u6cd5\u200b\uff0c\u200b\u6240\u4ee5\u200b \\(\\frac{\\partial q}{\\partial x} = 1, \\frac{\\partial q}{\\partial y} = 1\\)\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u5173\u5fc3\u200b\u4e2d\u95f4\u200b\u503c\u200b \\(q\\) \u200b\u7684\u200b\u68af\u5ea6\u200b \u2014\u2014 \\(\\frac{\\partial f}{\\partial q}\\) \u200b\u7684\u200b\u503c\u200b\u662f\u200b\u65e0\u7528\u200b\u7684\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u6211\u4eec\u200b\u6700\u7ec8\u200b\u5173\u5fc3\u200b\u7684\u200b\u662f\u200b \\(f\\) \u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u5176\u200b\u8f93\u5165\u200b \\(x, y, z\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u94fe\u5f0f\u6cd5\u5219\u200b \u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u6b63\u786e\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u68af\u5ea6\u200b\u8868\u8fbe\u5f0f\u200b\"\u200b\u94fe\u200b\"\u200b\u5728\u200b\u4e00\u8d77\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u901a\u8fc7\u200b\u4e58\u6cd5\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\\(\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}\\)\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u4e24\u4e2a\u200b\u4fdd\u5b58\u200b\u4e24\u4e2a\u200b\u68af\u5ea6\u200b\u7684\u200b\u6570\u200b\u7684\u200b\u4e58\u6cd5\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u6765\u200b\u770b\u770b\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> <pre><code># set some inputs\nx = -2; y = 5; z = -4\n\n# perform the forward pass\nq = x + y # q becomes 3\nf = q * z # f becomes -12\n\n# perform the backward pass (backpropagation) in reverse order:\n# first backprop through f = q * z\ndfdz = q # df/dz = q, so gradient on z becomes 3\ndfdq = z # df/dq = z, so gradient on q becomes -4\ndqdx = 1.0\ndqdy = 1.0\n# now backprop through q = x + y\ndfdx = dfdq * dqdx  # The multiplication here is the chain rule!\ndfdy = dfdq * dqdy  \n</code></pre> <p>\u200b\u6211\u4eec\u200b\u6700\u7ec8\u200b\u5f97\u5230\u200b\u4e86\u200b\u53d8\u91cf\u200b <code>[dfdx, dfdy, dfdz]</code> \u200b\u4e2d\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u8fd9\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u53d8\u91cf\u200b <code>x, y, z</code> \u200b\u5bf9\u200b <code>f</code> \u200b\u7684\u200b\u654f\u611f\u5ea6\u200b\uff01\u200b\u8fd9\u662f\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u6700\u200b\u7b80\u5355\u200b\u793a\u4f8b\u200b\u3002\u200b\u5728\u200b\u63a5\u4e0b\u6765\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u66f4\u200b\u7b80\u6d01\u200b\u7684\u200b\u7b26\u53f7\u200b\uff0c\u200b\u7701\u7565\u200b <code>df</code> \u200b\u524d\u7f00\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7b80\u5355\u200b\u5730\u5199\u200b <code>dq</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>dfdq</code>\uff0c\u200b\u5e76\u200b\u59cb\u7ec8\u200b\u5047\u8bbe\u200b\u68af\u5ea6\u200b\u662f\u200b\u9488\u5bf9\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u7684\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u8ba1\u7b97\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7535\u8def\u56fe\u200bcircuit diagram\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\uff1a</p> -2-4x5-4y-43z3-4q+-121f* <p>\u200b\u5b9e\u503c\u200breal-valued\u200b\u7684\u200b\"\u200b\u7535\u8def\u200b\"\u200b\u5728\u200b\u5de6\u4fa7\u200b\u5c55\u793a\u200b\u4e86\u200b\u8ba1\u7b97\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u8868\u793a\u200b\u3002\u200b\u6b63\u5411\u200b\u4f20\u64ad\u200bforward pass\u200b\u4ece\u200b\u8f93\u5165\u200b\u5230\u200b\u8f93\u51fa\u200b\u8ba1\u7b97\u200b\u503c\u200b\uff08\u200b\u7528\u200b\u7eff\u8272\u200b\u663e\u793a\u200b\uff09\u3002\u200b\u7136\u540e\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200bbackward pass\u200b\u4ece\u200b\u672b\u7aef\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u9012\u5f52\u200b\u5730\u200b\u5e94\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u6765\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff08\u200b\u7528\u200b\u7ea2\u8272\u200b\u663e\u793a\u200b\uff09\uff0c\u200b\u76f4\u5230\u200b\u7535\u8def\u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u68af\u5ea6\u200b\u770b\u4f5c\u200b\u662f\u200b\u901a\u8fc7\u200b\u7535\u8def\u200b\u53cd\u5411\u200b\u6d41\u52a8\u200b\u7684\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_4","title":"\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u76f4\u89c2\u200b\u7406\u89e3","text":"<p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7f8e\u5999\u200b\u7684\u200b\u5c40\u90e8\u200b\u8fc7\u7a0b\u200b\u3002\u200b\u7535\u8def\u56fe\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u95e8\u200b\u90fd\u200b\u63a5\u6536\u200b\u4e00\u4e9b\u200b\u8f93\u5165\u200b\uff0c\u200b\u5e76\u200b\u53ef\u4ee5\u200b\u7acb\u5373\u200b\u8ba1\u7b97\u200b\u4e24\u4ef6\u4e8b\u200b\uff1a1. \u200b\u5b83\u200b\u7684\u200b\u8f93\u51fa\u200b\u503c\u200b\u548c\u200b 2. \u200b\u5176\u200b\u8f93\u51fa\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u5176\u200b\u8f93\u5165\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u3002\u200b\u6ce8\u610f\u200b\uff0c\u200b\u95e8\u200b\u53ef\u4ee5\u200b\u5b8c\u5168\u200b\u72ec\u7acb\u200b\u5730\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u4e86\u89e3\u200b\u5b83\u4eec\u200b\u6240\u200b\u5d4c\u5165\u200b\u7684\u200b\u6574\u4e2a\u200b\u7535\u8def\u200b\u7684\u200b\u4efb\u4f55\u200b\u7ec6\u8282\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e00\u65e6\u200b\u6b63\u5411\u200b\u4f20\u64ad\u200b\u7ed3\u675f\u200b\uff0c\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u8be5\u95e8\u200b\u6700\u7ec8\u200b\u4f1a\u200b\u4e86\u89e3\u200b\u5230\u200b\u5176\u200b\u8f93\u51fa\u200b\u503c\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u7535\u8def\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\uff0c\u200b\u95e8\u200b\u5e94\u8be5\u200b\u5c06\u200b\u8be5\u200b\u68af\u5ea6\u200b\u4e58\u4ee5\u200b\u5b83\u200b\u901a\u5e38\u200b\u4e3a\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u68af\u5ea6\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u5bfc\u81f4\u200b\u7684\u200b\u8fd9\u79cd\u200b\u989d\u5916\u200b\u4e58\u6cd5\u200b\uff08\u200b\u5bf9\u4e8e\u200b\u6bcf\u4e2a\u200b\u8f93\u5165\u200b\uff09\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u4e14\u200b\u76f8\u5bf9\u200b\u65e0\u7528\u200b\u7684\u200b\u95e8\u200b\u53d8\u6210\u200b\u4e00\u4e2a\u200b\u590d\u6742\u200b\u7535\u8def\u200b\uff08\u200b\u5982\u200b\u6574\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u9f7f\u8f6e\u200bcog\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u518d\u6b21\u200b\u53c2\u8003\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u4ee5\u200b\u76f4\u89c2\u200b\u5730\u200b\u4e86\u89e3\u200b\u8fd9\u662f\u200b\u5982\u4f55\u200b\u5de5\u4f5c\u200b\u7684\u200b\u3002\u200b\u52a0\u6cd5\u200b\u95e8\u200b\u63a5\u6536\u200b\u8f93\u5165\u200b [-2, 5] \u200b\u5e76\u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b 3\u3002\u200b\u7531\u4e8e\u200b\u95e8\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u52a0\u6cd5\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u5176\u200b\u5bf9\u200b\u5176\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u90fd\u200b\u662f\u200b +1\u3002\u200b\u7535\u8def\u200b\u7684\u200b\u5176\u4f59\u90e8\u5206\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u6700\u7ec8\u200b\u503c\u200b\uff0c\u200b\u5373\u200b -12\u3002\u200b\u5728\u200b\u5e94\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u9012\u5f52\u200b\u53cd\u5411\u200b\u901a\u8fc7\u200b\u7535\u8def\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u52a0\u6cd5\u200b\u95e8\u200b\uff08\u200b\u662f\u200b\u4e58\u6cd5\u200b\u95e8\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\uff09\u200b\u4e86\u89e3\u200b\u5230\u200b\u5176\u200b\u8f93\u51fa\u200b\u7684\u200b\u68af\u5ea6\u200b\u662f\u200b -4\u3002\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7535\u8def\u200b\u62df\u4eba\u5316\u200banthropomorphize\u200b\u4e3a\u200b\u5e0c\u671b\u200b\u8f93\u51fa\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u503c\u200b\uff08\u200b\u8fd9\u200b\u6709\u52a9\u4e8e\u200b\u76f4\u89c2\u200b\u7406\u89e3\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u7535\u8def\u200b\u201c\u200b\u5e0c\u671b\u200b\u201d\u200b\u52a0\u6cd5\u200b\u95e8\u200b\u7684\u200b\u8f93\u51fa\u200b\u66f4\u200b\u4f4e\u200b\uff08\u200b\u7531\u4e8e\u200b\u8d1f\u53f7\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u6709\u200b 4 \u200b\u7684\u200b\u529b\u91cf\u200b\u3002\u200b\u4e3a\u4e86\u200b\u7ee7\u7eed\u200b\u9012\u5f52\u200b\u5e76\u200b\u94fe\u63a5\u200b\u68af\u5ea6\u200b\uff0c\u200b\u52a0\u6cd5\u200b\u95e8\u200b\u53d6\u5f97\u200b\u8be5\u200b\u68af\u5ea6\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4e58\u4ee5\u200b\u5176\u200b\u8f93\u5165\u200b\u7684\u200b\u6240\u6709\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\uff08\u200b\u4f7f\u200b x \u200b\u548c\u200b y \u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u90fd\u200b\u53d8\u4e3a\u200b 1 * -4 = -4\uff09\u3002\u200b\u6ce8\u610f\u200b\u5230\u200b\u8fd9\u200b\u6709\u200b\u9884\u671f\u200b\u7684\u200b\u6548\u679c\u200b\uff1a\u200b\u5982\u679c\u200b x, y \u200b\u51cf\u5c11\u200b\uff08\u200b\u54cd\u5e94\u200b\u4e8e\u200b\u4ed6\u4eec\u200b\u7684\u200b\u8d1f\u200b\u68af\u5ea6\u200b\uff09\uff0c\u200b\u90a3\u4e48\u200b\u52a0\u6cd5\u200b\u95e8\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c31\u200b\u4f1a\u200b\u51cf\u5c11\u200b\uff0c\u200b\u8fd9\u200b\u53cd\u8fc7\u6765\u200b\u53c8\u200b\u4f7f\u200b\u4e58\u6cd5\u200b\u95e8\u200b\u7684\u200b\u8f93\u51fa\u200b\u589e\u52a0\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u95e8\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u4fe1\u53f7\u200b\u5f7c\u6b64\u200b\u901a\u4fe1\u200b\uff0c\u200b\u5b83\u4eec\u200b\u662f\u5426\u200b\u5e0c\u671b\u200b\u5176\u200b\u8f93\u51fa\u200b\u589e\u52a0\u200b\u6216\u200b\u51cf\u5c11\u200b\uff08\u200b\u4ee5\u53ca\u200b\u591a\u200b\u5f3a\u70c8\u200b\uff09\uff0c\u200b\u4ee5\u200b\u4f7f\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u503c\u200b\u66f4\u200b\u9ad8\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#sigmoid","title":"\u6a21\u5757\u5316\u200b\uff1aSigmoid\u200b\u793a\u4f8b","text":"<p>\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u95e8\u200b\u662f\u200b\u76f8\u5bf9\u200b\u968f\u610f\u200b\u7684\u200b\u3002\u200b\u4efb\u4f55\u200b\u53ef\u200b\u5fae\u5206\u200b\u7684\u200b\u51fd\u6570\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u4f5c\u4e3a\u200b\u95e8\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u591a\u4e2a\u200b\u95e8\u200b\u7ec4\u5408\u6210\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u95e8\u200b\uff0c\u200b\u6216\u8005\u200b\u5728\u200b\u65b9\u4fbf\u200b\u7684\u200b\u65f6\u5019\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u5206\u89e3\u6210\u200b\u591a\u4e2a\u200b\u95e8\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u4ee5\u200b\u8bf4\u660e\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> \\[ f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}} \\] <p>\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bfe\u5802\u200b\u4e0a\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u8868\u8fbe\u5f0f\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200b sigmoid\u200b\u6fc0\u6d3b\u200b \u200b\u51fd\u6570\u200b\u7684\u200b\u4e8c\u7ef4\u200b\u795e\u7ecf\u5143\u200b\uff08\u200b\u5177\u6709\u200b\u8f93\u5165\u200b x \u200b\u548c\u200b\u6743\u91cd\u200b w\uff09\u3002\u200b\u4f46\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7b80\u5355\u200b\u5730\u200b\u5c06\u200b\u5176\u200b\u89c6\u4e3a\u200b\u4ece\u200b\u8f93\u5165\u200b w,x \u200b\u5230\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u6570\u5b57\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u7531\u200b\u591a\u4e2a\u200b\u95e8\u200b\u7ec4\u6210\u200b\u3002\u200b\u9664\u4e86\u200b\u4e0a\u9762\u200b\u63cf\u8ff0\u200b\u7684\u200b\u90a3\u4e9b\u200b\u95e8\u200b\uff08add\u3001mul\u3001max\uff09\u200b\u4e4b\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b\u56db\u4e2a\u200b\u95e8\u200b\uff1a</p> \\[ f(x) = \\frac{1}{x}  \\hspace{1in} \\rightarrow \\hspace{1in}  \\frac{df}{dx} = -1/x^2  \\\\\\\\ f_c(x) = c + x \\hspace{1in} \\rightarrow \\hspace{1in}  \\frac{df}{dx} = 1  \\\\\\\\ f(x) = e^x \\hspace{1in} \\rightarrow \\hspace{1in}  \\frac{df}{dx} = e^x \\\\\\\\ f_a(x) = ax \\hspace{1in} \\rightarrow \\hspace{1in}  \\frac{df}{dx} = a \\] <p>\u200b\u5176\u4e2d\u200b\u51fd\u6570\u200b \\(f_c, f_a\\) \u200b\u5206\u522b\u200b\u901a\u8fc7\u200b\u5e38\u6570\u200b \\(c\\) \u200b\u5e73\u79fb\u200b\u8f93\u5165\u200b\u548c\u200b\u901a\u8fc7\u200b\u5e38\u6570\u200b \\(a\\) \u200b\u7f29\u653e\u200b\u8f93\u5165\u200b\u3002\u200b\u4ece\u6280\u672f\u4e0a\u200b\u8bb2\u200b\uff0c\u200b\u5b83\u4eec\u200b\u662f\u200b\u52a0\u6cd5\u200b\u548c\u200b\u4e58\u6cd5\u200b\u7684\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5f15\u5165\u200b\u4e3a\u200b\uff08\u200b\u65b0\u200b\u7684\u200b\uff09\u200b\u4e00\u5143\u200b\u95e8\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u9700\u8981\u200b\u5e38\u6570\u200b \\(c,a\\) \u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u5b8c\u6574\u200b\u7684\u200b\u7535\u8def\u200b\u5982\u4e0b\u200b\u6240\u793a\u200b\uff1a</p> 2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.20*6.000.20*4.000.20+1.000.20+-1.00-0.20*-10.37-0.53exp1.37-0.53+10.731.001/x <p>\u200b\u5e26\u6709\u200b sigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u4e8c\u7ef4\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\u3002\u200b\u8f93\u5165\u200b\u662f\u200b [x0,x1]\uff0c\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\uff08\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\uff09\u200b\u6743\u91cd\u200b\u662f\u200b [w0,w1,w2]\u3002\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u795e\u7ecf\u5143\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\u7684\u200b\u70b9\u79ef\u200b\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b sigmoid \u200b\u51fd\u6570\u200b\u5c06\u200b\u5176\u200b\u6fc0\u6d3b\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u8303\u56f4\u200b\u5728\u200b 0 \u200b\u5230\u200b 1 \u200b\u4e4b\u95f4\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u4e00\u957f\u4e32\u200b\u7684\u200b\u51fd\u6570\u200b\u5e94\u7528\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4f5c\u7528\u200b\u4e8e\u200b w,x \u200b\u7684\u200b\u70b9\u79ef\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u5b9e\u73b0\u200b\u7684\u200b\u51fd\u6570\u200b\u79f0\u4e3a\u200b sigmoid \u200b\u51fd\u6570\u200b \\(\\sigma(x)\\)\u3002\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0csigmoid \u200b\u51fd\u6570\u200b\u5173\u4e8e\u200b\u5176\u200b\u8f93\u5165\u200b\u7684\u200b\u5bfc\u6570\u200b\u5728\u200b\u8fdb\u884c\u200b\u5bfc\u6570\u200b\u8ba1\u7b97\u200b\u65f6\u200b\uff08\u200b\u5728\u200b\u5206\u5b50\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u548c\u200b\u51cf\u53bb\u200b 1 \u200b\u7684\u200b\u6709\u8da3\u200b\u90e8\u5206\u200b\u4e4b\u540e\u200b\uff09\u200b\u4f1a\u200b\u7b80\u5316\u200b\uff1a</p> \\[ \\sigma(x) = \\frac{1}{1+e^{-x}} \\\\\\\\ \\rightarrow \\hspace{0.3in} \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right)  = \\left( 1 - \\sigma(x) \\right) \\sigma(x) \\] <p>\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u770b\u5230\u200b\u7684\u200b\uff0c\u200b\u68af\u5ea6\u200b\u7684\u200b\u8ba1\u7b97\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u7b80\u5316\u200b\u5e76\u4e14\u200b\u53d8\u5f97\u200b\u975e\u5e38\u7b80\u5355\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0csigmoid \u200b\u8868\u8fbe\u5f0f\u200b\u63a5\u6536\u200b\u8f93\u5165\u200b 1.0 \u200b\u5e76\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u8f93\u51fa\u200b 0.73\u3002\u200b\u4e0a\u9762\u200b\u7684\u200b\u63a8\u5bfc\u200b\u663e\u793a\u200b\uff0c\u200b\u5c40\u90e8\u200b \u200b\u68af\u5ea6\u200b\u53ea\u200b\u9700\u200b (1 - 0.73) * 0.73 ~= 0.2\uff0c\u200b\u5c31\u200b\u50cf\u200b\u7535\u8def\u200b\u4e4b\u524d\u200b\u8ba1\u7b97\u200b\u7684\u200b\u4e00\u6837\u200b\uff08\u200b\u89c1\u200b\u4e0a\u9762\u200b\u7684\u200b\u56fe\u50cf\u200b\uff09\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u7528\u200b\u5355\u4e2a\u200b\u7b80\u5355\u200b\u800c\u200b\u9ad8\u6548\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u5b8c\u6210\u200b\uff08\u200b\u5e76\u4e14\u200b\u51cf\u5c11\u200b\u4e86\u200b\u6570\u503c\u200b\u95ee\u9898\u200b\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u4efb\u4f55\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u7ec4\u5408\u6210\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u95e8\u5c06\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u8fd9\u4e2a\u200b\u795e\u7ecf\u5143\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>w = [2,-3,-3] # assume some random weights and data\nx = [-1, -2]\n\n# forward pass\ndot = w[0]*x[0] + w[1]*x[1] + w[2]\nf = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n\n# backward pass through the neuron (backpropagation)\nddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\ndx = [w[0] * ddot, w[1] * ddot] # backprop into x\ndw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w\n# we're done! we have the gradients on the inputs to the circuit\n</code></pre> <p>\u200b\u5b9e\u65bd\u200b\u6280\u5de7\u200b\uff1a\u200b\u5206\u9636\u6bb5\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u3002\u200b\u5982\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u6240\u793a\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u5206\u89e3\u6210\u200b\u5bb9\u6613\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u9636\u6bb5\u200b\u603b\u662f\u200b\u6709\u7528\u200b\u7684\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4e2d\u95f4\u200b\u53d8\u91cf\u200b<code>dot</code>\uff0c\u200b\u8be5\u200b\u53d8\u91cf\u200b\u4fdd\u5b58\u200b\u4e86\u200b<code>w</code>\u200b\u548c\u200b<code>x</code>\u200b\u4e4b\u95f4\u200b\u70b9\u79ef\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4fbf\u200b\u4f9d\u6b21\u200b\u8ba1\u7b97\u200b\uff08\u200b\u6309\u200b\u76f8\u53cd\u200b\u7684\u200b\u987a\u5e8f\u200b\uff09\u200b\u76f8\u5e94\u200b\u7684\u200b\u53d8\u91cf\u200b\uff08\u200b\u4f8b\u5982\u200b<code>ddot</code>\uff0c\u200b\u6700\u7ec8\u200b\u662f\u200b<code>dw, dx</code>\uff09\uff0c\u200b\u8fd9\u4e9b\u200b\u53d8\u91cf\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u53d8\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002</p> <p>\u200b\u672c\u8282\u200b\u7684\u200b\u91cd\u70b9\u200b\u662f\u200b\uff0c\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u7ec6\u8282\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u5c06\u200b\u524d\u200b\u5411\u200b\u51fd\u6570\u200b\u7684\u200b\u54ea\u4e9b\u200b\u90e8\u5206\u200b\u89c6\u4e3a\u200b\u201c\u200b\u95e8\u200b\u201d\uff08gates\uff09\uff0c\u200b\u90fd\u200b\u662f\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u3002\u200b\u4e86\u89e3\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u54ea\u4e9b\u200b\u90e8\u5206\u200b\u5177\u6709\u200b\u5bb9\u6613\u200b\u8ba1\u7b97\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u6709\u52a9\u4e8e\u200b\u7528\u200b\u6700\u5c11\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b\u52aa\u529b\u200b\u5c06\u200b\u5b83\u4eec\u200b\u94fe\u63a5\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_5","title":"\u5b9e\u8df5\u200b\u4e2d\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff1a\u200b\u5206\u9636\u6bb5\u200b\u8ba1\u7b97","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\u6765\u770b\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f62\u5f0f\u200b\u5982\u4e0b\u200b\u7684\u200b\u51fd\u6570\u200b\uff1a</p> \\[ f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2} \\] <p>\u200b\u660e\u786e\u200b\u5730\u8bf4\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5b8c\u5168\u200b\u6ca1\u6709\u200b\u7528\u200b\uff0c\u200b\u4e5f\u200b\u4e0d\u200b\u6e05\u695a\u200b\u4e3a\u4ec0\u4e48\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u8ba1\u7b97\u200b\u5b83\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u9664\u4e86\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u7684\u200b\u597d\u200b\u4f8b\u5b50\u200b\u3002\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\u5f3a\u8c03\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u8bd5\u56fe\u200b\u5bf9\u200b\\(x\\)\u200b\u6216\u200b\\(y\\)\u200b\u8fdb\u884c\u200b\u5fae\u5206\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u4f1a\u200b\u5f97\u5230\u200b\u975e\u5e38\u200b\u5927\u200b\u548c\u200b\u590d\u6742\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u8fd9\u6837\u200b\u505a\u200b\u662f\u200b\u5b8c\u5168\u200b\u4e0d\u5fc5\u8981\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u200b\u9700\u8981\u200b\u6709\u200b\u4e00\u4e2a\u200b\u663e\u5f0f\u200b\u5730\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u8ba1\u7b97\u200b\u5b83\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u8fd9\u6837\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u7ed3\u6784\u200b\uff1a</p> <pre><code>x = 3 # example values\ny = -4\n\n# forward pass\nsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\nnum = x + sigy # numerator                               #(2)\nsigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\nxpy = x + y                                              #(4)\nxpysqr = xpy**2                                          #(5)\nden = sigx + xpysqr # denominator                        #(6)\ninvden = 1.0 / den                                       #(7)\nf = num * invden # done!                                 #(8)\n</code></pre> <p>\u200b\u54ce\u5440\u200b\uff0c\u200b\u5728\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8ba1\u7b97\u200b\u4e86\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ee5\u200b\u4e00\u79cd\u200b\u7ed3\u6784\u5316\u200b\u7684\u200b\u65b9\u5f0f\u200b\u7f16\u5199\u200b\u4e86\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u542b\u200b\u591a\u4e2a\u200b\u4e2d\u95f4\u200b\u53d8\u91cf\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\u90fd\u200b\u53ea\u662f\u200b\u5df2\u77e5\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u7684\u200b\u7b80\u5355\u200b\u8868\u8fbe\u5f0f\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u975e\u5e38\u5bb9\u6613\u200b\uff1a\u200b\u6211\u4eec\u200b\u5c06\u200b\u53cd\u5411\u200b\u4f20\u9012\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u524d\u5411\u200b\u4f20\u9012\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u53d8\u91cf\u200b\uff08<code>sigy, num, sigx, xpy, xpysqr, den, invden</code>\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u53d8\u91cf\u200b\uff0c\u200b\u4f46\u200b\u4ee5\u200b<code>d</code>\u200b\u5f00\u5934\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u4fdd\u5b58\u200b\u7535\u8def\u200b\u8f93\u51fa\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u8be5\u200b\u53d8\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd8\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u90fd\u200b\u6d89\u53ca\u200b\u8ba1\u7b97\u200b\u8be5\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u8be5\u200b\u8868\u8fbe\u5f0f\u200b\u7684\u200b\u68af\u5ea6\u200b\u76f8\u4e58\u200b\u8fdb\u884c\u200b\u94fe\u63a5\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6bcf\u200b\u4e00\u884c\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u4f1a\u200b\u7a81\u51fa\u200b\u663e\u793a\u200b\u5b83\u200b\u6240\u6307\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u7684\u200b\u54ea\u200b\u4e00\u90e8\u5206\u200b\uff1a</p> <pre><code># backprop f = num * invden\ndnum = invden # gradient on numerator                             #(8)\ndinvden = num                                                     #(8)\n# backprop invden = 1.0 / den \ndden = (-1.0 / (den**2)) * dinvden                                #(7)\n# backprop den = sigx + xpysqr\ndsigx = (1) * dden                                                #(6)\ndxpysqr = (1) * dden                                              #(6)\n# backprop xpysqr = xpy**2\ndxpy = (2 * xpy) * dxpysqr                                        #(5)\n# backprop xpy = x + y\ndx = (1) * dxpy                                                   #(4)\ndy = (1) * dxpy                                                   #(4)\n# backprop sigx = 1.0 / (1 + math.exp(-x))\ndx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n# backprop num = x + sigy\ndx += (1) * dnum                                                  #(2)\ndsigy = (1) * dnum                                                #(2)\n# backprop sigy = 1.0 / (1 + math.exp(-y))\ndy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n# done! phew\n</code></pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u4ee5\u4e0b\u51e0\u70b9\u200b\uff1a</p> <p>\u200b\u7f13\u5b58\u200b\u524d\u5411\u200b\u4f20\u9012\u200b\u53d8\u91cf\u200b\u3002\u200b\u4e3a\u4e86\u200b\u8ba1\u7b97\u200b\u53cd\u5411\u200b\u4f20\u9012\u200b\uff0c\u200b\u7f13\u5b58\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u4e00\u4e9b\u200b\u53d8\u91cf\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002\u200b\u5728\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u4e2d\u200b\uff0c\u200b\u60a8\u200b\u5e0c\u671b\u200b\u6784\u9020\u200b\u60a8\u200b\u7684\u200b\u4ee3\u7801\u200b\u4ee5\u4fbf\u200b\u7f13\u5b58\u200b\u8fd9\u4e9b\u200b\u53d8\u91cf\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5b83\u4eec\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u671f\u95f4\u200b\u662f\u200b\u53ef\u7528\u200b\u7684\u200b\u3002\u200b\u5982\u679c\u200b\u8fd9\u592a\u200b\u56f0\u96be\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u200b\u8ba1\u7b97\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4f1a\u200b\u6d6a\u8d39\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u68af\u5ea6\u200b\u5728\u200b\u5206\u53c9\u200bforks\u200b\u70b9\u200b\u76f8\u52a0\u200b\u3002\u200b\u524d\u5411\u200b\u8868\u8fbe\u5f0f\u200b\u591a\u6b21\u200b\u6d89\u53ca\u200b\u53d8\u91cf\u200bx\u200b\u548c\u200by\uff0c\u200b\u56e0\u6b64\u200b\u5728\u200b\u6267\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u5c0f\u5fc3\u200b\u4f7f\u7528\u200b<code>+=</code>\u200b\u800c\u200b\u4e0d\u662f\u200b<code>=</code>\u200b\u6765\u200b\u7d2f\u79ef\u200b\u8fd9\u4e9b\u200b\u53d8\u91cf\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u5426\u5219\u200b\u4f1a\u200b\u8986\u76d6\u200b\u5b83\u200b\uff09\u3002\u200b\u8fd9\u200b\u9075\u5faa\u200b\u4e86\u200b\u5fae\u79ef\u5206\u200b\u4e2d\u200b\u7684\u200b\u591a\u200b\u53d8\u91cf\u200b\u94fe\u5f0f\u6cd5\u5219\u200bmultivariable chain rule\uff0c\u200b\u8be5\u200b\u6cd5\u5219\u200b\u89c4\u5b9a\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u53d8\u91cf\u200b\u5206\u652f\u200b\u5230\u200b\u7535\u8def\u200b\u7684\u200b\u4e0d\u540c\u200b\u90e8\u5206\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6d41\u56de\u200b\u8be5\u200b\u53d8\u91cf\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c06\u200b\u76f8\u52a0\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_6","title":"\u53cd\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f","text":"<p>\u200b\u6709\u8da3\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8bb8\u591a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u68af\u5ea6\u200b\u53ef\u4ee5\u200b\u76f4\u89c2\u200b\u5730\u200b\u89e3\u91ca\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u4e09\u4e2a\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u95e8\u200b\uff08\u200b\u52a0\u6cd5\u200b\u3001\u200b\u4e58\u6cd5\u200b\u3001\u200b\u6700\u5927\u503c\u200b\uff09\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u671f\u95f4\u200b\u90fd\u200b\u6709\u200b\u975e\u5e38\u7b80\u5355\u200b\u7684\u200b\u89e3\u91ca\u200b\u3002\u200b\u8003\u8651\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\uff1a</p> 3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.00*2.002.00max-10.002.00+-20.001.00*2 <p>\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\u53ef\u4ee5\u200b\u5c55\u793a\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u9012\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6267\u884c\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4ee5\u200b\u8ba1\u7b97\u200b\u8f93\u5165\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u6c42\u548c\u200b\u64cd\u4f5c\u200b\u5c06\u200b\u68af\u5ea6\u200b\u5747\u5300\u200b\u5206\u914d\u200b\u5230\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u3002\u200b\u6700\u5927\u503c\u200b\u64cd\u4f5c\u200b\u5c06\u200b\u68af\u5ea6\u200b\u8def\u7531\u200broute\u200b\u5230\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u4e58\u6cd5\u200b\u95e8\u200b\u83b7\u53d6\u200b\u8f93\u5165\u200b\u6fc0\u6d3b\u200b\uff0c\u200b\u4ea4\u6362\u200b\u5b83\u4eec\u200b\u5e76\u200b\u4e58\u4ee5\u200b\u5b83\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u4ee5\u4e0b\u200b\u60c5\u51b5\u200b\uff1a</p> <p>\u200b\u52a0\u6cd5\u200b\u95e8\u200b \u200b\u603b\u662f\u200b\u5c06\u200b\u68af\u5ea6\u200b\u5206\u914d\u200b\u5230\u200b\u5176\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u4e2d\u200b\uff0c\u200b\u4e0d\u7ba1\u200b\u5b83\u4eec\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u671f\u95f4\u200b\u7684\u200b\u503c\u200b\u662f\u200b\u4ec0\u4e48\u200b\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u52a0\u6cd5\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u4ec5\u4e3a\u200b+1.0\uff0c\u200b\u56e0\u6b64\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c06\u200b\u5b8c\u5168\u200b\u7b49\u4e8e\u200b\u8f93\u51fa\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5c06\u200b\u4e58\u4ee5\u200b1.0\uff08\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff09\u3002\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\u4e2d\u200b\uff0c\u200b\u8bf7\u200b\u6ce8\u610f\u200b\u52a0\u6cd5\u200b\u95e8\u5c06\u200b\u68af\u5ea6\u200b2.00\u200b\u5747\u5300\u200b\u5206\u914d\u200b\u7ed9\u200b\u4e86\u200b\u5176\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5e76\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> <p>\u200b\u6700\u5927\u503c\u200b\u95e8\u200b \u200b\u8def\u7531\u200b\u68af\u5ea6\u200b\u3002\u200b\u4e0e\u200b\u5c06\u200b\u68af\u5ea6\u200b\u4e0d\u53d8\u200b\u5730\u200b\u5206\u914d\u200b\u7ed9\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u7684\u200b\u52a0\u6cd5\u200b\u95e8\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6700\u5927\u503c\u200b\u95e8\u5c06\u200b\u68af\u5ea6\u200b\uff08\u200b\u4e0d\u53d8\u200b\u5730\u200b\uff09\u200b\u5206\u914d\u200b\u7ed9\u200b\u5176\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\uff08\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u9012\u200b\u671f\u95f4\u200b\u5177\u6709\u200b\u6700\u9ad8\u503c\u200b\u7684\u200b\u8f93\u5165\u200b\uff09\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6700\u5927\u503c\u200b\u95e8\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u5bf9\u4e8e\u200b\u6700\u9ad8\u503c\u200b\u4e3a\u200b1.0\uff0c\u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u503c\u4e3a\u200b0.0\u3002\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u7535\u8def\u200b\u4e2d\u200b\uff0c\u200b\u6700\u5927\u503c\u200b\u64cd\u4f5c\u200b\u5c06\u200b\u68af\u5ea6\u200b2.00\u200b\u8def\u7531\u200b\u5230\u200b\u4e86\u200b\u53d8\u91cf\u200bz\u200b\u4e0a\u200b\uff0c\u200b\u56e0\u4e3a\u200bz\u200b\u7684\u200b\u503c\u200b\u6bd4\u200bw\u200b\u9ad8\u200b\uff0c\u200b\u800c\u200bw\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u4fdd\u6301\u200b\u4e3a\u200b\u96f6\u200b\u3002</p> <p>\u200b\u4e58\u6cd5\u200b\u95e8\u200b \u200b\u7a0d\u5fae\u200b\u96be\u4ee5\u200b\u89e3\u91ca\u200b\u4e00\u4e9b\u200b\u3002\u200b\u5b83\u200b\u7684\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u662f\u200b\u8f93\u5165\u200b\u503c\u200b\uff08\u200b\u9664\u4e86\u200b\u4ea4\u6362\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u8fd9\u4e2a\u200b\u68af\u5ea6\u200b\u4f1a\u200b\u5728\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u4e2d\u200b\u4e0e\u5176\u200b\u8f93\u51fa\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u76f8\u4e58\u200b\u3002\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e2d\u200b\uff0cx\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e3a\u200b-8.00\uff0c\u200b\u8fd9\u662f\u200b-4.00 x 2.00\u3002</p> <p>\u200b\u4e0d\u200b\u76f4\u89c2\u200b\u7684\u200b\u6548\u679c\u200b\u53ca\u5176\u200b\u5f71\u54cd\u200b\uff1a\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5982\u679c\u200b\u4e58\u6cd5\u200b\u95e8\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff0c\u200b\u800c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u5927\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4e58\u6cd5\u200b\u95e8\u4f1a\u200b\u6267\u884c\u200b\u4e00\u4e9b\u200b\u7565\u5fae\u200b\u4e0d\u200b\u76f4\u89c2\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff1a\u200b\u5b83\u200b\u5c06\u200b\u4e3a\u200b\u5c0f\u200b\u8f93\u5165\u200b\u5206\u914d\u200b\u76f8\u5bf9\u200b\u8f83\u5927\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u4e3a\u200b\u5927\u200b\u8f93\u5165\u200b\u5206\u914d\u200b\u5fae\u5c0f\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u7ebf\u6027\u200b\u5206\u7c7b\u5668\u200b\u4e2d\u200b\uff0c\u200b\u6743\u91cd\u200b\u4e0e\u200b\u8f93\u5165\u200b\u8fdb\u884c\u200b\u70b9\u79ef\u200b \\(w^Tx_i\\)\uff08\u200b\u76f8\u4e58\u200b\uff09\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6570\u636e\u200b\u7684\u200b\u89c4\u6a21\u200b\u4f1a\u200b\u5f71\u54cd\u200b\u6743\u91cd\u200b\u7684\u200b\u68af\u5ea6\u200b\u5927\u5c0f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u5728\u200b\u9884\u5904\u7406\u200b\u8fc7\u7a0b\u4e2d\u5c06\u200b\u6240\u6709\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u793a\u4f8b\u200b \\(x_i\\) \u200b\u4e58\u4ee5\u200b1000\uff0c\u200b\u90a3\u4e48\u200b\u6743\u91cd\u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u5c06\u200b\u5927\u200b1000\u200b\u500d\u200b\uff0c\u200b\u60a8\u200b\u9700\u8981\u200b\u5c06\u200b\u5b66\u4e60\u200b\u7387\u200b\u964d\u4f4e\u200b\u76f8\u540c\u200b\u7684\u200b\u56e0\u5b50\u200b\u6765\u200b\u8865\u507f\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4e3a\u4ec0\u4e48\u200b\u9884\u5904\u7406\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\uff0c\u200b\u6709\u65f6\u200b\u662f\u200b\u4ee5\u200b\u5fae\u5999\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff01\u200b\u5e76\u4e14\u200b\u5bf9\u200b\u68af\u5ea6\u200b\u6d41\u52a8\u200b\u65b9\u5f0f\u200b\u6709\u200b\u76f4\u89c2\u200b\u7684\u200b\u7406\u89e3\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u8c03\u8bd5\u200b\u5176\u4e2d\u200b\u7684\u200b\u4e00\u4e9b\u200b\u60c5\u51b5\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_7","title":"\u77e2\u91cf\u5316\u200b\u64cd\u4f5c\u200b\u7684\u200b\u68af\u5ea6","text":"<p>\u200b\u4e0a\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u6d89\u53ca\u200b\u5355\u4e2a\u200b\u53d8\u91cf\u200b\uff0c\u200b\u4f46\u200b\u6240\u6709\u200b\u6982\u5ff5\u200b\u90fd\u200b\u4ee5\u200b\u76f4\u89c2\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6269\u5c55\u200b\u5230\u200b\u77e9\u9635\u200b\u548c\u200b\u5411\u91cf\u200b\u64cd\u4f5c\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5fc5\u987b\u200b\u66f4\u52a0\u200b\u5173\u6ce8\u200b\u7ef4\u5ea6\u200b\u548c\u200b\u8f6c\u7f6e\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u77e9\u9635\u200b-\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u68af\u5ea6\u200b\u3002\u200b\u53ef\u80fd\u200b\u6700\u200b\u68d8\u624b\u200b\u7684\u200b\u64cd\u4f5c\u200b\u4e4b\u4e00\u200b\u662f\u200b\u77e9\u9635\u200b-\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u901a\u7528\u5316\u200b\u4e86\u200b\u6240\u6709\u200b\u77e9\u9635\u200b-\u200b\u5411\u91cf\u200b\u548c\u200b\u5411\u91cf\u200b-\u200b\u5411\u91cf\u200b\u4e58\u6cd5\u200b\u64cd\u4f5c\u200b\uff09\uff1a</p> <pre><code># forward pass\nW = np.random.randn(5, 10)\nX = np.random.randn(10, 3)\nD = W.dot(X)\n\n# now suppose we had the gradient on D from above in the circuit\ndD = np.random.randn(*D.shape) # same shape as D\ndW = dD.dot(X.T) #.T gives the transpose of the matrix\ndX = W.T.dot(dD)\n</code></pre> <p>\u200b\u63d0\u793a\u200b\uff1a\u200b\u4f7f\u7528\u200b\u7ef4\u5ea6\u200b\u5206\u6790\u200b\uff01 \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u60a8\u200b\u4e0d\u200b\u9700\u8981\u200b\u8bb0\u4f4f\u200b <code>dW</code> \u200b\u548c\u200b <code>dX</code> \u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u53ef\u4ee5\u200b\u57fa\u4e8e\u200b\u7ef4\u5ea6\u200b\u8f7b\u677e\u200b\u91cd\u65b0\u200b\u63a8\u5bfc\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u6743\u91cd\u200b <code>dW</code> \u200b\u5728\u200b\u8ba1\u7b97\u200b\u540e\u200b\u5fc5\u987b\u200b\u4e0e\u200b <code>W</code> \u200b\u7684\u200b\u5927\u5c0f\u200b\u76f8\u540c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5b83\u200b\u5fc5\u987b\u200b\u4f9d\u8d56\u4e8e\u200b <code>X</code> \u200b\u548c\u200b <code>dD</code> \u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u5f53\u200b <code>X,W</code> \u200b\u90fd\u200b\u662f\u200b\u5355\u4e2a\u200b\u6570\u5b57\u200b\u800c\u200b\u4e0d\u662f\u200b\u77e9\u9635\u200b\u65f6\u200b\u5c31\u662f\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff09\u3002\u200b\u603b\u662f\u200b\u6709\u200b\u4e00\u79cd\u200b\u786e\u5207\u200b\u7684\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4ee5\u200b\u4f7f\u200b\u7ef4\u5ea6\u200b\u914d\u5408\u8d77\u6765\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b <code>X</code> \u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b [10 x 3]\uff0c<code>dD</code> \u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b [5 x 3]\uff0c\u200b\u90a3\u4e48\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b <code>dW</code>\uff0c\u200b\u800c\u200b <code>W</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b [5 x 10]\uff0c\u200b\u90a3\u4e48\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u7684\u200b\u552f\u4e00\u200b\u65b9\u6cd5\u200b\u5c31\u662f\u200b <code>dD.dot(X.T)</code>\uff0c\u200b\u5982\u200b\u4e0a\u200b\u6240\u793a\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u5c0f\u800c\u200b\u660e\u786e\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002\u200b\u4e00\u4e9b\u200b\u4eba\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u9996\u6b21\u200b\u4e3a\u200b\u4e00\u4e9b\u200b\u77e2\u91cf\u5316\u200b\u8868\u8fbe\u5f0f\u200b\u63a8\u5bfc\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u6709\u4e9b\u200b\u56f0\u96be\u200b\u3002\u200b\u6211\u4eec\u200b\u5efa\u8bae\u200b\u9996\u5148\u200b\u660e\u786e\u200b\u5730\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u6700\u5c0f\u200b\u7684\u200b\u77e2\u91cf\u5316\u200b\u793a\u4f8b\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u7eb8\u200b\u4e0a\u200b\u63a8\u5bfc\u200b\u68af\u5ea6\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u6a21\u5f0f\u200b\u63a8\u5e7f\u200b\u5230\u200b\u5176\u200b\u9ad8\u6548\u200b\u7684\u200b\u77e2\u91cf\u5316\u200b\u5f62\u5f0f\u200b\u3002</p> <p>Erik Learned-Miller \u200b\u8fd8\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e00\u4efd\u200b\u4e0e\u200b\u77e9\u9635\u200b/\u200b\u5411\u91cf\u200b\u5bfc\u6570\u200b\u6709\u5173\u200b\u7684\u200b\u66f4\u957f\u200b\u7684\u200b\u76f8\u5173\u200b\u6587\u6863\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u5b83\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\u627e\u5230\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_8","title":"\u603b\u7ed3","text":"<ul> <li>\u200b\u6211\u4eec\u200b\u5bf9\u200b\u68af\u5ea6\u200b\u7684\u200b\u542b\u4e49\u200b\u6709\u200b\u4e86\u200b\u76f4\u89c2\u200b\u8ba4\u8bc6\u200b\uff0c\u200b\u4e86\u89e3\u200b\u4e86\u200b\u5b83\u4eec\u200b\u5728\u200b\u7535\u8def\u200b\u4e2d\u200b\u5982\u4f55\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200bbackwards \uff0c\u200b\u4ee5\u53ca\u200b\u5b83\u4eec\u200b\u5982\u4f55\u200b\u4f20\u8fbe\u200b\u7535\u8def\u200b\u7684\u200b\u54ea\u4e2a\u200b\u90e8\u5206\u200b\u5e94\u8be5\u200b\u589e\u52a0\u200b\u6216\u200b\u51cf\u5c11\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4ee5\u200b\u4f55\u79cd\u200b\u529b\u91cf\u200b\u6765\u200b\u4f7f\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u53d8\u9ad8\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u5206\u9636\u6bb5\u200b\u8ba1\u7b97\u200b\u5728\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u5b9e\u9645\u200b\u5b9e\u73b0\u200b\u4e2d\u200b\u7684\u200b\u91cd\u8981\u6027\u200b\u3002\u200b\u60a8\u200b\u59cb\u7ec8\u200b\u5e0c\u671b\u200b\u5c06\u200b\u51fd\u6570\u200b\u5206\u89e3\u200b\u4e3a\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u63a8\u5bfc\u200b\u51fa\u200b\u5c40\u90e8\u200b\u68af\u5ea6\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u94fe\u5f0f\u6cd5\u5219\u200b\u8fde\u63a5\u200b\u5b83\u4eec\u200b\u3002\u200b\u5173\u952e\u200b\u662f\u200b\uff0c\u200b\u60a8\u200b\u51e0\u4e4e\u200b\u6c38\u8fdc\u200b\u4e0d\u200b\u5e0c\u671b\u200b\u5728\u200b\u7eb8\u200b\u4e0a\u200b\u5199\u51fa\u200b\u8fd9\u4e9b\u200b\u8868\u8fbe\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u4ee5\u200b\u7b26\u53f7\u200b\u65b9\u5f0f\u200b\u5b8c\u5168\u200b\u4e0d\u540c\u200b\u5730\u200b\u8fdb\u884c\u200b\u5fae\u5206\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u60a8\u200b\u6c38\u8fdc\u200b\u4e0d\u200b\u9700\u8981\u200b\u8f93\u5165\u200b\u53d8\u91cf\u200b\u7684\u200b\u663e\u5f0f\u200b\u6570\u5b66\u200b\u65b9\u7a0b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8bf7\u200b\u5c06\u200b\u60a8\u200b\u7684\u200b\u8868\u8fbe\u5f0f\u200b\u5206\u89e3\u6210\u200b\u9636\u6bb5\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u53ef\u4ee5\u200b\u72ec\u7acb\u200b\u5730\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u9636\u6bb5\u200b\u8fdb\u884c\u200b\u5fae\u5206\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u9636\u6bb5\u200b\u5c06\u200b\u662f\u200b\u77e9\u9635\u200b\u5411\u91cf\u200b\u4e58\u6cd5\u200b\u3001\u200b\u6700\u5927\u200b\u64cd\u4f5c\u200b\u3001\u200b\u6c42\u548c\u200b\u64cd\u4f5c\u200b\u7b49\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u4e00\u6b21\u6027\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u53d8\u91cf\u200b\u3002</li> </ul> <p>\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f00\u59cb\u200b\u5b9a\u4e49\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u800c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u5c06\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u6709\u6548\u200b\u5730\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u5176\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u6700\u5177\u200b\u6982\u5ff5\u6027\u200b\u6311\u6218\u200b\u7684\u200b\u90e8\u5206\u200b\u5df2\u7ecf\u200b\u8fc7\u53bb\u200b\u4e86\u200b\uff01\u200b\u7136\u540e\u200b\uff0c\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5c31\u200b\u8fd1\u5728\u773c\u524d\u200b\u4e86\u200b\u3002</p>"},{"location":"courses/neural_network/optimization-2/#_9","title":"\u53c2\u8003\u8d44\u6599","text":"<ul> <li>Automatic differentiation in machine learning: a survey</li> </ul>"},{"location":"courses/preparation/python-numpy-tutorial/","title":"Python/Numpy\u200b\u6559\u7a0b","text":"<p>\u200b\u672c\u200b\u6559\u7a0b\u200b\u7531\u200b Justin Johnson\u200b\u539f\u521b\u200b</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u4f5c\u4e1a\u200b\u4e2d\u200b\u4f7f\u7528\u200bPython\u200b\u7f16\u7a0b\u8bed\u8a00\u200b\u3002Python\u200b\u672c\u8eab\u200b\u5c31\u662f\u200b\u4e00\u79cd\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u901a\u7528\u200b\u7f16\u7a0b\u8bed\u8a00\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u4e00\u4e9b\u200b\u6d41\u884c\u200b\u5e93\u200b\uff08numpy\u3001scipy\u3001matplotlib\uff09\u200b\u7684\u200b\u5e2e\u52a9\u200b\u4e0b\u200b\uff0c\u200b\u5b83\u200b\u6210\u4e3a\u200b\u4e86\u200b\u79d1\u5b66\u8ba1\u7b97\u200b\u7684\u200b\u5f3a\u5927\u200b\u73af\u5883\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4f60\u4eec\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u4eba\u200b\u90fd\u200b\u5bf9\u200bPython\u200b\u548c\u200bnumpy\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7ecf\u9a8c\u200b\uff1b\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u4eba\u200b\u6765\u8bf4\u200b\uff0c\u200b\u672c\u8282\u200b\u5c06\u200b\u4f5c\u4e3a\u200b\u5173\u4e8e\u200bPython\u200b\u7f16\u7a0b\u8bed\u8a00\u200b\u53ca\u5176\u200b\u5728\u200b\u79d1\u5b66\u8ba1\u7b97\u200b\u4e2d\u200b\u7684\u200b\u4f7f\u7528\u200b\u7684\u200b\u901f\u6210\u200b\u8bfe\u7a0b\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4ecb\u7ecd\u200bnotebook\uff0c\u200b\u8fd9\u662f\u200b\u4fee\u6539\u200bPython\u200b\u4ee3\u7801\u200b\u7684\u200b\u4e00\u79cd\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\u7684\u200b\u65b9\u5f0f\u200b\u3002\u200b\u4f60\u4eec\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e9b\u200b\u4eba\u200b\u53ef\u80fd\u200b\u4ee5\u524d\u200b\u6709\u200b\u4e0d\u540c\u200b\u8bed\u8a00\u200b\u7684\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5efa\u8bae\u200b\u53c2\u8003\u200b\uff1a \u200b\u7528\u4e8e\u200bMatlab\u200b\u7528\u6237\u200b\u7684\u200bNumPy\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200bR\u200b\u7528\u6237\u200b\u7684\u200bPython\uff0c\u200b\u548c\u200b/\u200b\u6216\u200b\u9002\u7528\u200b\u4e8e\u200bSAS\u200b\u7528\u6237\u200b\u7684\u200bPython\u3002</p>"},{"location":"courses/preparation/python-numpy-tutorial/#jupytercolab-notebooks","title":"Jupyter\u200b\u548c\u200bColab Notebooks","text":"<p>\u200b\u5728\u200b\u6df1\u5165\u7814\u7a76\u200bPython\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u60f3\u200b\u7b80\u5355\u200b\u5730\u200b\u8c08\u8c08\u200b \u200b\u7b14\u8bb0\u672c\u200b\u3002Jupyter\u200b\u7b14\u8bb0\u672c\u200b\u5141\u8bb8\u200b\u60a8\u200b\u5728\u200bweb\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u672c\u5730\u200b\u7f16\u5199\u200b\u548c\u200b\u6267\u884c\u200bPython\u200b\u4ee3\u7801\u200b\u3002Jupyter \u200b\u7b14\u8bb0\u672c\u200b\u4f7f\u5f97\u200b\u4fee\u6539\u200b\u4ee3\u7801\u200b\u5e76\u200b\u96f6\u96f6\u661f\u661f\u200b\u5730\u200b\u6267\u884c\u200b\u4ee3\u7801\u200b\u53d8\u5f97\u200b\u975e\u5e38\u5bb9\u6613\u200b\uff1b\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5b83\u4eec\u200b\u88ab\u200b\u5e7f\u6cdb\u200b\u7528\u4e8e\u200b\u79d1\u5b66\u8ba1\u7b97\u200b\u3002\u200b\u53e6\u4e00\u65b9\u9762\u200b\uff0cColab\u200b\u662f\u200b\u8c37\u6b4c\u200b\u98ce\u683c\u200b\u7684\u200bJupyter \u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u7279\u522b\u200b\u9002\u5408\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6570\u636e\u5206\u6790\u200b\uff0c\u200b\u5b8c\u5168\u200b\u5728\u200b\u4e91\u4e2d\u200b\u8fd0\u884c\u200b\u3002 Colab\u200b\u662f\u200b\u57fa\u4e8e\u200bsteroids\u200b\u7684\u200bJupyter\u200b\u7b14\u8bb0\u672c\u200b\uff1a\u200b\u5b83\u200b\u662f\u200b\u514d\u8d39\u200b\u7684\u200b\uff0c\u200b\u65e0\u9700\u200b\u5b89\u88c5\u200b\uff0c\u200b\u9884\u88c5\u200b\u4e86\u200b\u8bb8\u591a\u200b\u8f6f\u4ef6\u5305\u200b\uff0c\u200b\u5f88\u200b\u5bb9\u6613\u200b\u4e0e\u200b\u4e16\u754c\u200b\u5206\u4eab\u200b\uff0c\u200b\u5e76\u200b\u53d7\u76ca\u200b\u4e8e\u200b\u514d\u8d39\u200b\u8bbf\u95ee\u200bGPU\u200b\u548c\u200bTPU\u200b\u7b49\u200b\u786c\u4ef6\u200b\u52a0\u901f\u5668\u200b\uff08\u200b\u6709\u200b\u4e00\u4e9b\u200b\u6ce8\u610f\u4e8b\u9879\u200b\uff09\u3002</p> <p>\u200b\u5728\u200bColab\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u6559\u7a0b\u200b\uff08\u200b\u63a8\u8350\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5e0c\u671b\u200b\u5b8c\u5168\u200b\u5728\u200bColab\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u672c\u200b\u6559\u7a0b\u200b\uff0c\u200b\u8bf7\u5355\u51fb\u200b\u6b64\u200b\u9875\u9762\u200b\u9876\u90e8\u200b\u7684\u200b <code>Open in Colab</code> \u200b\u56fe\u6807\u200b\u3002 \u200b\u5728\u200bJupyter\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u6559\u7a0b\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200bJupyter\u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u7b14\u8bb0\u672c\u7535\u8111\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\u5df2\u200b\u6b63\u786e\u200b\u5b89\u88c5\u200b\uff08\u200b\u6839\u636e\u200b\u5b89\u88c5\u200b\u8bf4\u660e\u200b\uff09\uff0c\u200b\u6fc0\u6d3b\u200b\u5b83\u200b\uff0c\u200b\u7136\u540e\u200b\u8fd0\u884c\u200b<code>pip install notebook</code>\u200b\u5b89\u88c5\u200bJupyter\u200b\u7b14\u8bb0\u672c\u7535\u8111\u200b\u3002\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6253\u5f00\u200b\u7b14\u8bb0\u672c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u53f3\u952e\u200b\u5355\u51fb\u200b\u9875\u9762\u200b\u5e76\u200b\u9009\u62e9\u200b\u201c\u200b\u5c06\u200b\u9875\u9762\u200b\u53e6\u5b58\u4e3a\u200b\u201d\u200b\u5c06\u200b\u5176\u200b\u4e0b\u8f7d\u200b\u5230\u200b\u60a8\u200b\u9009\u62e9\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002\u200b\u7136\u540e\u200b <code>cd</code>\u200b\u5230\u200b\u8be5\u200b\u76ee\u5f55\u200b\u5e76\u200b\u8fd0\u884c\u200b<code>jupyter notebook</code>\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u81ea\u52a8\u200b\u542f\u52a8\u200b<code>http://localhost:8888</code>\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u670d\u52a1\u5668\u200b\u3002 \u200b\u5982\u679c\u200b\u4e00\u5207\u6b63\u5e38\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u6837\u200b\u7684\u200b\u5c4f\u5e55\u200b\uff0c\u200b\u663e\u793a\u200b\u5f53\u524d\u76ee\u5f55\u200b\u4e2d\u200b\u6240\u6709\u200b\u53ef\u7528\u200b\u7684\u200b\u7b14\u8bb0\u672c\u7535\u8111\u200b\u3002\u200b\u5355\u51fb\u200b<code>jupyter-notebook-tutorial.ipynb</code> \u200b\u5e76\u200b\u6309\u7167\u200b\u7b14\u8bb0\u672c\u200b\u4e0a\u200b\u7684\u200b\u8bf4\u660e\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\u3002\u200b\u5426\u5219\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u7ee7\u7eed\u200b\u9605\u8bfb\u200b\u4e0b\u9762\u200b\u7684\u200b\u6559\u7a0b\u200b\u548c\u200b\u4ee3\u7801\u200b\u7247\u6bb5\u200b\u3002</p>"},{"location":"courses/preparation/python-numpy-tutorial/#python","title":"Python","text":"<p>Python\u200b\u662f\u200b\u4e00\u79cd\u200b\u9ad8\u7ea7\u200b\u3001\u200b\u52a8\u6001\u200b\u7c7b\u578b\u200b\u7684\u200b\u591a\u200b\u8303\u5f0f\u200b\u7f16\u7a0b\u200b(dynamically typed multiparadigm programming)\u200b\u8bed\u8a00\u200b\u3002Python\u200b\u4ee3\u7801\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u51e0\u4e4e\u200b\u50cf\u200b\u4f2a\u200b\u4ee3\u7801\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5141\u8bb8\u200b\u4f60\u200b\u7528\u200b\u5f88\u5c11\u200b\u7684\u200b\u4ee3\u7801\u200b\u884c\u6765\u200b\u8868\u8fbe\u200b\u975e\u5e38\u200b\u5f3a\u5927\u200b\u7684\u200b\u601d\u60f3\u200b\uff0c\u200b\u540c\u65f6\u200b\u975e\u5e38\u200b\u6613\u8bfb\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200bPython\u200b\u4e2d\u200b\u7ecf\u5178\u200b\u5feb\u901f\u200b\u6392\u5e8f\u200b\u7b97\u6cd5\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5b9e\u73b0\u200b\u793a\u4f8b\u200b\uff1a</p> <pre><code>def quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nprint(quicksort([3,6,8,10,1,2,1]))\n# Prints \"[1, 1, 2, 3, 6, 8, 10]\"\n</code></pre>"},{"location":"courses/preparation/python-numpy-tutorial/#python_1","title":"Python \u200b\u7248\u672c","text":"<p>\u200b\u622a\u81f3\u200b2020\u200b\u5e74\u200b1\u200b\u6708\u200b1\u200b\u65e5\u200b\uff0cPython\u200b\u6b63\u5f0f\u200b\u505c\u6b62\u200b\u5bf9\u200b<code>python2</code>\u200b\u7684\u200b\u652f\u6301\u200b\u652f\u6301\u200b \u3002\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6240\u6709\u200b\u4ee3\u7801\u200b\u5c06\u200b\u4f7f\u7528\u200bPython 3.7\u3002\u200b\u5728\u200b\u7ee7\u7eed\u200b\u672c\u200b\u6559\u7a0b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u5df2\u200b\u6309\u7167\u200b\u8bbe\u7f6e\u200b\u8bf4\u660e\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5e76\u200b\u6b63\u786e\u200b\u5b89\u88c5\u200b\u4e86\u200b<code>python3</code>\u200b\u865a\u62df\u73af\u5883\u200b\u3002\u200b\u5728\u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\u5e76\u200b\u8fd0\u884c\u200b<code>python --version</code>\u200b\u547d\u4ee4\u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u547d\u4ee4\u884c\u200b\u53cc\u91cd\u200b\u68c0\u67e5\u200b\u60a8\u200b\u7684\u200bPython\u200b\u7248\u672c\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_1","title":"\u57fa\u672c\u200b\u6570\u636e\u7c7b\u578b","text":"<p>\u200b\u4e0e\u200b\u5927\u591a\u6570\u200b\u8bed\u8a00\u200b\u4e00\u6837\u200b\uff0cPython\u200b\u6709\u200b\u8bb8\u591a\u200b\u57fa\u672c\u200b\u7c7b\u578b\u200b\uff0c\u200b\u5305\u62ec\u200b\u6574\u6570\u200b\u3001\u200b\u6d6e\u70b9\u6570\u200b\u3001\u200b\u5e03\u5c14\u503c\u200b\u548c\u200b\u5b57\u7b26\u4e32\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u884c\u4e3a\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u7f16\u7a0b\u8bed\u8a00\u200b\u76f8\u4f3c\u200b\u3002</p> <p>Numbers\u200b\u6570\u5b57\u200b:\u200b\u6574\u6570\u200b\u548c\u200b\u6d6e\u70b9\u6570\u200b\u7684\u200b\u5de5\u4f5c\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u8bed\u8a00\u200b\u76f8\u540c\u200b\u3002</p> <p><pre><code>x = 3\nprint(type(x)) # Prints \"&lt;class 'int'&gt;\"\nprint(x)       # Prints \"3\"\nprint(x + 1)   # Addition; prints \"4\"\nprint(x - 1)   # Subtraction; prints \"2\"\nprint(x * 2)   # Multiplication; prints \"6\"\nprint(x ** 2)  # Exponentiation; prints \"9\"\nx += 1\nprint(x)  # Prints \"4\"\nx *= 2\nprint(x)  # Prints \"8\"\ny = 2.5\nprint(type(y)) # Prints \"&lt;class 'float'&gt;\"\nprint(y, y + 1, y * 2, y ** 2) # Prints \"2.5 3.5 5.0 6.25\"\n</code></pre> \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4e0e\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u8bed\u8a00\u200b\u4e0d\u540c\u200b\uff0cPython\u200b\u6ca1\u6709\u200b\u4e00\u5143\u200b\u9012\u589e\u200b\uff08<code>x++</code>\uff09\u200b\u6216\u200b\u9012\u51cf\u200b\uff08<code>x--</code>\uff09\u200b\u64cd\u4f5c\u7b26\u200b\u3002</p> <p>Python\u200b\u8fd8\u200b\u4e3a\u200b\u590d\u6570\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5185\u7f6e\u200b\u7c7b\u578b\u200b\uff1b\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u6240\u6709\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002</p> <p>Booleans\u200b\u5e03\u5c14\u503c\u200b\uff1aPython\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u6240\u6709\u200b\u901a\u5e38\u200b\u7684\u200b\u5e03\u5c14\u200b\u903b\u8f91\u200b\u8fd0\u7b97\u7b26\u200b\uff0c\u200b\u4f46\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u82f1\u6587\u5355\u8bcd\u200b\u800c\u200b\u4e0d\u662f\u200b\u7b26\u53f7\u200b\uff08<code>&amp;&amp;</code>, <code>||</code>, \u200b\u7b49\u200b\uff09\uff1a</p> <pre><code>t = True\nf = False\nprint(type(t)) # Prints \"&lt;class 'bool'&gt;\"\nprint(t and f) # Logical AND; prints \"False\"\nprint(t or f)  # Logical OR; prints \"True\"\nprint(not t)   # Logical NOT; prints \"False\"\nprint(t != f)  # Logical XOR; prints \"True\"\n</code></pre> <p>Strings: Python\u200b\u975e\u5e38\u200b\u652f\u6301\u200b\u5b57\u7b26\u4e32\u200b</p> <pre><code>hello = 'hello'    # String literals can use single quotes\nworld = \"world\"    # or double quotes; it does not matter.\nprint(hello)       # Prints \"hello\"\nprint(len(hello))  # String length; prints \"5\"\nhw = hello + ' ' + world  # String concatenation\nprint(hw)  # prints \"hello world\"\nhw12 = '%s %s %d' % (hello, world, 12)  # sprintf style string formatting\nprint(hw12)  # prints \"hello world 12\"\n</code></pre> <p>\u200b\u5b57\u7b26\u4e32\u200b\u5bf9\u8c61\u200b\u6709\u200b\u5f88\u591a\u200b\u6709\u7528\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff1b\u200b\u4f8b\u5982\u200b\uff1a</p> <p><pre><code>s = \"hello\"\nprint(s.capitalize())  # Capitalize a string; prints \"Hello\"\nprint(s.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(s.rjust(7))      # Right-justify a string, padding with spaces; prints \"  hello\"\nprint(s.center(7))     # Center a string, padding with spaces; prints \" hello \"\nprint(s.replace('l', '(ell)'))  # Replace all instances of one substring with another;\n                                # prints \"he(ell)(ell)o\"\nprint('  world '.strip())  # Strip leading and trailing whitespace; prints \"world\"\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u6240\u6709\u200b\u5b57\u7b26\u4e32\u200b\u65b9\u6cd5\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#containers","title":"Containers\u200b\u5bb9\u5668","text":"<p>Python \u200b\u5305\u62ec\u200b\u51e0\u79cd\u200b\u5185\u7f6e\u200b\u7684\u200b\u5bb9\u5668\u200b\u7c7b\u578b\u200b\uff1a\u200b\u5217\u8868\u200b\uff08lists\uff09\u3001\u200b\u5b57\u5178\u200b\uff08dictionaries\uff09\u3001\u200b\u96c6\u5408\u200b\uff08sets\uff09\u200b\u548c\u200b\u5143\u7ec4\u200b\uff08tuples\uff09\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#lists","title":"Lists\u200b\u5217\u8868","text":"<p>\u200b\u5217\u8868\u200b\u662f\u200b Python \u200b\u4e2d\u200b\u7684\u200b\u6570\u7ec4\u200b\u7b49\u4ef7\u7269\u200b\uff0c\u200b\u4f46\u200b\u53ef\u200b\u8c03\u6574\u200b\u5927\u5c0f\u200b\u5e76\u4e14\u200b\u53ef\u4ee5\u200b\u5305\u542b\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u5143\u7d20\u200b\uff1a</p> <p><pre><code>xs = [3, 1, 2]    # Create a list\nprint(xs, xs[2])  # Prints \"[3, 1, 2] 2\"\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\nxs[2] = 'foo'     # Lists can contain elements of different types\nprint(xs)         # Prints \"[3, 1, 'foo']\"\nxs.append('bar')  # Add a new element to the end of the list\nprint(xs)         # Prints \"[3, 1, 'foo', 'bar']\"\nx = xs.pop()      # Remove and return the last element of the list\nprint(x, xs)      # Prints \"bar [3, 1, 'foo']\"\n</code></pre> \u200b\u50cf\u200b\u5f80\u5e38\u200b\u4e00\u6837\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u6709\u5173\u200b\u5217\u8868\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002 \u200b\u5207\u7247\u200b\uff1a</p> <p>Slicing\u200b\u5207\u7247\u200b: \u200b\u9664\u4e86\u200b\u9010\u4e2a\u200b\u8bbf\u95ee\u200b\u5217\u8868\u200b\u5143\u7d20\u200b\u5916\u200b\uff0cPython\u200b\u8fd8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u7b80\u6d01\u200b\u7684\u200b\u8bed\u6cd5\u200b\u6765\u200b\u8bbf\u95ee\u200b\u5b50\u200b\u5217\u8868\u200b\uff1b\u200b\u8fd9\u200b\u88ab\u200b\u79f0\u4e3a\u200b slicing\u200b\u5207\u7247\u200b:</p> <p><pre><code>nums = list(range(5))     # range is a built-in function that creates a list of integers\nprint(nums)               # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])          # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])           # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])           # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])            # Get a slice of the whole list; prints \"[0, 1, 2, 3, 4]\"\nprint(nums[:-1])          # Slice indices can be negative; prints \"[0, 1, 2, 3]\"\nnums[2:4] = [8, 9]        # Assign a new sublist to a slice\nprint(nums)               # Prints \"[0, 1, 8, 9, 4]\"\n</code></pre> \u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200bnumpy\u200b\u6570\u7ec4\u200b\u7684\u200b\u4e0a\u4e0b\u6587\u200b\u4e2d\u200b\u518d\u6b21\u200b\u770b\u5230\u200b\u5207\u7247\u200b\u3002</p> <p>Loops\u200b\u5faa\u73af\u200b: \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u8fd9\u6837\u200b\u5faa\u73af\u200b\u904d\u5386\u200b\u5217\u8868\u200b\u7684\u200b\u5143\u7d20\u200b\uff1a</p> <pre><code>animals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n# Prints \"cat\", \"dog\", \"monkey\", each on its own line.\n</code></pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u8981\u200b\u5728\u200b\u5faa\u73af\u200b\u4f53\u5185\u200b\u8bbf\u95ee\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u7684\u200b\u7d22\u5f15\u200b\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u7684\u200b<code>enumerate</code>\u200b\u51fd\u6570\u200b\u3002</p> <pre><code>animals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#%d: %s' % (idx + 1, animal))\n# Prints \"#1: cat\", \"#2: dog\", \"#3: monkey\", each on its own line\n</code></pre> <p>List comprehensions\u200b\u5217\u8868\u200b\u63a8\u5bfc\u200b\u5f0f\u200b: \u200b\u5728\u200b\u7f16\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u60f3\u8981\u200b\u5c06\u200b\u4e00\u79cd\u200b\u6570\u636e\u200b\u7c7b\u578b\u8f6c\u6362\u200b\u4e3a\u200b\u53e6\u200b\u4e00\u79cd\u200b\u3002\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b\u6765\u200b\u8bf4\u660e\u200b\uff0c\u200b\u8003\u8651\u200b\u4e0b\u9762\u200b\u8ba1\u7b97\u200b\u5e73\u65b9\u200b\u6570\u200b\u7684\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>nums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)   # Prints [0, 1, 4, 9, 16]\n</code></pre> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5217\u8868\u200b\u63a8\u5bfc\u200b\u5f0f\u200b\u6765\u200b\u7b80\u5316\u200b\u8fd9\u200b\u6bb5\u200b\u4ee3\u7801\u200b\uff1a</p> <pre><code>nums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)   # Prints [0, 1, 4, 9, 16]\n</code></pre> <p>\u200b\u5217\u8868\u200b\u63a8\u5bfc\u200b\u5f0f\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5305\u542b\u200b\u6761\u4ef6\u200b\uff1a</p> <pre><code>nums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)  # Prints \"[0, 4, 16]\"\n</code></pre> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#dictionaries","title":"Dictionaries\u200b\u5b57\u5178","text":"<p>\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u50a8\u5b58\u200b\uff08\u200b\u952e\u200b\uff0c\u200b\u503c\u200b\uff09\u200b\u5bf9\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bJava\u200b\u4e2d\u200b\u7684\u200b<code>Map</code>\u200b\u6216\u200bJavascript\u200b\u4e2d\u200b\u7684\u200b\u5bf9\u8c61\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u8fd9\u6837\u200b\u4f7f\u7528\u200b\u5b83\u200b\uff1a</p> <p><pre><code>d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\nd['fish'] = 'wet'     # Set an entry in a dictionary\nprint(d['fish'])      # Prints \"wet\"\n# print(d['monkey'])  # KeyError: 'monkey' not a key of d\nprint(d.get('monkey', 'N/A'))  # Get an element with a default; prints \"N/A\"\nprint(d.get('fish', 'N/A'))    # Get an element with a default; prints \"wet\"\ndel d['fish']         # Remove an element from a dictionary\nprint(d.get('fish', 'N/A')) # \"fish\" is no longer a key; prints \"N/A\"\n</code></pre> \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u5173\u4e8e\u200b\u5b57\u5178\u200b\u7684\u200b\u6240\u6709\u200b\u9700\u8981\u200b\u4e86\u89e3\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002 \u200b\u5faa\u73af\u200b\uff1a\u200b\u5728\u200b\u5b57\u5178\u200b\u4e2d\u200b\u8fed\u4ee3\u200b\u952e\u200b\u975e\u5e38\u5bb9\u6613\u200b\uff1a</p> <p>Loops\u200b\u5faa\u73af\u200b: \u200b\u5728\u200b\u5b57\u5178\u200b\u4e2d\u200b\u8fed\u4ee3\u200bkeys \u200b\u952e\u200b\u975e\u5e38\u5bb9\u6613\u200b\uff1a</p> <pre><code>d = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal in d:\n    legs = d[animal]\n    print('A %s has %d legs' % (animal, legs))\n# Prints \"A person has 2 legs\", \"A cat has 4 legs\", \"A spider has 8 legs\"\n</code></pre> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u60f3\u8981\u200b\u8bbf\u95ee\u200b\u952e\u200b\u548c\u200b\u5b83\u4eec\u200b\u5bf9\u5e94\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u8bf7\u200b\u4f7f\u7528\u200b<code>items</code>\u200b\u65b9\u6cd5\u200b\uff1a</p> <pre><code>d = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A %s has %d legs' % (animal, legs))\n# Prints \"A person has 2 legs\", \"A cat has 4 legs\", \"A spider has 8 legs\"\n</code></pre> <p>Dictionary comprehensions\u200b\u5b57\u5178\u200b\u63a8\u5bfc\u200b\u5f0f\u200b: \u200b\u8fd9\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u5217\u8868\u200b\u63a8\u5bfc\u200b\u5f0f\u200b\uff0c\u200b\u4f46\u200b\u5141\u8bb8\u200b\u4f60\u200b\u8f7b\u677e\u200b\u5730\u200b\u6784\u5efa\u200b\u5b57\u5178\u200b\u3002\u200b\u4f8b\u5982\u200b\uff1a</p> <pre><code>nums = [0, 1, 2, 3, 4]\neven_num_to_square = {x: x ** 2 for x in nums if x % 2 == 0}\nprint(even_num_to_square)  # Prints \"{0: 0, 2: 4, 4: 16}\"\n</code></pre> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#sets","title":"Sets\u200b\u96c6\u5408","text":"<p>\u200b\u96c6\u5408\u200b\u662f\u200b\u4e00\u4e2a\u200b\u65e0\u5e8f\u200b\u7684\u200b\u3001\u200b\u4e0d\u200b\u91cd\u590d\u200b\u5143\u7d20\u200b\u7684\u200b\u96c6\u5408\u200b\u3002\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u8003\u8651\u200b\u4e0b\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a</p> <pre><code>animals = {'cat', 'dog'}\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' in animals)  # prints \"False\"\nanimals.add('fish')       # Add an element to a set\nprint('fish' in animals)  # Prints \"True\"\nprint(len(animals))       # Number of elements in a set; prints \"3\"\nanimals.add('cat')        # Adding an element that is already in the set does nothing\nprint(len(animals))       # Prints \"3\"\nanimals.remove('cat')     # Remove an element from a set\nprint(len(animals))       # Prints \"2\"\n</code></pre> <p>\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5173\u4e8e\u200b\u96c6\u5408\u200b\u7684\u200b\u6240\u6709\u200b\u4fe1\u606f\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u3002</p> <p>Loops\u200b\u5faa\u73af\u200b: \u200b\u904d\u5386\u200b\u96c6\u5408\u200b\u7684\u200b\u8bed\u6cd5\u200b\u4e0e\u200b\u904d\u5386\u200b\u5217\u8868\u200b\u76f8\u540c\u200b\uff1b\u200b\u7136\u800c\u200b\uff0c\u200b\u7531\u4e8e\u200b\u96c6\u5408\u200b\u662f\u200b\u65e0\u5e8f\u200b\u7684\u200b\uff0c\u200b\u60a8\u200b\u65e0\u6cd5\u200b\u5bf9\u200b\u8bbf\u95ee\u200b\u96c6\u5408\u200b\u5143\u7d20\u200b\u7684\u200b\u987a\u5e8f\u200b\u505a\u51fa\u200b\u4efb\u4f55\u200b\u5047\u8bbe\u200b\u3002</p> <pre><code>animals = {'cat', 'dog', 'fish'}\nfor idx, animal in enumerate(animals):\n    print('#%d: %s' % (idx + 1, animal))\n# Prints \"#1: fish\", \"#2: dog\", \"#3: cat\"\n</code></pre> <p>Set comprehensions\u200b\u96c6\u5408\u200b\u63a8\u5bfc\u200b\u5f0f\u200b: \u200b\u4e0e\u200b\u5217\u8868\u200b\u548c\u200b\u5b57\u5178\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u96c6\u5408\u200b\u63a8\u5bfc\u200b\u5f0f\u200b\u8f7b\u677e\u200b\u6784\u5efa\u200b\u96c6\u5408\u200b\uff1a</p> <pre><code>from math import sqrt\nnums = {int(sqrt(x)) for x in range(30)}\nprint(nums)  # Prints \"{0, 1, 2, 3, 4, 5}\"\n</code></pre> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#tuples","title":"Tuples\u200b\u5143\u7ec4","text":"<p>\u200b\u5143\u7ec4\u200b\u662f\u200b\u4e00\u4e2a\u200b\uff08\u200b\u4e0d\u53ef\u200b\u53d8\u200b\u7684\u200bimmutable\uff09\u200b\u6709\u5e8f\u200b\u503c\u200b\u5217\u8868\u200b\u3002\u200b\u5143\u7ec4\u200b\u5728\u200b\u8bb8\u591a\u200b\u65b9\u9762\u200b\u4e0e\u200b\u5217\u8868\u200b\u7c7b\u4f3c\u200b\uff1b\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u533a\u522b\u200b\u662f\u200b\uff0c\u200b\u5143\u7ec4\u200b\u53ef\u4ee5\u200b\u7528\u4f5c\u200b\u5b57\u5178\u200b\u7684\u200b\u952e\u200b\u548c\u200b\u96c6\u5408\u200b\u7684\u200b\u5143\u7d20\u200b\uff0c\u200b\u800c\u200b\u5217\u8868\u200b\u4e0d\u80fd\u200b\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u793a\u4f8b\u200b\uff1a</p> <p><pre><code>d = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)        # Create a tuple\nprint(type(t))    # Prints \"&lt;class 'tuple'&gt;\"\nprint(d[t])       # Prints \"5\"\nprint(d[(1, 2)])  # Prints \"1\"\n</code></pre> \u200b\u6587\u6863\u200b\u4e2d\u6709\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u5143\u7ec4\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#functions","title":"Functions\u200b\u51fd\u6570","text":"<p>Python\u200b\u4f7f\u7528\u200b<code>def</code>\u200b\u5173\u952e\u5b57\u200b\u6765\u200b\u5b9a\u4e49\u200b\u51fd\u6570\u200b\u3002\u200b\u4f8b\u5982\u200b\uff1a</p> <pre><code>def sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\nfor x in [-1, 0, 1]:\n    print(sign(x))\n# Prints \"negative\", \"zero\", \"positive\"\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u5b9a\u4e49\u200b\u51fd\u6570\u200b\u6765\u200b\u63a5\u53d7\u200b\u53ef\u9009\u200b\u7684\u200b\u5173\u952e\u5b57\u200b\u53c2\u6570\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u8fd9\u6837\u200b\uff1a</p> <p><pre><code>def hello(name, loud=False):\n    if loud:\n        print('HELLO, %s!' % name.upper())\n    else:\n        print('Hello, %s' % name)\n\nhello('Bob') # Prints \"Hello, Bob\"\nhello('Fred', loud=True)  # Prints \"HELLO, FRED!\"\n</code></pre> \u200b\u6709\u5173\u200bPython\u200b\u51fd\u6570\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u9605\u200b\u6587\u6863\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#classes","title":"Classes\u200b\u7c7b","text":"<p>\u200b\u5728\u200bPython\u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u7c7b\u200b\u7684\u200b\u8bed\u6cd5\u200b\u5f88\u200b\u7b80\u5355\u200b\uff1a</p> <p><pre><code>class Greeter(object):\n\n    # Constructor\n    def __init__(self, name):\n        self.name = name  # Create an instance variable\n\n    # Instance method\n    def greet(self, loud=False):\n        if loud:\n            print('HELLO, %s!' % self.name.upper())\n        else:\n            print('Hello, %s' % self.name)\n\ng = Greeter('Fred')  # Construct an instance of the Greeter class\ng.greet()            # Call an instance method; prints \"Hello, Fred\"\ng.greet(loud=True)   # Call an instance method; prints \"HELLO, FRED!\"\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bPython\u200b\u7c7b\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u6570\u7ec4\u200b\u2693\ufe0e</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u5d4c\u5957\u200b\u7684\u200bPython\u200b\u5217\u8868\u200b\u4e2d\u200b\u521d\u59cb\u5316\u200bnumpy\u200b\u6570\u7ec4\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u65b9\u62ec\u53f7\u200b\u8bbf\u95ee\u200b\u5143\u7d20\u200b\uff1a</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#numpy","title":"Numpy","text":"<p>Numpy\u200b\u662f\u200bPython\u200b\u79d1\u5b66\u8ba1\u7b97\u200b\u7684\u200b\u6838\u5fc3\u200b\u5e93\u200b\u3002\u200b\u5b83\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u9ad8\u6027\u80fd\u200b\u7684\u200b\u591a\u7ef4\u200b\u6570\u7ec4\u200b\u5bf9\u8c61\u200b\u548c\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u8fd9\u4e9b\u200b\u6570\u7ec4\u200b\u7684\u200b\u5de5\u5177\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u7ecf\u200b\u719f\u6089\u200bMATLAB\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u8fd9\u4e2a\u200b\u6559\u7a0b\u200b\u5bf9\u4e8e\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200bNumpy\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#arrays","title":"Arrays","text":"<p>\u200b\u4e00\u4e2a\u200bnumpy\u200b\u6570\u7ec4\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7531\u503c\u200b\u7ec4\u6210\u200b\u7684\u200b\u7f51\u683c\u200bgrid\uff0c\u200b\u6240\u6709\u200b\u503c\u200b\u90fd\u200b\u662f\u200b\u76f8\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\uff0c\u200b\u901a\u8fc7\u200b\u975e\u8d1f\u200b\u6574\u6570\u200b\u7684\u200b\u5143\u7ec4\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u3002\u200b\u6570\u7ec4\u200b\u7684\u200b\u7ef4\u6570\u200b\u662f\u200b\u6570\u7ec4\u200b\u7684\u200b\u79e9\u200b rank\uff1b\u200b\u6570\u7ec4\u200b\u7684\u200b\u5f62\u72b6\u200bshape\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7531\u200b\u6574\u6570\u200b\u7ec4\u6210\u200b\u7684\u200b\u5143\u7ec4\u200b\uff0c\u200b\u7ed9\u51fa\u200b\u4e86\u200b\u6570\u7ec4\u200b\u6cbf\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u5d4c\u5957\u200b\u7684\u200bPython\u200b\u5217\u8868\u200b\u4e2d\u200b\u521d\u59cb\u5316\u200bnumpy\u200b\u6570\u7ec4\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u65b9\u62ec\u53f7\u200b\u8bbf\u95ee\u200b\u5143\u7d20\u200b\uff1a</p> <pre><code>import numpy as np\n\na = np.array([1, 2, 3])   # Create a rank 1 array\nprint(type(a))            # Prints \"&lt;class 'numpy.ndarray'&gt;\"\nprint(a.shape)            # Prints \"(3,)\"\nprint(a[0], a[1], a[2])   # Prints \"1 2 3\"\na[0] = 5                  # Change an element of the array\nprint(a)                  # Prints \"[5, 2, 3]\"\n\nb = np.array([[1,2,3],[4,5,6]])    # Create a rank 2 array\nprint(b.shape)                     # Prints \"(2, 3)\"\nprint(b[0, 0], b[0, 1], b[1, 0])   # Prints \"1 2 4\"\n</code></pre> <p>Numpy\u200b\u8fd8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u51fd\u6570\u200b\u6765\u200b\u521b\u5efa\u200b\u6570\u7ec4\u200b\uff1a</p> <p><pre><code>import numpy as np\n\na = np.zeros((2,2))   # Create an array of all zeros\nprint(a)              # Prints \"[[ 0.  0.]\n                      #          [ 0.  0.]]\"\n\nb = np.ones((1,2))    # Create an array of all ones\nprint(b)              # Prints \"[[ 1.  1.]]\"\n\nc = np.full((2,2), 7)  # Create a constant array\nprint(c)               # Prints \"[[ 7.  7.]\n                       #          [ 7.  7.]]\"\n\nd = np.eye(2)         # Create a 2x2 identity matrix\nprint(d)              # Prints \"[[ 1.  0.]\n                      #          [ 0.  1.]]\"\n\ne = np.random.random((2,2))  # Create an array filled with random values\nprint(e)                     # Might print \"[[ 0.91940167  0.08143941]\n                             #               [ 0.68744134  0.87236687]]\"\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u4e86\u89e3\u200b\u5176\u4ed6\u200b\u6570\u7ec4\u200b\u521b\u5efa\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_2","title":"\u6570\u7ec4\u200b\u7d22\u5f15","text":"<p>Numpy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u51e0\u79cd\u200b\u8bbf\u95ee\u200b\u6570\u7ec4\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <p>Slicing\u200b\u5207\u7247\u200b: \u200b\u4e0e\u200bPython\u200b\u5217\u8868\u200b\u7c7b\u4f3c\u200b\uff0cNumpy\u200b\u6570\u7ec4\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u5207\u7247\u200b\u64cd\u4f5c\u200b\u3002\u200b\u7531\u4e8e\u200b\u6570\u7ec4\u200b\u53ef\u80fd\u200b\u662f\u200b\u591a\u7ef4\u200b\u7684\u200b\uff0c\u200b\u60a8\u200b\u9700\u8981\u200b\u4e3a\u200b\u6570\u7ec4\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7ef4\u5ea6\u200b\u6307\u5b9a\u200b\u4e00\u4e2a\u200b\u5207\u7247\u200b\uff1a</p> <pre><code>import numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Use slicing to pull out the subarray consisting of the first 2 rows\n# and columns 1 and 2; b is the following array of shape (2, 2):\n# [[2 3]\n#  [6 7]]\nb = a[:2, 1:3]\n\n# \u200b\u6570\u7ec4\u200b\u7684\u200b\u5207\u7247\u200b\u662f\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u7684\u200b\u89c6\u56fe\u200bview\uff0c\u200b\u56e0\u6b64\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u4fee\u6539\u200b\u5c06\u200b\u4fee\u6539\u200b\u539f\u59cb\u200b\u6570\u7ec4\u200b\u3002\nprint(a[0, 1])   # Prints \"2\"\nb[0, 0] = 77     # b[0, 0] is the same piece of data as a[0, 1]\nprint(a[0, 1])   # Prints \"77\"\n</code></pre> <p>\u200b\u60a8\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6574\u6570\u200b\u7d22\u5f15\u200b\u4e0e\u200b\u5207\u7247\u200b\u7d22\u5f15\u200b\u6df7\u5408\u200b\u4f7f\u7528\u200b\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u8fd9\u6837\u200b\u505a\u4f1a\u200b\u4ea7\u751f\u200b\u4e00\u4e2a\u200b\u6bd4\u200b\u539f\u59cb\u200b\u6570\u7ec4\u200b\u4f4e\u9636\u200blower rank\u200b\u7684\u200b\u6570\u7ec4\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u200b\u4e0e\u200b MATLAB \u200b\u5904\u7406\u200b\u6570\u7ec4\u200b\u5207\u7247\u200b\u7684\u200b\u65b9\u5f0f\u200b\u975e\u5e38\u200b\u4e0d\u540c\u200b\u3002</p> <pre><code>import numpy as np\n\n# Create the following rank 2 array with shape (3, 4)\n# [[ 1  2  3  4]\n#  [ 5  6  7  8]\n#  [ 9 10 11 12]]\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\n# Two ways of accessing the data in the middle row of the array.\n# Mixing integer indexing with slices yields an array of lower rank,\n# while using only slices yields an array of the same rank as the\n# original array:\nrow_r1 = a[1, :]    # Rank 1 view of the second row of a\nrow_r2 = a[1:2, :]  # Rank 2 view of the second row of a\nprint(row_r1, row_r1.shape)  # Prints \"[5 6 7 8] (4,)\"\nprint(row_r2, row_r2.shape)  # Prints \"[[5 6 7 8]] (1, 4)\"\n\n# We can make the same distinction when accessing columns of an array:\ncol_r1 = a[:, 1]\ncol_r2 = a[:, 1:2]\nprint(col_r1, col_r1.shape)  # Prints \"[ 2  6 10] (3,)\"\nprint(col_r2, col_r2.shape)  # Prints \"[[ 2]\n                             #          [ 6]\n                             #          [10]] (3, 1)\"\n</code></pre> <p>Integer array indexing\u200b\u6574\u6570\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b:\u200b\u5f53\u200b\u4f60\u200b\u4f7f\u7528\u200b\u5207\u7247\u200b\u5bf9\u200bnumpy\u200b\u6570\u7ec4\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u65f6\u200b\uff0c\u200b\u6240\u200b\u5f97\u5230\u200b\u7684\u200b\u6570\u7ec4\u200b\u89c6\u56fe\u200b\u5c06\u200b\u59cb\u7ec8\u200b\u662f\u200b\u539f\u59cb\u200b\u6570\u7ec4\u200b\u7684\u200b\u5b50\u200b\u6570\u7ec4\u200b\u3002\u200b\u76f8\u6bd4\u4e4b\u4e0b\u200b\uff0c\u200b\u6574\u6570\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b\u5141\u8bb8\u200b\u4f60\u200b\u4f7f\u7528\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u6570\u636e\u200b\u6784\u5efa\u200b\u4efb\u610f\u200b\u6570\u7ec4\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code>import numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\n# An example of integer array indexing.\n# The returned array will have shape (3,) and\nprint(a[[0, 1, 2], [0, 1, 0]])  # Prints \"[1 4 5]\"\n\n# The above example of integer array indexing is equivalent to this:\nprint(np.array([a[0, 0], a[1, 1], a[2, 0]]))  # Prints \"[1 4 5]\"\n\n# When using integer array indexing, you can reuse the same\n# element from the source array:\nprint(a[[0, 0], [1, 1]])  # Prints \"[2 2]\"\n\n# Equivalent to the previous integer array indexing example\nprint(np.array([a[0, 1], a[0, 1]]))  # Prints \"[2 2]\"\n</code></pre> <p>\u200b\u4e00\u4e2a\u200b\u6709\u7528\u200b\u7684\u200b\u6574\u6570\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b\u6280\u5de7\u200b\u662f\u200b\u9009\u62e9\u200b\u6216\u200b\u6539\u53d8\u200b\u77e9\u9635\u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5143\u7d20\u200b</p> <pre><code>import numpy as np\n\n# Create a new array from which we will select elements\na = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n\nprint(a)  # prints \"array([[ 1,  2,  3],\n          #                [ 4,  5,  6],\n          #                [ 7,  8,  9],\n          #                [10, 11, 12]])\"\n\n# Create an array of indices\nb = np.array([0, 2, 0, 1])\n\n# Select one element from each row of a using the indices in b\nprint(a[np.arange(4), b])  # Prints \"[ 1  6  7 11]\"\n\n# Mutate one element from each row of a using the indices in b\na[np.arange(4), b] += 10\n\nprint(a)  # prints \"array([[11,  2,  3],\n          #                [ 4,  5, 16],\n          #                [17,  8,  9],\n          #                [10, 21, 12]])\n</code></pre> <p>Boolean array indexing\u200b\u5e03\u5c14\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b:\u200b\u5e03\u5c14\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b\u5141\u8bb8\u200b\u60a8\u200b\u9009\u62e9\u200b\u6570\u7ec4\u200b\u7684\u200b\u4efb\u610f\u200b\u5143\u7d20\u200b\u3002\u200b\u7ecf\u5e38\u200b\u4f7f\u7528\u200b\u8fd9\u79cd\u200b\u7c7b\u578b\u200b\u7684\u200b\u7d22\u5f15\u200b\u6765\u200b\u9009\u62e9\u200b\u6ee1\u8db3\u200b\u67d0\u4e9b\u200b\u6761\u4ef6\u200b\u7684\u200b\u6570\u7ec4\u200b\u5143\u7d20\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code>import numpy as np\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)   # Find the elements of a that are bigger than 2;\n                     # this returns a numpy array of Booleans of the same\n                     # shape as a, where each slot of bool_idx tells\n                     # whether that element of a is &gt; 2.\n\nprint(bool_idx)      # Prints \"[[False False]\n                     #          [ True  True]\n                     #          [ True  True]]\"\n\n# We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])  # Prints \"[3 4 5 6]\"\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])     # Prints \"[3 4 5 6]\"\n</code></pre> <p>\u200b\u4e3a\u4e86\u200b\u7b80\u6d01\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u4eec\u200b\u7701\u7565\u200b\u4e86\u200b\u5f88\u591a\u200b\u5173\u4e8e\u200bnumpy\u200b\u6570\u7ec4\u200b\u7d22\u5f15\u200b\u7684\u200b\u7ec6\u8282\u200b\uff1b\u200b\u5982\u679c\u200b\u60a8\u200b\u60f3\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u5e94\u8be5\u200b\u9605\u8bfb\u200b\u6587\u6863\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#datatypes","title":"Datatypes\u200b\u6570\u636e\u7c7b\u578b","text":"<p>\u200b\u6bcf\u4e2a\u200bnumpy\u200b\u6570\u7ec4\u200b\u90fd\u200b\u662f\u200b\u7531\u200b\u76f8\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u5143\u7d20\u200b\u7ec4\u6210\u200b\u7684\u200b\u7f51\u683c\u200b\u3002Numpy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u53ef\u4ee5\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b\u6570\u7ec4\u200b\u7684\u200b\u6570\u5b57\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002\u200b\u5f53\u200b\u521b\u5efa\u200b\u6570\u7ec4\u200b\u65f6\u200b\uff0cNumpy\u200b\u4f1a\u200b\u5c1d\u8bd5\u200b\u731c\u6d4b\u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u4f46\u200b\u6784\u5efa\u200b\u6570\u7ec4\u200b\u7684\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u4e5f\u200b\u4f1a\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u9009\u200b\u53c2\u6570\u200b\u6765\u200b\u660e\u786e\u200b\u6307\u5b9a\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\uff1a</p> <p><pre><code>import numpy as np\n\nx = np.array([1, 2])   # Let numpy choose the datatype\nprint(x.dtype)         # Prints \"int64\"\n\nx = np.array([1.0, 2.0])   # Let numpy choose the datatype\nprint(x.dtype)             # Prints \"float64\"\n\nx = np.array([1, 2], dtype=np.int64)   # Force a particular datatype\nprint(x.dtype)                         # Prints \"int64\"\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u9605\u8bfb\u200b\u5173\u4e8e\u200bnumpy\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_3","title":"\u6570\u7ec4\u200b\u6570\u5b66","text":"<p>\u200b\u57fa\u672c\u200b\u7684\u200b\u6570\u5b66\u200b\u51fd\u6570\u200b\u5bf9\u200b\u6570\u7ec4\u200b\u8fdb\u884c\u200b\u9010\u200b\u5143\u7d20\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u7b97\u7b26\u200b\u91cd\u8f7d\u200b\u548c\u200bnumpy\u200b\u6a21\u5757\u200b\u4e2d\u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u4f7f\u7528\u200b\u3002</p> <pre><code>import numpy as np\n\nx = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\n\n# Elementwise sum; both produce the array\n# [[ 6.0  8.0]\n#  [10.0 12.0]]\nprint(x + y)\nprint(np.add(x, y))\n\n# Elementwise difference; both produce the array\n# [[-4.0 -4.0]\n#  [-4.0 -4.0]]\nprint(x - y)\nprint(np.subtract(x, y))\n\n# Elementwise product; both produce the array\n# [[ 5.0 12.0]\n#  [21.0 32.0]]\nprint(x * y)\nprint(np.multiply(x, y))\n\n# Elementwise division; both produce the array\n# [[ 0.2         0.33333333]\n#  [ 0.42857143  0.5       ]]\nprint(x / y)\nprint(np.divide(x, y))\n\n# Elementwise square root; produces the array\n# [[ 1.          1.41421356]\n#  [ 1.73205081  2.        ]]\nprint(np.sqrt(x))\n</code></pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u4e0e\u200bMATLAB\u200b\u4e0d\u540c\u200b\uff0c<code>*</code> \u200b\u8868\u793a\u200b\u9010\u200b\u5143\u7d20\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>dot</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8ba1\u7b97\u200b\u5411\u91cf\u200b\u7684\u200b\u5185\u79ef\u200b\uff0c\u200b\u5c06\u200b\u5411\u91cf\u200b\u4e0e\u200b\u77e9\u9635\u200b\u76f8\u4e58\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u77e9\u9635\u200b\u76f8\u4e58\u200b\u3002<code>dot</code> \u200b\u51fd\u6570\u200b\u5728\u200b numpy \u200b\u6a21\u5757\u200b\u4e2d\u662f\u200b\u53ef\u7528\u200b\u7684\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4e5f\u200b\u662f\u200b\u6570\u7ec4\u200b\u5bf9\u8c61\u200b\u7684\u200b\u5b9e\u4f8b\u200b\u65b9\u6cd5\u200b\u3002</p> <pre><code>import numpy as np\n\nx = np.array([[1,2],[3,4]])\ny = np.array([[5,6],[7,8]])\n\nv = np.array([9,10])\nw = np.array([11, 12])\n\n# Inner product of vectors; both produce 219\nprint(v.dot(w))\nprint(np.dot(v, w))\n\n# Matrix / vector product; both produce the rank 1 array [29 67]\nprint(x.dot(v))\nprint(np.dot(x, v))\n\n# Matrix / matrix product; both produce the rank 2 array\n# [[19 22]\n#  [43 50]]\nprint(x.dot(y))\nprint(np.dot(x, y))\n</code></pre> <p>Numpy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u7528\u4e8e\u200b\u5bf9\u200b\u6570\u7ec4\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u7684\u200b\u6709\u7528\u200b\u51fd\u6570\u200b\uff1b\u200b\u5176\u4e2d\u200b\u6700\u200b\u6709\u7528\u200b\u7684\u200b\u4e4b\u4e00\u200b\u662f\u200b<code>sum</code>\u3002</p> <p><pre><code>import numpy as np\n\nx = np.array([[1,2],[3,4]])\n\nprint(np.sum(x))  # Compute sum of all elements; prints \"10\"\nprint(np.sum(x, axis=0))  # Compute sum of each column; prints \"[4 6]\"\nprint(np.sum(x, axis=1))  # Compute sum of each row; prints \"[3 7]\"\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200bNumPy\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5b8c\u6574\u200b\u6570\u5b66\u200b\u51fd\u6570\u200b\u5217\u8868\u200b\u3002</p> <p>\u200b\u9664\u4e86\u200b\u4f7f\u7528\u200b\u6570\u7ec4\u200b\u8ba1\u7b97\u200b\u6570\u5b66\u200b\u51fd\u6570\u200b\u4e4b\u5916\u200b\uff0c\u200b\u6211\u4eec\u200b\u7ecf\u5e38\u200b\u9700\u8981\u200b\u5bf9\u200b\u6570\u7ec4\u200b\u4e2d\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u91cd\u5851\u200b\u6216\u200b\u5176\u4ed6\u200b\u64cd\u4f5c\u200b\u3002\u200b\u8fd9\u79cd\u200b\u64cd\u4f5c\u200b\u7684\u200b\u6700\u200b\u7b80\u5355\u200b\u793a\u4f8b\u200b\u662f\u200b\u8f6c\u7f6e\u200b\u77e9\u9635\u200b\uff1b\u200b\u8981\u200b\u8f6c\u7f6e\u200b\u77e9\u9635\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u4f7f\u7528\u200b\u6570\u7ec4\u200b\u5bf9\u8c61\u200b\u7684\u200b<code>T</code>\u200b\u5c5e\u6027\u200b\u5373\u53ef\u200b\u3002</p> <p><pre><code>import numpy as np\n\nx = np.array([[1,2], [3,4]])\nprint(x)    # Prints \"[[1 2]\n            #          [3 4]]\"\nprint(x.T)  # Prints \"[[1 3]\n            #          [2 4]]\"\n\n# Note that taking the transpose of a rank 1 array does nothing:\nv = np.array([1,2,3])\nprint(v)    # Prints \"[1 2 3]\"\nprint(v.T)  # Prints \"[1 2 3]\"\n</code></pre> Numpy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u7528\u4e8e\u200b\u64cd\u4f5c\u200b\u6570\u7ec4\u200b\u7684\u200b\u51fd\u6570\u200b\uff1b\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u770b\u5230\u200b\u5b8c\u6574\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#broadcasting","title":"Broadcasting\u200b\u5e7f\u64ad","text":"<p>\u200b\u5e7f\u64ad\u200b\u662f\u200b\u4e00\u79cd\u200b\u5f3a\u5927\u200b\u7684\u200b\u673a\u5236\u200b\uff0c\u200b\u5b83\u200b\u5141\u8bb8\u200bnumpy\u200b\u5728\u200b\u6267\u884c\u200b\u7b97\u672f\u200b\u8fd0\u7b97\u200b\u65f6\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u5f62\u72b6\u200b\u7684\u200b\u6570\u7ec4\u200b\u3002\u200b\u5e38\u5e38\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u7ec4\u200b\u548c\u200b\u4e00\u4e2a\u200b\u8f83\u5927\u200b\u7684\u200b\u6570\u7ec4\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u7ec4\u200b\u591a\u6b21\u200b\u5bf9\u200b\u8f83\u5927\u200b\u7684\u200b\u6570\u7ec4\u200b\u6267\u884c\u200b\u67d0\u4e2a\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u5e38\u91cf\u200b\u5411\u91cf\u200b\u6dfb\u52a0\u200b\u5230\u200b\u77e9\u9635\u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1a</p> <pre><code>import numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = np.empty_like(x)   # Create an empty matrix with the same shape as x\n\n# Add the vector v to each row of the matrix x with an explicit loop\nfor i in range(4):\n    y[i, :] = x[i, :] + v\n\n# Now y is the following\n# [[ 2  2  4]\n#  [ 5  5  7]\n#  [ 8  8 10]\n#  [11 11 13]]\nprint(y)\n</code></pre> <p>\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u5de5\u4f5c\u200b\uff1b\u200b\u7136\u800c\u200b\u5f53\u200b\u77e9\u9635\u200b <code>x</code> \u200b\u5f88\u5927\u200b\u65f6\u200b\uff0c\u200b\u7528\u200b Python \u200b\u8ba1\u7b97\u200b\u4e00\u4e2a\u200b\u663e\u5f0f\u200b\u5faa\u73af\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f88\u6162\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5c06\u200b\u5411\u91cf\u200b <code>v</code> \u200b\u52a0\u200b\u5230\u200b\u77e9\u9635\u200b <code>x</code> \u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u4e0a\u200b\u76f8\u5f53\u4e8e\u200b\u901a\u8fc7\u200b\u5782\u76f4\u200b\u5806\u53e0\u200b\u591a\u4e2a\u200b <code>v</code> \u200b\u7684\u200b\u526f\u200b\u672c\u6765\u200b\u5f62\u6210\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b <code>vv</code>\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b <code>x</code> \u200b\u548c\u200b <code>vv</code> \u200b\u8fdb\u884c\u200b\u9010\u200b\u5143\u7d20\u200b\u6c42\u548c\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u8fd9\u6837\u200b\u5b9e\u73b0\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\uff1a</p> <pre><code>import numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\nvv = np.tile(v, (4, 1))   # Stack 4 copies of v on top of each other\nprint(vv)                 # Prints \"[[1 0 1]\n                          #          [1 0 1]\n                          #          [1 0 1]\n                          #          [1 0 1]]\"\ny = x + vv  # Add x and vv elementwise\nprint(y)  # Prints \"[[ 2  2  4\n          #          [ 5  5  7]\n          #          [ 8  8 10]\n          #          [11 11 13]]\"\n</code></pre> <p>Numpy\u200b\u5e7f\u64ad\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u8fd9\u4e2a\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u521b\u5efa\u200b\u591a\u4e2a\u200b<code>v</code>\u200b\u7684\u200b\u526f\u672c\u200b\u3002\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b\u5e7f\u64ad\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u7248\u672c\u200b\uff1a</p> <pre><code>import numpy as np\n\n# We will add the vector v to each row of the matrix x,\n# storing the result in the matrix y\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  # Add v to each row of x using broadcasting\nprint(y)  # Prints \"[[ 2  2  4]\n          #          [ 5  5  7]\n          #          [ 8  8 10]\n          #          [11 11 13]]\"\n</code></pre> <p>\u200b\u7ebf\u6027\u65b9\u7a0b\u200b <code>y = x + v</code> \u200b\u5373\u4f7f\u200b x \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(4, 3)</code>\uff0c<code>v</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(3,)</code>\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5e7f\u64ad\u200b\u6765\u200b\u8ba1\u7b97\u200b\uff1b\u200b\u8fd9\u4e2a\u200b\u65b9\u7a0b\u200b\u4f1a\u200b\u50cf\u200b <code>v</code> \u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(4, 3)</code> \u200b\u4e00\u6837\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5176\u4e2d\u200b\u6bcf\u200b\u4e00\u884c\u200b\u90fd\u200b\u662f\u200b <code>v</code> \u200b\u7684\u200b\u526f\u672c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6309\u200b\u5143\u7d20\u200b\u8fdb\u884c\u200b\u6c42\u548c\u200b\u3002</p> <p>\u200b\u5c06\u200b\u4e24\u4e2a\u200b\u6570\u7ec4\u200b\u8fdb\u884c\u200b\u5e7f\u64ad\u200b\u9075\u5faa\u200b\u4ee5\u4e0b\u200b\u89c4\u5219\u200b\uff1a</p> <ol> <li>\u200b\u5982\u679c\u200b\u4e24\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u79e9\u200b\u4e0d\u540c\u200b\uff0c\u200b\u5219\u200b\u5728\u200b\u79e9\u8f83\u200b\u4f4e\u200b\u7684\u200b\u6570\u7ec4\u200b\u7684\u200b\u5f62\u72b6\u200b\u524d\u9762\u200b\u6dfb\u52a0\u200b 1\uff0c\u200b\u76f4\u5230\u200b\u4e24\u4e2a\u200b\u5f62\u72b6\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u957f\u5ea6\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4e24\u4e2a\u200b\u6570\u7ec4\u200b\u5728\u200b\u67d0\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6216\u8005\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\u5728\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b 1\uff0c\u200b\u5219\u200b\u79f0\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6570\u7ec4\u200b\u5728\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u662f\u200b\u517c\u5bb9\u200b\u7684\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4e24\u4e2a\u200b\u6570\u7ec4\u200b\u5728\u200b\u6240\u6709\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u90fd\u200b\u662f\u200b\u517c\u5bb9\u200b\u7684\u200b\uff0c\u200b\u5219\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u5e7f\u64ad\u200b\u3002</li> <li>\u200b\u5e7f\u64ad\u200b\u540e\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u884c\u4e3a\u200b\u90fd\u200b\u597d\u50cf\u200b\u5b83\u4eec\u200b\u7684\u200b\u5f62\u72b6\u200b\u7b49\u4e8e\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\u6570\u7ec4\u200b\u7684\u200b\u5f62\u72b6\u200b\u7684\u200b\u5143\u7d20\u200b\u6700\u5927\u503c\u200b\u3002</li> <li>\u200b\u5728\u200b\u4efb\u4f55\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b 1\uff0c\u200b\u800c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u5927\u5c0f\u200b\u5927\u4e8e\u200b 1\uff0c\u200b\u5219\u200b\u7b2c\u4e00\u4e2a\u200b\u6570\u7ec4\u200b\u7684\u200b\u884c\u4e3a\u200b\u597d\u50cf\u200b\u6cbf\u8be5\u200b\u7ef4\u5ea6\u200b\u88ab\u200b\u590d\u5236\u200b\u4e86\u200b\u3002</li> </ol> <p>\u200b\u5982\u679c\u200b\u8fd9\u4e2a\u200b\u89e3\u91ca\u200b\u4e0d\u200b\u6e05\u695a\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u4ece\u200b\u6587\u6863\u200b\u6216\u8005\u200b\u8fd9\u4e2a\u200b\u89e3\u91ca\u200b\u4e2d\u200b\u9605\u8bfb\u200b\u89e3\u91ca\u200b\u3002</p> <p>\u200b\u652f\u6301\u200b\u5e7f\u64ad\u200b\u7684\u200b\u51fd\u6570\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u901a\u7528\u200b\u51fd\u6570\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b\u6240\u6709\u200b\u901a\u7528\u200b\u51fd\u6570\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u5e7f\u64ad\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff1a</p> <pre><code>import numpy as np\n\n# Compute outer product of vectors\nv = np.array([1,2,3])  # v has shape (3,)\nw = np.array([4,5])    # w has shape (2,)\n# To compute an outer product, we first reshape v to be a column\n# vector of shape (3, 1); we can then broadcast it against w to yield\n# an output of shape (3, 2), which is the outer product of v and w:\n# [[ 4  5]\n#  [ 8 10]\n#  [12 15]]\nprint(np.reshape(v, (3, 1)) * w)\n\n# Add a vector to each row of a matrix\nx = np.array([[1,2,3], [4,5,6]])\n# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n# giving the following matrix:\n# [[2 4 6]\n#  [5 7 9]]\nprint(x + v)\n\n# Add a vector to each column of a matrix\n# x has shape (2, 3) and w has shape (2,).\n# If we transpose x then it has shape (3, 2) and can be broadcast\n# against w to yield a result of shape (3, 2); transposing this result\n# yields the final result of shape (2, 3) which is the matrix x with\n# the vector w added to each column. Gives the following matrix:\n# [[ 5  6  7]\n#  [ 9 10 11]]\nprint((x.T + w).T)\n# Another solution is to reshape w to be a column vector of shape (2, 1);\n# we can then broadcast it directly against x to produce the same\n# output.\nprint(x + np.reshape(w, (2, 1)))\n\n# Multiply a matrix by a constant:\n# x has shape (2, 3). Numpy treats scalars as arrays of shape ();\n# these can be broadcast together to shape (2, 3), producing the\n# following array:\n# [[ 2  4  6]\n#  [ 8 10 12]]\nprint(x * 2)\n</code></pre> <p>\u200b\u5e7f\u64ad\u200b\u901a\u5e38\u200b\u80fd\u200b\u4f7f\u200b\u4f60\u200b\u7684\u200b\u4ee3\u7801\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\u548c\u200b\u5feb\u901f\u200b\uff0c\u200b\u6240\u4ee5\u200b\u4f60\u200b\u5e94\u8be5\u200b\u5c3d\u91cf\u200b\u5728\u200b\u53ef\u80fd\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p>"},{"location":"courses/preparation/python-numpy-tutorial/#numpy_1","title":"Numpy\u200b\u6587\u6863","text":"<p>\u200b\u8fd9\u4e2a\u200b\u7b80\u8981\u200b\u6982\u8ff0\u200b\u5df2\u7ecf\u200b\u6d89\u53ca\u200b\u5230\u200b\u4e86\u200b\u4f60\u200b\u9700\u8981\u200b\u4e86\u89e3\u200b\u5173\u4e8e\u200bnumpy\u200b\u7684\u200b\u8bb8\u591a\u200b\u91cd\u8981\u200b\u4e8b\u9879\u200b\uff0c\u200b\u4f46\u200b\u8fd8\u200b\u8fdc\u8fdc\u200b\u4e0d\u200b\u5b8c\u6574\u200b\u3002\u200b\u8bf7\u200b\u67e5\u9605\u200bnumpy\u200b\u53c2\u8003\u8d44\u6599\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bnumpy\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#scipy","title":"SciPy","text":"<p>Numpy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u9ad8\u6027\u80fd\u200b\u7684\u200b\u591a\u7ef4\u200b\u6570\u7ec4\u200b\u548c\u200b\u57fa\u672c\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5de5\u5177\u200b\u6765\u200b\u5904\u7406\u200b\u548c\u200b\u64cd\u4f5c\u200b\u8fd9\u4e9b\u200b\u6570\u7ec4\u200b\u3002SciPy\u200b\u5728\u200b\u6b64\u57fa\u7840\u200b\u4e0a\u200b\u6784\u5efa\u200b\uff0c\u200b\u5e76\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5927\u91cf\u200b\u64cd\u4f5c\u200bnumpy\u200b\u6570\u7ec4\u200b\u5e76\u200b\u9002\u7528\u200b\u4e8e\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u79d1\u5b66\u200b\u548c\u200b\u5de5\u7a0b\u200b\u5e94\u7528\u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u719f\u6089\u200bSciPy\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u662f\u200b\u6d4f\u89c8\u200b\u6587\u6863\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u7a81\u51fa\u200b\u4e00\u4e9b\u200b\u5bf9\u200b\u4f60\u200b\u6765\u8bf4\u200b\u5728\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u6709\u7528\u200b\u7684\u200bSciPy\u200b\u90e8\u5206\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_4","title":"\u56fe\u50cf\u200b\u64cd\u4f5c","text":"<p>SciPy\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u57fa\u672c\u200b\u7684\u200b\u56fe\u50cf\u5904\u7406\u200b\u51fd\u6570\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5b83\u200b\u5177\u6709\u200b\u5c06\u200b\u56fe\u50cf\u200b\u4ece\u200b\u78c1\u76d8\u200b\u8bfb\u5165\u200bnumpy\u200b\u6570\u7ec4\u200b\u3001\u200b\u5c06\u200bnumpy\u200b\u6570\u7ec4\u200b\u5199\u5165\u200b\u78c1\u76d8\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u4ee5\u53ca\u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\u200b\u4e0b\u9762\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5c55\u793a\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u7684\u200b\u7b80\u5355\u200b\u793a\u4f8b\u200b\u3002</p> <pre><code>from scipy.misc import imread, imsave, imresize\n\n# Read an JPEG image into a numpy array\nimg = imread('assets/cat.jpg')\nprint(img.dtype, img.shape)  # Prints \"uint8 (400, 248, 3)\"\n\n# We can tint the image by scaling each of the color channels\n# by a different scalar constant. The image has shape (400, 248, 3);\n# we multiply it by the array [1, 0.95, 0.9] of shape (3,);\n# numpy broadcasting means that this leaves the red channel unchanged,\n# and multiplies the green and blue channels by 0.95 and 0.9\n# respectively.\nimg_tinted = img * [1, 0.95, 0.9]\n\n# Resize the tinted image to be 300 by 300 pixels.\nimg_tinted = imresize(img_tinted, (300, 300))\n\n# Write the tinted image back to disk\nimsave('assets/cat_tinted.jpg', img_tinted)\n</code></pre>      \u200b\u5de6\u200b\uff1a\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u3002     \u200b\u53f3\u200b\uff1a\u200b\u7ecf\u8fc7\u200b\u7740\u8272\u200b\u548c\u200b\u8c03\u6574\u200b\u5927\u5c0f\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002    <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#matlab","title":"MATLAB\u200b\u6587\u4ef6","text":"<p>\u200b\u51fd\u6570\u200b<code>scipy.io.loadmat</code>\u200b\u548c\u200b<code>scipy.io.savemat</code>\u200b\u5141\u8bb8\u200b\u60a8\u200b\u8bfb\u53d6\u200b\u548c\u200b\u5199\u5165\u200bMATLAB\u200b\u6587\u4ef6\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_5","title":"\u70b9\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ddd\u79bb","text":"<p>SciPy\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u70b9\u96c6\u200b\u4e4b\u95f4\u200b\u8ddd\u79bb\u200b\u7684\u200b\u6709\u7528\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u51fd\u6570\u200b<code>scipy.spatial.distance.pdist</code>\u200b\u8ba1\u7b97\u200b\u7ed9\u200b\u5b9a\u70b9\u200b\u96c6\u4e2d\u200b\u6240\u6709\u200b\u70b9\u200b\u5bf9\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ddd\u79bb\u200b\u3002</p> <p><pre><code>import numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Create the following array where each row is a point in 2D space:\n# [[0 1]\n#  [1 0]\n#  [2 0]]\nx = np.array([[0, 1], [1, 0], [2, 0]])\nprint(x)\n\n# Compute the Euclidean distance between all rows of x.\n# d[i, j] is the Euclidean distance between x[i, :] and x[j, :],\n# and d is the following array:\n# [[ 0.          1.41421356  2.23606798]\n#  [ 1.41421356  0.          1.        ]\n#  [ 2.23606798  1.          0.        ]]\nd = squareform(pdist(x, 'euclidean'))\nprint(d)\n</code></pre> \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u9605\u8bfb\u200b\u6709\u5173\u200b\u8be5\u200b\u529f\u80fd\u200b\u7684\u200b\u6240\u6709\u200b\u8be6\u60c5\u200b\u3002</p> <p>\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u51fd\u6570\u200b\uff08<code>scipy.spatial.distance.cdist</code>\uff09\u200b\u8ba1\u7b97\u200b\u4e24\u7ec4\u200b\u70b9\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ddd\u79bb\u200b\uff1b\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u9605\u8bfb\u200b\u6709\u5173\u200b\u6b64\u200b\u51fd\u6570\u200b\u7684\u200b\u8be6\u60c5\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#matplotlib","title":"Matplotlib","text":"<p>Matplotlib\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7ed8\u200b\u56fe\u5e93\u200b\u3002\u200b\u5728\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u200b<code>matplotlib.pyplot</code>\u200b\u6a21\u5757\u200b\u8fdb\u884c\u200b\u7b80\u8981\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u8be5\u200b\u6a21\u5757\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bMATLAB\u200b\u7684\u200b\u7ed8\u56fe\u200b\u7cfb\u7edf\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#plotting","title":"Plotting\u200b\u7ed8\u56fe","text":"<p>Matplotlib\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u51fd\u6570\u200b\u662f\u200b<code>plot</code>\uff0c\u200b\u5b83\u200b\u5141\u8bb8\u200b\u60a8\u200b\u7ed8\u5236\u200b2D\u200b\u6570\u636e\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u793a\u4f8b\u200b\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()  # You must call plt.show() to make graphics appear.\n</code></pre> <p>\u200b\u6267\u884c\u200b\u6b64\u200b\u4ee3\u7801\u200b\u4f1a\u200b\u4ea7\u751f\u200b\u4ee5\u4e0b\u200b\u56fe\u8868\u200b\uff1a</p> <p>\u200b\u53ea\u200b\u9700\u200b\u7a0d\u5fae\u200b\u52aa\u529b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u5730\u200b\u540c\u65f6\u200b\u7ed8\u5236\u200b\u591a\u6761\u200b\u7ebf\u200b\uff0c\u200b\u5e76\u200b\u6dfb\u52a0\u200b\u6807\u9898\u200b\u3001\u200b\u56fe\u4f8b\u200b\u548c\u200b\u5750\u6807\u8f74\u200b\u6807\u7b7e\u200b\u3002</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('x axis label')\nplt.ylabel('y axis label')\nplt.title('Sine and Cosine')\nplt.legend(['Sine', 'Cosine'])\nplt.show()\n</code></pre> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u6709\u5173\u200b<code>plot</code>\u200b\u51fd\u6570\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#subplots","title":"Subplots\u200b\u5b50\u56fe","text":"<p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>subplot</code>\u200b\u51fd\u6570\u200b\u5728\u200b\u540c\u200b\u4e00\u5f20\u200b\u56fe\u4e2d\u200b\u7ed8\u5236\u200b\u4e0d\u540c\u200b\u7684\u200b\u4e1c\u897f\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\uff1a</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n</code></pre> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6587\u6863\u200b\u4e2d\u200b\u9605\u8bfb\u200b\u66f4\u200b\u591a\u200b\u6709\u5173\u200b<code>subplot</code>\u200b\u51fd\u6570\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p></p>"},{"location":"courses/preparation/python-numpy-tutorial/#_6","title":"\u56fe\u50cf","text":"<p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>imshow</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u663e\u793a\u200b\u56fe\u50cf\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u4f8b\u5b50\u200b\uff1a</p> <pre><code>import numpy as np\nfrom scipy.misc import imread, imresize\nimport matplotlib.pyplot as plt\n\nimg = imread('assets/cat.jpg')\nimg_tinted = img * [1, 0.95, 0.9]\n\n# Show the original image\nplt.subplot(1, 2, 1)\nplt.imshow(img)\n\n# Show the tinted image\nplt.subplot(1, 2, 2)\n\n# A slight gotcha with imshow is that it might give strange results\n# if presented with data that is not uint8. To work around this, we\n# explicitly cast the image to uint8 before displaying it.\nplt.imshow(np.uint8(img_tinted))\nplt.show()\n</code></pre>"},{"location":"courses/preparation/setup/","title":"\u8f6f\u4ef6\u200b\u8bbe\u7f6e","text":"<p>\u200b\u4eca\u5e74\u200b\uff0c\u200b\u63a8\u8350\u200b\u7684\u200b\u4f5c\u4e1a\u200b\u65b9\u5f0f\u200b\u662f\u200b\u901a\u8fc7\u200b Google Colaboratory\u3002\u200b\u4f46\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u5df2\u7ecf\u200b\u62e5\u6709\u200bGPU\u200b\u652f\u6301\u200b\u7684\u200b\u786c\u4ef6\u200b\uff0c\u200b\u5e76\u4e14\u200b\u66f4\u200b\u559c\u6b22\u200b\u5728\u200b\u672c\u5730\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u8bbe\u7f6e\u200b\u865a\u62df\u73af\u5883\u200b\u7684\u200b\u8bf4\u660e\u200b\u3002</p> <ul> <li>\u200b\u5728\u200bGoogle Colaboratory\u200b\u5e73\u53f0\u200b\u4e0a\u200b\u8fdc\u7a0b\u200b\u5de5\u4f5c\u200b</li> <li>\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u672c\u5730\u200b\u5de5\u4f5c\u200b</li> <li>Anaconda \u200b\u865a\u62df\u73af\u5883\u200b</li> <li>Python venv</li> <li>\u200b\u5b89\u88c5\u200b\u8f6f\u4ef6\u5305\u200b</li> </ul>"},{"location":"courses/preparation/setup/#google-colaboratory","title":"\u5728\u200bGoogle Colaboratory\u200b\u5e73\u53f0\u200b\u4e0a\u200b\u8fdc\u7a0b\u200b\u5de5\u4f5c","text":"<p>Google Colaboratory\u200b\u603b\u7684\u6765\u8bf4\u200b\u662f\u200bJupyter notebook \u200b\u548c\u200bGoogle Drive\u200b\u7684\u200b\u7ed3\u5408\u200b\u3002\u200b\u5b83\u200b\u5b8c\u5168\u200b\u5728\u200b\u4e91\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5e76\u200b\u9884\u88c5\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5305\u200b\uff08\u200b\u4f8b\u5982\u200bPyTorch\u200b\u548c\u200bTensorflow\uff09\uff0c\u200b\u56e0\u6b64\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b\u76f8\u540c\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\u3002\u200b\u66f4\u9177\u200b\u7684\u200b\u662f\u200b\uff0cColab\u200b\u53ef\u4ee5\u200b\u514d\u8d39\u200b\u8bbf\u95ee\u200bGPU\uff08K80\u3001P100\uff09\u200b\u548c\u200bTPU\u200b\u7b49\u200b\u786c\u4ef6\u200b\u52a0\u901f\u5668\u200b\uff0c\u200b\u8fd9\u200b\u5bf9\u200b\u4efb\u52a1\u200b2\u200b\u548c\u200b3\u200b\u7279\u522b\u200b\u6709\u7528\u200b\u3002</p> <p>\u200b\u8981\u6c42\u200b\u3002\u200b\u8981\u200b\u4f7f\u7528\u200bColab\uff0c\u200b\u60a8\u200b\u5fc5\u987b\u200b\u62e5\u6709\u200b\u4e00\u4e2a\u200b\u5e26\u6709\u200b\u76f8\u5173\u200bGoogle Drive\u200b\u7684\u200b\u8c37\u6b4c\u200b\u5e10\u6237\u200b\u3002\u200b\u5047\u8bbe\u200b\u4e24\u8005\u200b\u90fd\u200b\u6709\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5c06\u200bColab\u200b\u8fde\u63a5\u200b\u5230\u200b\u60a8\u200b\u7684\u200bDrive\uff1a</p> <ol> <li>\u200b\u5355\u51fb\u200b\u53f3\u4e0a\u89d2\u200b\u7684\u200b\u63a7\u5236\u200b\u76d8\u200b\uff0c\u200b\u7136\u540e\u200b\u9009\u62e9\u200b<code>\u200b\u8bbe\u7f6e\u200b</code>\u3002</li> <li>\u200b\u5355\u51fb\u200b<code>\u200b\u7ba1\u7406\u5e94\u7528\u7a0b\u5e8f\u200b</code>\u200b\u9009\u9879\u5361\u200b\u3002</li> <li>\u200b\u5728\u200b\u9876\u90e8\u200b\uff0c\u200b\u9009\u62e9\u200b<code>\u200b\u8fde\u63a5\u200b\u66f4\u200b\u591a\u200b\u5e94\u7528\u7a0b\u5e8f\u200b</code>\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u6253\u5f00\u200b<code>GSuite Marketplace</code>\u200b\u7a97\u53e3\u200b\u3002</li> <li>\u200b\u641c\u7d22\u200bColab\uff0c\u200b\u7136\u540e\u200b\u5355\u51fb\u200b<code>\u200b\u6dfb\u52a0\u200b</code>\u3002</li> </ol> <p>\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u4f5c\u4e1a\u200b\u90fd\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4e0b\u8f7d\u200b\u94fe\u63a5\u200b\uff0c\u200b\u6307\u5411\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200bColab\u200b\u7b14\u8bb0\u672c\u200b\u548c\u200bPython\u200b\u5165\u95e8\u200b\u4ee3\u7801\u200b\u7684\u200bzip\u200b\u6587\u4ef6\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6587\u4ef6\u5939\u200b\u4e0a\u200b\u4f20\u5230\u200b\u4e91\u76d8\u200b\uff0c\u200b\u5728\u200bColab\u200b\u4e2d\u200b\u6253\u5f00\u200b\u7b14\u8bb0\u672c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8fdb\u5ea6\u200b\u4fdd\u5b58\u200b\u56de\u200b\u4e91\u76d8\u200b\u3002\u200b\u6211\u4eec\u200b\u5efa\u8bae\u60a8\u200b\u89c2\u770b\u200b\u4e0b\u9762\u200b\u7684\u200b\u6559\u7a0b\u200b\u89c6\u9891\u200b\uff0c\u200b\u5176\u4e2d\u200b\u4ee5\u200b\u4f5c\u4e1a\u200b1\u200b\u4e3a\u4f8b\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u63a8\u8350\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002</p> <p></p> <p>\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\u3002\u200b\u5728\u200b\u4e0e\u200bColab\u200b\u5408\u4f5c\u200b\u65f6\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u6ce8\u610f\u200b\u4ee5\u4e0b\u51e0\u70b9\u200b\u3002\u200b\u9996\u5148\u200b\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8d44\u6e90\u200b\u5e76\u200b\u6ca1\u6709\u200b\u5f97\u5230\u200b\u4fdd\u8bc1\u200b\uff08\u200b\u8fd9\u662f\u200b\u514d\u8d39\u200b\u7684\u200b\u4ef7\u683c\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u7a7a\u95f2\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u65f6\u95f4\u200b\uff0c\u200b\u6216\u8005\u200b\u60a8\u200b\u7684\u200b\u603b\u200b\u8fde\u63a5\u65f6\u95f4\u200b\u8d85\u8fc7\u200b\u4e86\u200b\u5141\u8bb8\u200b\u7684\u200b\u6700\u5927\u200b\u65f6\u95f4\u200b\uff08\u200b\u7ea6\u200b12\u200b\u5c0f\u65f6\u200b\uff09\uff0cColab\u200b\u865a\u62df\u673a\u200b\u5c06\u200b\u65ad\u5f00\u8fde\u63a5\u200b\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4efb\u4f55\u200b\u672a\u200b\u4fdd\u5b58\u200b\u7684\u200b\u8fdb\u5ea6\u200b\u90fd\u200b\u5c06\u200b\u4e22\u5931\u200b\u56e0\u6b64\u200b\uff0c\u200b\u517b\u6210\u200b\u5728\u200b\u5904\u7406\u200b\u4f5c\u4e1a\u200b\u65f6\u200b\u7ecf\u5e38\u200b\u4fdd\u5b58\u200b\u4ee3\u7801\u200b\u7684\u200b\u4e60\u60ef\u200b\u3002\u200b\u8981\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bColab\u200b\u4e2d\u200b\u8d44\u6e90\u200b\u9650\u5236\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u6b64\u5904\u200b\u9605\u8bfb\u200b\u4ed6\u4eec\u200b\u7684\u200b\u5e38\u89c1\u200b\u95ee\u9898\u89e3\u7b54\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200bGPU\u3002\u200b\u4f7f\u7528\u200bGPU\u200b\u5c31\u200b\u50cf\u200b\u5728\u200bColab\u200b\u4e2d\u200b\u5207\u6362\u200b\u8fd0\u884c\u200b\u65f6\u200b\u4e00\u6837\u200b\u7b80\u5355\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c<code>\u200b\u5355\u51fb\u200b\u8fd0\u884c\u200b\u65f6\u200b-&gt;\u200b\u66f4\u6539\u200b\u8fd0\u884c\u200b\u65f6\u200b\u7c7b\u578b\u200b-&gt;\u200b\u786c\u4ef6\u200b\u52a0\u901f\u5668\u200b-&gt;GPU</code>\uff0c\u200b\u60a8\u200b\u7684\u200bColab\u200b\u5b9e\u4f8b\u200b\u5c06\u200b\u81ea\u52a8\u200b\u7531\u200bGPU\u200b\u8ba1\u7b97\u200b\u652f\u6301\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u6709\u200b\u5174\u8da3\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bColab\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6211\u4eec\u200b\u9f13\u52b1\u200b\u60a8\u200b\u8bbf\u95ee\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u8c37\u6b4c\u200bColab\u200b\u7b80\u4ecb\u200b</li> <li>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200bColab</li> <li>Colab\u200b\u529f\u80fd\u200b\u6982\u8ff0\u200b</li> </ul>"},{"location":"courses/preparation/setup/#_1","title":"\u5728\u200b\u60a8\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u672c\u5730\u200b\u5de5\u4f5c","text":"<p>\u200b\u5982\u679c\u200b\u60a8\u200b\u5e0c\u671b\u200b\u5728\u200b\u672c\u5730\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u865a\u62df\u73af\u5883\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bAnaconda\uff08\u200b\u63a8\u8350\u200b\uff09\u200b\u6216\u200bPython\u200b\u7684\u200b\u672c\u5730\u200b<code>venv</code>\u200b\u6a21\u5757\u200b\u5b89\u88c5\u200b\u4e00\u4e2a\u200b\u3002\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bPython 3.7\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4e0d\u518d\u200b\u652f\u6301\u200bPython 2\u3002</p>"},{"location":"courses/preparation/setup/#anaconda","title":"Anaconda \u200b\u865a\u62df\u73af\u5883","text":"<p>\u200b\u6211\u4eec\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\u4f7f\u7528\u200b\u514d\u8d39\u200b\u7684\u200bAnaconda Python\u200b\u53d1\u884c\u7248\u200b\uff0c\u200b\u4e3a\u200b\u60a8\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u79cd\u200b\u5904\u7406\u200b\u5305\u200b\u4f9d\u8d56\u200b\u5173\u7cfb\u200b\u7684\u200b\u7b80\u5355\u200b\u65b9\u6cd5\u200b\u3002\u200b\u8bf7\u200b\u52a1\u5fc5\u200b\u4e0b\u8f7d\u200bPython 3\u200b\u7248\u672c\u200b\uff0c\u200b\u8be5\u200b\u7248\u672c\u200b\u76ee\u524d\u200b\u5b89\u88c5\u200b\u7684\u200b\u662f\u200bPython 3.7\u3002Anaconda\u200b\u7684\u200b\u5de7\u5999\u200b\u4e4b\u200b\u5904\u200b\u5728\u4e8e\u200b\u5b83\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u9644\u5e26\u200b\u4e86\u200bMKL\u200b\u4f18\u5316\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u60a8\u200b\u7684\u200b<code>numpy</code>\u200b\u548c\u200b<code>scipy</code>\u200b\u4ee3\u7801\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u663e\u8457\u200b\u7684\u200b\u52a0\u901f\u200b\u4e2d\u200b\u83b7\u76ca\u200b\uff0c\u200b\u800c\u200b\u65e0\u9700\u200b\u66f4\u6539\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4e00\u65e6\u200b\u5b89\u88c5\u200b\u4e86\u200bAnaconda\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e3a\u200b\u8bfe\u7a0b\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u865a\u62df\u73af\u5883\u200b\u3002\u200b\u5982\u679c\u200b\u60a8\u200b\u9009\u62e9\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u865a\u62df\u73af\u5883\u200b\uff08\u200b\u5f3a\u70c8\u200b\u4e0d\u200b\u63a8\u8350\u200b\uff01\uff09\uff0c\u200b\u4ee3\u7801\u200b\u7684\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b\u9879\u200b\u90fd\u200b\u5168\u5c40\u200b\u5b89\u88c5\u200b\u5728\u200b\u60a8\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u3002\u200b\u8981\u200b\u8bbe\u7f6e\u200b\u540d\u4e3a\u200b<code>cs231n</code>\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u7ec8\u7aef\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u64cd\u4f5c\u200b\uff1a</p> <pre><code># \u200b\u8fd9\u200b\u5c06\u200b\u5728\u200b'path/to/anaconda3/envs/'\u200b\u4e2d\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200bcs231n\u200b\u7684\u200banaconda\u200b\u73af\u5883\u200b\nconda create -n cs231n python=3.7\n</code></pre> <p>\u200b\u8981\u200b\u6fc0\u6d3b\u200b\u5e76\u200b\u8fdb\u5165\u200b\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u8fd0\u884c\u200b<code>conda activate cs231n</code>\u3002\u200b\u8981\u200b\u505c\u7528\u200b\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u8fd0\u884c\u200b<code>conda deactivate cs231n</code>\u200b\u6216\u200b\u9000\u51fa\u200b\u7ec8\u7aef\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u60a8\u200b\u8981\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u65f6\u200b\uff0c\u200b\u90fd\u200b\u5e94\u8be5\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b<code>conda activate cs231n</code>\u3002</p> <pre><code># \u200b\u5728\u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\u4e4b\u540e\u200b\uff0c\u200b\u68c0\u67e5\u200bpython\u200b\u4e8c\u8fdb\u5236\u200b\u6587\u4ef6\u200b\u7684\u200b\u8def\u5f84\u200b\u662f\u5426\u200b\u4e0e\u200banaconda env\u200b\u7684\u200b\u8def\u5f84\u200b\u5339\u914d\u200b\nwhich python\n# \u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\uff0c\u200b\u6253\u5370\u200b\u51fa\u200b\n# $ '/Users/kevin/anaconda3/envs/sci/bin/python'\n</code></pre> <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u672c\u9875\u200b\u6709\u5173\u200b\u4f7f\u7528\u200bAnaconda\u200b\u7ba1\u7406\u200b\u865a\u62df\u73af\u5883\u200b\u7684\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u8bf4\u660e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u5982\u679c\u200b\u60a8\u200b\u9009\u62e9\u200b\u8d70\u200bAnaconda \u200b\u8def\u7ebf\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5b89\u5168\u200b\u5730\u200b\u8df3\u200b\u8fc7\u200b\u4e0b\u200b\u4e00\u8282\u200b\uff0c\u200b\u76f4\u63a5\u200b\u8fdb\u5165\u200b\u5b89\u88c5\u200b\u8f6f\u4ef6\u5305\u200b\u3002</p> <p>\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u672c\u9875\u200b\u6709\u5173\u200b\u4f7f\u7528\u200bAnaconda\u200b\u7ba1\u7406\u200b\u865a\u62df\u73af\u5883\u200b\u7684\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u8bf4\u660e\u200b\u3002</p>"},{"location":"courses/preparation/setup/#python-venv","title":"Python venv","text":"<p>\u200b\u4ece\u200b3.3\u200b\u5f00\u59cb\u200b\uff0cPython\u200b\u81ea\u5e26\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b venv\u200b\u7684\u200b\u8f7b\u91cf\u7ea7\u200b\u865a\u62df\u73af\u5883\u200b\u6a21\u5757\u200b\u3002\u200b\u6bcf\u4e2a\u200b\u865a\u62df\u73af\u5883\u200b\u90fd\u200b\u6253\u5305\u200b\u81ea\u5df1\u200b\u72ec\u7acb\u200b\u7684\u200b\u4e00\u7ec4\u200b\u5df2\u200b\u5b89\u88c5\u200bPython\u200b\u5305\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5305\u4e0e\u200b\u7cfb\u7edf\u200b\u8303\u56f4\u200b\u7684\u200bPython\u200b\u5305\u200b\u9694\u79bb\u200b\uff0c\u200b\u5e76\u200b\u8fd0\u884c\u200b\u4e0e\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u5b83\u200b\u7684\u200b\u4e8c\u8fdb\u5236\u200b\u6587\u4ef6\u200b\u76f8\u5339\u914d\u200b\u7684\u200bPython\u200b\u7248\u672c\u200b\u3002\u200b\u8981\u200b\u8bbe\u7f6e\u200b\u540d\u4e3a\u200b<code>cs231n</code>\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u7ec8\u7aef\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u64cd\u4f5c\u200b\uff1a</p> <pre><code># \u200b\u8fd9\u200b\u5c06\u200b\u5728\u200b\u4e3b\u76ee\u5f55\u200b\u4e2d\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200bcs231n\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\npython3.7 -m venv ~/cs231n\n</code></pre> <p>\u200b\u8981\u200b\u6fc0\u6d3b\u200b\u5e76\u200b\u8fdb\u5165\u200b\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u8fd0\u884c\u200b<code>source~/cs231n/bin/activate</code>\u3002\u200b\u8981\u200b\u505c\u7528\u200b\u73af\u5883\u200b\uff0c\u200b\u8bf7\u200b\u8fd0\u884c\u200b<code>deactivate</code>\u200b\u6216\u200b\u9000\u51fa\u200b\u7ec8\u7aef\u200b\u3002\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u60a8\u200b\u60f3\u200b\u5904\u7406\u200b\u4efb\u52a1\u200b\u65f6\u200b\uff0c\u200b\u90fd\u200b\u5e94\u8be5\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b<code>source~/cs231n/bin/activate</code>\u3002</p> <pre><code># \u200b\u5728\u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\u4e4b\u540e\u200b\uff0c\u200b\u68c0\u67e5\u200bpython\u200b\u4e8c\u8fdb\u5236\u200b\u6587\u4ef6\u200b\u7684\u200b\u8def\u5f84\u200b\u662f\u5426\u200b\u4e0e\u200bvirtual env\u200b\u7684\u200b\u8def\u5f84\u200b\u5339\u914d\u200b\nwhich python\n# \u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\uff0c\u200b\u6253\u5370\u200b\u51fa\u200b\n# $ '/Users/kevin/cs231n/bin/python'\n</code></pre>"},{"location":"courses/preparation/setup/#_2","title":"\u5b89\u88c5\u200b\u8f6f\u4ef6\u5305","text":"<p>\u200b\u4e00\u65e6\u200b\u60a8\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u5e76\u200b\u6fc0\u6d3b\u200b\u4e86\u200b\u60a8\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff08\u200b\u901a\u8fc7\u200b<code>conda</code>\u200b\u6216\u200b<code>venv</code>\uff09\uff0c\u200b\u60a8\u200b\u5c31\u200b\u5e94\u8be5\u200b\u5b89\u88c5\u200b\u4f7f\u7528\u200b<code>pip</code>\u200b\u8fd0\u884c\u200b\u4efb\u52a1\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5e93\u200b\u3002\u200b\u8981\u200b\u6267\u884c\u200b\u6b64\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8bf7\u200b\u8fd0\u884c\u200b\uff1a</p> <pre><code># \u200b\u540c\u6837\u200b\uff0c\u200b\u5728\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u60a8\u200b\u7684\u200b\u865a\u62df\u73af\u5883\u200b\uff08conda\u200b\u6216\u200bvenv\uff09\u200b\u5df2\u200b\u88ab\u200b\u6fc0\u6d3b\u200b\ncd assignment1  # \u200b\u8f6c\u200b\u5230\u200b\u4efb\u52a1\u200b\u7684\u200b\u76ee\u5f55\u200b\n\n# \u200b\u5b89\u88c5\u200b\u4efb\u52a1\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\n# \u200b\u7531\u4e8e\u200b\u865a\u62df\u200benv\u200b\u88ab\u200b\u6fc0\u6d3b\u200b\uff0c\u200b\u8be5\u200bpip\u200b\u4e0e\u200b\u73af\u5883\u200b\u7684\u200bpython\u200b\u4e8c\u8fdb\u5236\u200b\u6587\u4ef6\u200b\u76f8\u5173\u8054\u200b\npip install -r requirements.txt\n</code></pre>"}]}